setGPU: Setting GPU to: 2
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_111/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
7189869 events read in 15 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_111/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_111/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
4796089 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_111/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_111/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
605887 events read in 2 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_111/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
training on 1917872 events, validating on 479469

training QR for quantile 0.1
WARNING: Logging before flag parsing goes to stderr.
W0201 17:19:19.521909 139630528583488 deprecation.py:323] From /cvmfs/sft.cern.ch/lcg/views/LCG_96bpython3/x86_64-centos7-gcc9-opt/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense (Dense)                (None, 60)                120       
_________________________________________________________________
dense_1 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_2 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_3 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_4 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Train on 1917872 samples, validate on 479469 samples
Epoch 1/300
1917872/1917872 - 21s - loss: 0.0057 - val_loss: 0.0035
Epoch 2/300
1917872/1917872 - 20s - loss: 0.0037 - val_loss: 0.0035
Epoch 3/300
1917872/1917872 - 19s - loss: 0.0035 - val_loss: 0.0034
Epoch 4/300

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 5/300
1917872/1917872 - 19s - loss: 0.0034 - val_loss: 0.0034
Epoch 6/300
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 7/300

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 8/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 9/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 10/300

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 11/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 12/300
1917872/1917872 - 19s - loss: 0.0034 - val_loss: 0.0034
Epoch 13/300

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 14/300
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 15/300
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 16/300

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 17/300
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 18/300
1917872/1917872 - 22s - loss: 0.0034 - val_loss: 0.0034
Epoch 19/300

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 20/300
1917872/1917872 - 21s - loss: 0.0034 - val_loss: 0.0034
Epoch 21/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 22/300

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 23/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 24/300
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 25/300

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
1917872/1917872 - 20s - loss: 0.0034 - val_loss: 0.0034
Epoch 00025: early stopping
saving model  QRmodel_run_111_qnt_10_sigx_100.0_20210201.h5
running prediction
Traceback (most recent call last):
  File "main_bump_hunt.py", line 119, in <module>
    rpu.make_bg_vs_sig_ratio_plot(sample.rejected(mjj_key), sample.accepted(mjj_key), target_value=quantile, n_bins=60, title=title, plot_name='mJJ_raio_bg_vs_sig_'+sample.name, fig_dir=plot_dir_mjj)
  File "/eos/home-k/kiwoznia/dev/physics_objects_for_anomaly_hunting/pofah/jet_sample.py", line 109, in rejected
    q_key = 'sel_q{:02}'.format(int(quantile*100)) 
ValueError: invalid literal for int() with base 10: 'mJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJmJJm
