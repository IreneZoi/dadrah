setGPU: Setting GPU to: 0
bin centers:  [1227.5, 1287.5, 1353.5, 1422.0, 1493.0, 1566.5, 1642.5, 1721.0, 1802.5, 1887.0, 1974.5, 2065.0, 2158.5, 2255.5, 2355.5, 2459.0, 2566.0, 2676.5, 2791.0, 2909.0, 3031.0, 3157.0, 3287.0, 3421.5, 3561.0, 3705.0, 3853.0, 4006.0, 4164.5, 4328.0, 4497.0, 4671.5, 4851.5, 5037.5, 5229.5, 5450.5, 5655.5, 5844.0, 6062.0, 6287.5, 6520.0, 6760.0]
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
4793609 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
4796089 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
qcd all: min mjj = 1200.0001220703125, max mjj = 7285.58154296875
training on 1917939 events, validating on 1917940

training QR for quantile 0.1
Model: "functional_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense (Dense)                (None, 60)                120       
_________________________________________________________________
dense_1 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_2 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_3 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_4 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 25s - loss: 0.0250 - val_loss: 0.0242
Epoch 2/100
7492/7492 - 25s - loss: 0.0240 - val_loss: 0.0241
Epoch 3/100
7492/7492 - 26s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7492/7492 - 24s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7492/7492 - 25s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100
7492/7492 - 25s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7492/7492 - 25s - loss: 0.0238 - val_loss: 0.0239
Epoch 8/100
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0238
Epoch 10/100
7492/7492 - 25s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7492/7492 - 23s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7492/7492 - 25s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 31/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 32/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_10_no_signal_sigx_0_loss_rk5_05_20210510_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917939

training QR for quantile 0.1
Model: "functional_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_6 (Dense)              (None, 60)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_8 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_9 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_10 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 25s - loss: 0.0249 - val_loss: 0.0239
Epoch 2/100
7492/7492 - 24s - loss: 0.0240 - val_loss: 0.0245
Epoch 3/100
7492/7492 - 23s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7492/7492 - 24s - loss: 0.0239 - val_loss: 0.0237
Epoch 5/100
7492/7492 - 26s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 24s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7492/7492 - 25s - loss: 0.0237 - val_loss: 0.0236
Epoch 8/100
7492/7492 - 23s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 11/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 13/100
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0236
Epoch 14/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_10_no_signal_sigx_0_loss_rk5_05_20210510_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917939 events, validating on 1917940

training QR for quantile 0.1
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_13 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_14 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_15 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_16 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 25s - loss: 0.0250 - val_loss: 0.0240
Epoch 2/100
7492/7492 - 23s - loss: 0.0240 - val_loss: 0.0239
Epoch 3/100
7492/7492 - 24s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7492/7492 - 24s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7492/7492 - 24s - loss: 0.0238 - val_loss: 0.0239
Epoch 6/100
7492/7492 - 24s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 23s - loss: 0.0238 - val_loss: 0.0238
Epoch 8/100
7492/7492 - 26s - loss: 0.0237 - val_loss: 0.0238
Epoch 9/100
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100
7492/7492 - 26s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 25s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 21s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100
7492/7492 - 20s - loss: 0.0236 - val_loss: 0.0236
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_no_signal_sigx_0_loss_rk5_05_20210510_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917940

training QR for quantile 0.1
Model: "functional_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_18 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_19 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_20 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_21 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_22 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_23 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 21s - loss: 0.0249 - val_loss: 0.0239
Epoch 2/100
7492/7492 - 22s - loss: 0.0240 - val_loss: 0.0240
Epoch 3/100
7492/7492 - 24s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0237
Epoch 8/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0239
Epoch 9/100
7492/7492 - 20s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 23s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 21s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 21s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7492/7492 - 24s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7492/7492 - 22s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 23s - loss: 0.0236 - val_loss: 0.0237
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_10_no_signal_sigx_0_loss_rk5_05_20210510_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917939

training QR for quantile 0.1
Model: "functional_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_24 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_25 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_26 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_27 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_28 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_29 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0248 - val_loss: 0.0239
Epoch 2/100
7492/7492 - 22s - loss: 0.0240 - val_loss: 0.0241
Epoch 3/100
7492/7492 - 21s - loss: 0.0239 - val_loss: 0.0242
Epoch 4/100
7492/7492 - 21s - loss: 0.0239 - val_loss: 0.0237
Epoch 5/100
7492/7492 - 21s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7492/7492 - 22s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 21s - loss: 0.0238 - val_loss: 0.0237
Epoch 8/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7492/7492 - 23s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7492/7492 - 23s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7492/7492 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100
7492/7492 - 21s - loss: 0.0237 - val_loss: 0.0236
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_no_signal_sigx_0_loss_rk5_05_20210510_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2cd463bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58446448e+00 1.34762411e-04 1.58423328e+00
  1.58464801e+00]
 [1.28750000e+03 1.61229701e+00 3.17618369e-04 1.61181045e+00
  1.61265981e+00]
 [1.35350000e+03 1.64329665e+00 2.52658881e-04 1.64304328e+00
  1.64365196e+00]
 [1.42200000e+03 1.67537217e+00 5.65705207e-04 1.67470264e+00
  1.67626143e+00]
 [1.49300000e+03 1.70715919e+00 7.42293686e-04 1.70606971e+00
  1.70823312e+00]
 [1.56650000e+03 1.74193828e+00 1.94091330e-04 1.74157333e+00
  1.74212945e+00]
 [1.64250000e+03 1.77669539e+00 3.11206506e-04 1.77612960e+00
  1.77703667e+00]
 [1.72100000e+03 1.81296391e+00 5.97628333e-04 1.81220007e+00
  1.81401801e+00]
 [1.80250000e+03 1.85181370e+00 4.58259186e-04 1.85119319e+00
  1.85251558e+00]
 [1.88700000e+03 1.89293189e+00 6.45301000e-04 1.89213371e+00
  1.89365625e+00]
 [1.97450000e+03 1.93660262e+00 9.30092523e-04 1.93536341e+00
  1.93783820e+00]
 [2.06500000e+03 1.98340940e+00 9.39024690e-04 1.98216116e+00
  1.98455012e+00]
 [2.15850000e+03 2.03373680e+00 7.28539887e-04 2.03287244e+00
  2.03477025e+00]
 [2.25550000e+03 2.08805375e+00 6.28078230e-04 2.08716226e+00
  2.08867979e+00]
 [2.35550000e+03 2.14667158e+00 8.15771129e-04 2.14551020e+00
  2.14791751e+00]
 [2.45900000e+03 2.21047401e+00 1.17531923e-03 2.20842743e+00
  2.21181440e+00]
 [2.56600000e+03 2.27926044e+00 1.44070862e-03 2.27664089e+00
  2.28090954e+00]
 [2.67650000e+03 2.35361657e+00 1.62999228e-03 2.35197377e+00
  2.35587502e+00]
 [2.79100000e+03 2.43508310e+00 2.80096417e-03 2.43078756e+00
  2.43777132e+00]
 [2.90900000e+03 2.52506967e+00 3.73289776e-03 2.51992941e+00
  2.53003407e+00]
 [3.03100000e+03 2.62457862e+00 5.02817913e-03 2.61997461e+00
  2.63329124e+00]
 [3.15700000e+03 2.73455806e+00 6.48570075e-03 2.72805905e+00
  2.74653006e+00]
 [3.28700000e+03 2.85674295e+00 7.95216299e-03 2.84738708e+00
  2.87112236e+00]
 [3.42150000e+03 2.98990407e+00 9.44416053e-03 2.98105359e+00
  3.00678730e+00]
 [3.56100000e+03 3.13491096e+00 8.27187564e-03 3.12588406e+00
  3.15007257e+00]
 [3.70500000e+03 3.29007077e+00 1.17201356e-02 3.27717996e+00
  3.30645537e+00]
 [3.85300000e+03 3.45405607e+00 1.88085293e-02 3.43235803e+00
  3.48615050e+00]
 [4.00600000e+03 3.62860465e+00 2.59038999e-02 3.59770846e+00
  3.67289066e+00]
 [4.16450000e+03 3.81476831e+00 3.08937687e-02 3.77723289e+00
  3.86704469e+00]
 [4.32800000e+03 4.01408715e+00 3.25195723e-02 3.97707009e+00
  4.06783962e+00]
 [4.49700000e+03 4.23372698e+00 3.65597680e-02 4.18101263e+00
  4.27578020e+00]
 [4.67150000e+03 4.47327309e+00 6.00845734e-02 4.38491917e+00
  4.56594992e+00]
 [4.85150000e+03 4.72124624e+00 9.41389710e-02 4.59533787e+00
  4.86839867e+00]
 [5.03750000e+03 4.97776699e+00 1.32067097e-01 4.81279087e+00
  5.18134928e+00]
 [5.22950000e+03 5.24264860e+00 1.72163930e-01 5.03723001e+00
  5.50458717e+00]
 [5.45050000e+03 5.54739952e+00 2.18604790e-01 5.29549026e+00
  5.87666512e+00]
 [5.65550000e+03 5.82969465e+00 2.61499335e-01 5.53494883e+00
  6.22167349e+00]
 [5.84400000e+03 6.08868093e+00 3.00491207e-01 5.75503016e+00
  6.53871202e+00]
 [6.06200000e+03 6.38715954e+00 3.44669270e-01 6.00941181e+00
  6.90506077e+00]
 [6.28750000e+03 6.69418659e+00 3.88779404e-01 6.27236795e+00
  7.28360415e+00]
 [6.52000000e+03 7.00795164e+00 4.31706610e-01 6.54328537e+00
  7.67340279e+00]
 [6.76000000e+03 7.32700567e+00 4.71942325e-01 6.82270575e+00
  8.07519627e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917939 events, validating on 1917940

training QR for quantile 0.9
Model: "functional_11"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_30 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_31 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_32 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_33 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_34 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_35 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0281 - val_loss: 0.0261
Epoch 2/100
7492/7492 - 24s - loss: 0.0257 - val_loss: 0.0265
Epoch 3/100
7492/7492 - 23s - loss: 0.0256 - val_loss: 0.0257
Epoch 4/100
7492/7492 - 22s - loss: 0.0255 - val_loss: 0.0257
Epoch 5/100
7492/7492 - 24s - loss: 0.0255 - val_loss: 0.0253
Epoch 6/100
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 9/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 18/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 19/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 22/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0252
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 24/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 25/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 27/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0252
Epoch 28/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0252
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 30/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 31/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0252
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 33/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 34/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0252
Epoch 36/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 37/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 38/100

Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0252
Epoch 00038: early stopping
saving model QRmodel_run_113_qnt_90_no_signal_sigx_0_loss_rk5_05_20210510_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917939

training QR for quantile 0.9
Model: "functional_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_36 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_37 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_38 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_39 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_40 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_41 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0278 - val_loss: 0.0253
Epoch 2/100
7492/7492 - 24s - loss: 0.0256 - val_loss: 0.0253
Epoch 3/100
7492/7492 - 23s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 22s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 15/100
7492/7492 - 24s - loss: 0.0252 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 24s - loss: 0.0252 - val_loss: 0.0253
Epoch 17/100
7492/7492 - 21s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7492/7492 - 22s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 23s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100
7492/7492 - 22s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7492/7492 - 22s - loss: 0.0252 - val_loss: 0.0253
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_90_no_signal_sigx_0_loss_rk5_05_20210510_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917939 events, validating on 1917940

training QR for quantile 0.9
Model: "functional_15"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_42 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_43 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_44 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_45 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_46 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_47 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0269 - val_loss: 0.0260
Epoch 2/100
7492/7492 - 22s - loss: 0.0257 - val_loss: 0.0253
Epoch 3/100
7492/7492 - 22s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100
7492/7492 - 22s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 21s - loss: 0.0255 - val_loss: 0.0254
Epoch 6/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0254
Epoch 9/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7492/7492 - 20s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_90_no_signal_sigx_0_loss_rk5_05_20210510_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917940

training QR for quantile 0.9
Model: "functional_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_48 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_49 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_50 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_51 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_52 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_53 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0280 - val_loss: 0.0255
Epoch 2/100
7492/7492 - 22s - loss: 0.0257 - val_loss: 0.0256
Epoch 3/100
7492/7492 - 23s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 23s - loss: 0.0255 - val_loss: 0.0256
Epoch 5/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0254
Epoch 6/100
7492/7492 - 25s - loss: 0.0253 - val_loss: 0.0254
Epoch 7/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 20s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7492/7492 - 21s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_90_no_signal_sigx_0_loss_rk5_05_20210510_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c2c079598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917940 events, validating on 1917939

training QR for quantile 0.9
Model: "functional_19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_54 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_55 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_56 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_57 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_58 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_59 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 24s - loss: 0.0280 - val_loss: 0.0255
Epoch 2/100
7492/7492 - 24s - loss: 0.0258 - val_loss: 0.0255
Epoch 3/100
7492/7492 - 25s - loss: 0.0256 - val_loss: 0.0257
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 24s - loss: 0.0255 - val_loss: 0.0255
Epoch 5/100
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 6/100
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100
7492/7492 - 23s - loss: 0.0254 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 25s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7492/7492 - 27s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7492/7492 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7492/7492 - 23s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7492/7492 - 24s - loss: 0.0253 - val_loss: 0.0253
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_90_no_signal_sigx_0_loss_rk5_05_20210510_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c24785d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88125708e+00 1.06287564e-04 1.88106263e+00
  1.88137996e+00]
 [1.28750000e+03 1.91377280e+00 2.72917045e-04 1.91335309e+00
  1.91409409e+00]
 [1.35350000e+03 1.95109966e+00 4.61276395e-04 1.95025885e+00
  1.95152235e+00]
 [1.42200000e+03 1.99169793e+00 6.06043602e-04 1.99102247e+00
  1.99281502e+00]
 [1.49300000e+03 2.03740983e+00 9.13949832e-04 2.03611016e+00
  2.03880167e+00]
 [1.56650000e+03 2.08701487e+00 3.48530140e-04 2.08638048e+00
  2.08735895e+00]
 [1.64250000e+03 2.14002118e+00 7.46599847e-04 2.13903451e+00
  2.14118695e+00]
 [1.72100000e+03 2.20041280e+00 5.18493755e-04 2.19972372e+00
  2.20121288e+00]
 [1.80250000e+03 2.26631117e+00 5.36720500e-04 2.26557255e+00
  2.26704550e+00]
 [1.88700000e+03 2.33858953e+00 9.44330776e-04 2.33699346e+00
  2.33971453e+00]
 [1.97450000e+03 2.41829476e+00 7.90440613e-04 2.41736650e+00
  2.41961193e+00]
 [2.06500000e+03 2.50485268e+00 1.38570515e-03 2.50300360e+00
  2.50692344e+00]
 [2.15850000e+03 2.59889097e+00 1.27361617e-03 2.59664416e+00
  2.60052538e+00]
 [2.25550000e+03 2.70321403e+00 1.58650802e-03 2.70174384e+00
  2.70569730e+00]
 [2.35550000e+03 2.81849213e+00 2.32391392e-03 2.81507516e+00
  2.82153749e+00]
 [2.45900000e+03 2.94383411e+00 3.64653600e-03 2.94049931e+00
  2.94962406e+00]
 [2.56600000e+03 3.08057008e+00 4.87204957e-03 3.07357693e+00
  3.08740449e+00]
 [2.67650000e+03 3.23068776e+00 5.69530744e-03 3.22108412e+00
  3.23617291e+00]
 [2.79100000e+03 3.39751267e+00 6.88865227e-03 3.38614249e+00
  3.40647078e+00]
 [2.90900000e+03 3.58227496e+00 1.00135708e-02 3.56934047e+00
  3.59622765e+00]
 [3.03100000e+03 3.78543310e+00 1.13118895e-02 3.77275276e+00
  3.80130434e+00]
 [3.15700000e+03 4.00861697e+00 1.77562436e-02 3.98809838e+00
  4.03598928e+00]
 [3.28700000e+03 4.25144768e+00 2.91794869e-02 4.21969318e+00
  4.30095959e+00]
 [3.42150000e+03 4.51884565e+00 3.31160775e-02 4.48794270e+00
  4.57886076e+00]
 [3.56100000e+03 4.80995493e+00 3.27362137e-02 4.77154779e+00
  4.87052107e+00]
 [3.70500000e+03 5.12156315e+00 3.53462477e-02 5.07884884e+00
  5.17513037e+00]
 [3.85300000e+03 5.46530981e+00 3.76673113e-02 5.40661144e+00
  5.51463413e+00]
 [4.00600000e+03 5.85072718e+00 5.35312465e-02 5.78116655e+00
  5.93060684e+00]
 [4.16450000e+03 6.25575399e+00 9.10731594e-02 6.16693926e+00
  6.37061119e+00]
 [4.32800000e+03 6.67567635e+00 1.35567192e-01 6.52330160e+00
  6.84927654e+00]
 [4.49700000e+03 7.11107426e+00 1.83478979e-01 6.89260197e+00
  7.35456324e+00]
 [4.67150000e+03 7.56150017e+00 2.33847776e-01 7.27442837e+00
  7.87753630e+00]
 [4.85150000e+03 8.02669888e+00 2.86272014e-01 7.66867304e+00
  8.41780090e+00]
 [5.03750000e+03 8.50778446e+00 3.40668924e-01 8.07636929e+00
  8.97654533e+00]
 [5.22950000e+03 9.00462971e+00 3.96890358e-01 8.49746513e+00
  9.55349922e+00]
 [5.45050000e+03 9.57666512e+00 4.61524547e-01 8.98239136e+00
  1.02174702e+01]
 [5.65550000e+03 1.01073133e+01 5.21283477e-01 9.43234634e+00
  1.08329325e+01]
 [5.84400000e+03 1.05951466e+01 5.75984534e-01 9.84611416e+00
  1.13981943e+01]
 [6.06200000e+03 1.11588570e+01 6.38960714e-01 1.03245554e+01
  1.20509691e+01]
 [6.28750000e+03 1.17414797e+01 7.04164119e-01 1.08192062e+01
  1.27261810e+01]
 [6.52000000e+03 1.23415123e+01 7.71656060e-01 1.13286085e+01
  1.34225101e+01]
 [6.76000000e+03 1.29598274e+01 8.41867398e-01 1.18531733e+01
  1.41414652e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917939 events, validating on 1917940

training QR for quantile 0.99
Model: "functional_21"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_11 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_60 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_61 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_62 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_63 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_64 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_65 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 25s - loss: 0.0057 - val_loss: 0.0044
Epoch 2/100
7492/7492 - 24s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7492/7492 - 23s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 21s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7492/7492 - 20s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_99_no_signal_sigx_0_loss_rk5_05_20210510_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917939

training QR for quantile 0.99
Model: "functional_23"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_66 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_67 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_68 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_69 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_70 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_71 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 22s - loss: 0.0064 - val_loss: 0.0045
Epoch 2/100
7492/7492 - 22s - loss: 0.0045 - val_loss: 0.0049
Epoch 3/100
7492/7492 - 21s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7492/7492 - 23s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7492/7492 - 22s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100
7492/7492 - 23s - loss: 0.0044 - val_loss: 0.0044
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 24s - loss: 0.0044 - val_loss: 0.0043
Epoch 8/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7492/7492 - 25s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 00021: early stopping
saving model QRmodel_run_113_qnt_99_no_signal_sigx_0_loss_rk5_05_20210510_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917939 events, validating on 1917940

training QR for quantile 0.99
Model: "functional_25"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_13 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_72 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_73 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_74 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_75 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_76 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_77 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 23s - loss: 0.0058 - val_loss: 0.0047
Epoch 2/100
7492/7492 - 24s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7492/7492 - 24s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7492/7492 - 23s - loss: 0.0044 - val_loss: 0.0045
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 24s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7492/7492 - 20s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_99_no_signal_sigx_0_loss_rk5_05_20210510_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
training on 1917940 events, validating on 1917940

training QR for quantile 0.99
Model: "functional_27"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_14 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_78 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_79 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_80 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_81 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_82 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_83 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 24s - loss: 0.0057 - val_loss: 0.0044
Epoch 2/100
7492/7492 - 22s - loss: 0.0045 - val_loss: 0.0046
Epoch 3/100
7492/7492 - 22s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7492/7492 - 22s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0044
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7492/7492 - 25s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 25s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_99_no_signal_sigx_0_loss_rk5_05_20210510_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2ce448ab70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917940 events, validating on 1917939

training QR for quantile 0.99
Model: "functional_29"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_84 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_85 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_86 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_87 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_88 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_89 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7492/7492 - 24s - loss: 0.0060 - val_loss: 0.0044
Epoch 2/100
7492/7492 - 22s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7492/7492 - 22s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7492/7492 - 21s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7492/7492 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7492/7492 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7492/7492 - 23s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7492/7492 - 21s - loss: 0.0043 - val_loss: 0.0043
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_99_no_signal_sigx_0_loss_rk5_05_20210510_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2cd46bb9d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.99: 
[[1.22750000e+03 2.01607976e+00 1.32574555e-03 2.01423097e+00
  2.01767755e+00]
 [1.28750000e+03 2.05657630e+00 1.07741798e-03 2.05464578e+00
  2.05780387e+00]
 [1.35350000e+03 2.10471525e+00 1.31529304e-03 2.10260653e+00
  2.10658979e+00]
 [1.42200000e+03 2.15445766e+00 8.19522000e-04 2.15346503e+00
  2.15582728e+00]
 [1.49300000e+03 2.21259794e+00 1.75748500e-03 2.20923758e+00
  2.21419644e+00]
 [1.56650000e+03 2.27533383e+00 2.12568989e-03 2.27229548e+00
  2.27774072e+00]
 [1.64250000e+03 2.34721136e+00 1.37636845e-03 2.34572172e+00
  2.34977031e+00]
 [1.72100000e+03 2.42346487e+00 2.55790764e-03 2.41996169e+00
  2.42669702e+00]
 [1.80250000e+03 2.51032519e+00 1.13816247e-03 2.50900626e+00
  2.51204348e+00]
 [1.88700000e+03 2.60403113e+00 3.05414676e-03 2.59919524e+00
  2.60834789e+00]
 [1.97450000e+03 2.70734076e+00 4.16313398e-03 2.70110440e+00
  2.71410847e+00]
 [2.06500000e+03 2.82311816e+00 3.00907387e-03 2.82034492e+00
  2.82850933e+00]
 [2.15850000e+03 2.94988122e+00 4.45588660e-03 2.94344091e+00
  2.95624399e+00]
 [2.25550000e+03 3.08906937e+00 5.59673679e-03 3.08022213e+00
  3.09746766e+00]
 [2.35550000e+03 3.24120383e+00 6.14131647e-03 3.23285151e+00
  3.24872327e+00]
 [2.45900000e+03 3.40788279e+00 7.15660532e-03 3.40138626e+00
  3.42030239e+00]
 [2.56600000e+03 3.58996882e+00 9.95121435e-03 3.57968497e+00
  3.60770369e+00]
 [2.67650000e+03 3.78954754e+00 1.27955270e-02 3.77606392e+00
  3.81166077e+00]
 [2.79100000e+03 4.00959826e+00 1.78522634e-02 3.98085976e+00
  4.03607464e+00]
 [2.90900000e+03 4.24898701e+00 2.56926993e-02 4.21124029e+00
  4.28168344e+00]
 [3.03100000e+03 4.51142778e+00 3.02582271e-02 4.46445656e+00
  4.54850912e+00]
 [3.15700000e+03 4.79632101e+00 3.98299489e-02 4.72045374e+00
  4.83767271e+00]
 [3.28700000e+03 5.10408354e+00 4.81095294e-02 5.00979662e+00
  5.14104462e+00]
 [3.42150000e+03 5.44161329e+00 5.13500474e-02 5.34663057e+00
  5.49343491e+00]
 [3.56100000e+03 5.81781454e+00 3.98549106e-02 5.76078081e+00
  5.87904596e+00]
 [3.70500000e+03 6.23654261e+00 3.84656568e-02 6.17036629e+00
  6.27897549e+00]
 [3.85300000e+03 6.69125795e+00 6.54753343e-02 6.59019709e+00
  6.76476431e+00]
 [4.00600000e+03 7.17367229e+00 1.03875271e-01 7.03031921e+00
  7.30969620e+00]
 [4.16450000e+03 7.68030052e+00 1.48607740e-01 7.49178791e+00
  7.89175987e+00]
 [4.32800000e+03 8.20513992e+00 1.98274590e-01 7.96949005e+00
  8.49697590e+00]
 [4.49700000e+03 8.74958801e+00 2.52423539e-01 8.46392345e+00
  9.12803078e+00]
 [4.67150000e+03 9.31364098e+00 3.11515238e-01 8.97495270e+00
  9.78709984e+00]
 [4.85150000e+03 9.89737186e+00 3.77033351e-01 9.47417259e+00
  1.04776096e+01]
 [5.03750000e+03 1.05011900e+01 4.49811242e-01 9.98108673e+00
  1.12001667e+01]
 [5.22950000e+03 1.11131182e+01 5.15311914e-01 1.04895744e+01
  1.19014683e+01]
 [5.45050000e+03 1.17824120e+01 5.45159216e-01 1.10645838e+01
  1.25378561e+01]
 [5.65550000e+03 1.23995592e+01 5.80863555e-01 1.15824814e+01
  1.31162424e+01]
 [5.84400000e+03 1.29619932e+01 6.28929992e-01 1.20282164e+01
  1.36502390e+01]
 [6.06200000e+03 1.35974977e+01 7.17693137e-01 1.24614420e+01
  1.42781353e+01]
 [6.28750000e+03 1.42501120e+01 8.35049520e-01 1.28626118e+01
  1.49525566e+01]
 [6.52000000e+03 1.49309065e+01 9.69622175e-01 1.32701006e+01
  1.57469730e+01]
 [6.76000000e+03 1.56350159e+01 1.11645945e+00 1.36811218e+01
  1.65680161e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
