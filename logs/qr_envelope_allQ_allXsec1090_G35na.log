setGPU: Setting GPU to: 0
bin centers:  [1227.5, 1287.5, 1353.5, 1422.0, 1493.0, 1566.5, 1642.5, 1721.0, 1802.5, 1887.0, 1974.5, 2065.0, 2158.5, 2255.5, 2355.5, 2459.0, 2566.0, 2676.5, 2791.0, 2909.0, 3031.0, 3157.0, 3287.0, 3421.5, 3561.0, 3705.0, 3853.0, 4006.0, 4164.5, 4328.0, 4497.0, 4671.5, 4851.5, 5037.5, 5229.5, 5450.5, 5655.5, 5844.0, 6062.0, 6287.5, 6520.0, 6760.0]
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
4793609 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
4796089 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
qcd all: min mjj = 1200.0001220703125, max mjj = 7285.58154296875
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
531825 events read in 2 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
training on 1917961 events, validating on 1917962

training QR for quantile 0.1
Model: "functional_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense (Dense)                (None, 60)                120       
_________________________________________________________________
dense_1 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_2 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_3 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_4 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 44s - loss: 0.0249 - val_loss: 0.0239
Epoch 2/100
7493/7493 - 38s - loss: 0.0240 - val_loss: 0.0237
Epoch 3/100
7493/7493 - 36s - loss: 0.0239 - val_loss: 0.0241
Epoch 4/100
7493/7493 - 35s - loss: 0.0239 - val_loss: 0.0238
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0240
Epoch 6/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100
7493/7493 - 28s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 13s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 22s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 18s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 24s - loss: 0.0237 - val_loss: 0.0236
Epoch 31/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 19s - loss: 0.0237 - val_loss: 0.0236
Epoch 34/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 36/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0236
Epoch 37/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 00037: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.1
Model: "functional_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_6 (Dense)              (None, 60)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_8 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_9 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_10 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 14s - loss: 0.0249 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 16s - loss: 0.0240 - val_loss: 0.0242
Epoch 3/100
7493/7493 - 16s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 16s - loss: 0.0238 - val_loss: 0.0239
Epoch 5/100
7493/7493 - 16s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 17s - loss: 0.0238 - val_loss: 0.0238
Epoch 7/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 13s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0237
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.1
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_13 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_14 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_15 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_16 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 14s - loss: 0.0249 - val_loss: 0.0246
Epoch 2/100
7493/7493 - 15s - loss: 0.0240 - val_loss: 0.0241
Epoch 3/100
7493/7493 - 16s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 15s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 17s - loss: 0.0238 - val_loss: 0.0239
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 15s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 25s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 13s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 13s - loss: 0.0237 - val_loss: 0.0237
Epoch 30/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.1
Model: "functional_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_18 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_19 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_20 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_21 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_22 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_23 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 16s - loss: 0.0247 - val_loss: 0.0237
Epoch 2/100
7493/7493 - 14s - loss: 0.0240 - val_loss: 0.0244
Epoch 3/100
7493/7493 - 12s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 12s - loss: 0.0239 - val_loss: 0.0242
Epoch 5/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 8/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 9/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.1
Model: "functional_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_24 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_25 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_26 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_27 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_28 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_29 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0248 - val_loss: 0.0245
Epoch 2/100
7493/7493 - 32s - loss: 0.0240 - val_loss: 0.0249
Epoch 3/100
7493/7493 - 41s - loss: 0.0238 - val_loss: 0.0243
Epoch 4/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0257
Epoch 5/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0238
Epoch 7/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0238
Epoch 9/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0237
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 33/100
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0237
Epoch 34/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4153806400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58429022e+00 2.43423228e-04 1.58393240e+00
  1.58457768e+00]
 [1.28750000e+03 1.61251009e+00 4.32400920e-04 1.61186361e+00
  1.61307359e+00]
 [1.35350000e+03 1.64319336e+00 5.08454348e-04 1.64273834e+00
  1.64408660e+00]
 [1.42200000e+03 1.67527754e+00 6.61232649e-04 1.67433214e+00
  1.67611527e+00]
 [1.49300000e+03 1.70745966e+00 6.05839415e-04 1.70677328e+00
  1.70817244e+00]
 [1.56650000e+03 1.74186895e+00 2.38668804e-04 1.74148679e+00
  1.74223351e+00]
 [1.64250000e+03 1.77647662e+00 5.84650538e-04 1.77556157e+00
  1.77715409e+00]
 [1.72100000e+03 1.81311123e+00 6.07070816e-04 1.81238377e+00
  1.81385481e+00]
 [1.80250000e+03 1.85171847e+00 6.74591576e-04 1.85044849e+00
  1.85237908e+00]
 [1.88700000e+03 1.89274018e+00 7.27848788e-04 1.89171028e+00
  1.89359260e+00]
 [1.97450000e+03 1.93651006e+00 1.09207072e-03 1.93438673e+00
  1.93741131e+00]
 [2.06500000e+03 1.98327110e+00 1.40194212e-03 1.98074436e+00
  1.98492527e+00]
 [2.15850000e+03 2.03392873e+00 1.28734045e-03 2.03216052e+00
  2.03554273e+00]
 [2.25550000e+03 2.08851333e+00 1.31769465e-03 2.08736110e+00
  2.09076834e+00]
 [2.35550000e+03 2.14676018e+00 1.01961783e-03 2.14548302e+00
  2.14854622e+00]
 [2.45900000e+03 2.20975714e+00 7.19647773e-04 2.20882463e+00
  2.21084714e+00]
 [2.56600000e+03 2.27838774e+00 1.99191880e-03 2.27571130e+00
  2.28084469e+00]
 [2.67650000e+03 2.35323644e+00 3.22535144e-03 2.34944320e+00
  2.35779762e+00]
 [2.79100000e+03 2.43617320e+00 3.51609563e-03 2.43090940e+00
  2.44125962e+00]
 [2.90900000e+03 2.52697821e+00 4.50991131e-03 2.51835227e+00
  2.53109026e+00]
 [3.03100000e+03 2.62677593e+00 6.44519772e-03 2.61515999e+00
  2.63361144e+00]
 [3.15700000e+03 2.73537736e+00 7.27099169e-03 2.72449660e+00
  2.74488664e+00]
 [3.28700000e+03 2.85581737e+00 4.21380739e-03 2.85138011e+00
  2.86232758e+00]
 [3.42150000e+03 2.98775334e+00 3.37435964e-03 2.98352933e+00
  2.99203372e+00]
 [3.56100000e+03 3.12950854e+00 8.29840089e-03 3.12141562e+00
  3.14017487e+00]
 [3.70500000e+03 3.28241119e+00 1.20686539e-02 3.27003264e+00
  3.30098653e+00]
 [3.85300000e+03 3.45092392e+00 1.38181871e-02 3.43813944e+00
  3.47711754e+00]
 [4.00600000e+03 3.63639646e+00 1.60836635e-02 3.61823869e+00
  3.66599536e+00]
 [4.16450000e+03 3.83264823e+00 2.41308675e-02 3.79493856e+00
  3.87068820e+00]
 [4.32800000e+03 4.03822789e+00 3.26809093e-02 3.98493934e+00
  4.08774519e+00]
 [4.49700000e+03 4.25442486e+00 3.65661934e-02 4.19767666e+00
  4.31288004e+00]
 [4.67150000e+03 4.48547134e+00 3.13354139e-02 4.45502377e+00
  4.54575491e+00]
 [4.85150000e+03 4.73286343e+00 3.56497056e-02 4.69966030e+00
  4.78606844e+00]
 [5.03750000e+03 4.98888292e+00 6.10220620e-02 4.93616772e+00
  5.08696604e+00]
 [5.22950000e+03 5.25326729e+00 9.27572081e-02 5.18041515e+00
  5.41978550e+00]
 [5.45050000e+03 5.55757103e+00 1.31217229e-01 5.46157551e+00
  5.80328083e+00]
 [5.65550000e+03 5.83977156e+00 1.67623671e-01 5.72231674e+00
  6.15922070e+00]
 [5.84400000e+03 6.09914637e+00 2.01412211e-01 5.96140385e+00
  6.48657179e+00]
 [6.06200000e+03 6.39890223e+00 2.40715298e-01 6.23671436e+00
  6.86510611e+00]
 [6.28750000e+03 6.70862799e+00 2.81546322e-01 6.52095604e+00
  7.25648355e+00]
 [6.52000000e+03 7.02745342e+00 3.23787951e-01 6.81329441e+00
  7.65966368e+00]
 [6.76000000e+03 7.35578632e+00 3.67541877e-01 7.11408424e+00
  8.07529545e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917961 events, validating on 1917962

training QR for quantile 0.3
Model: "functional_11"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_30 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_31 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_32 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_33 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_34 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_35 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 44s - loss: 0.0488 - val_loss: 0.0523
Epoch 2/100
7493/7493 - 36s - loss: 0.0472 - val_loss: 0.0477
Epoch 3/100
7493/7493 - 38s - loss: 0.0470 - val_loss: 0.0474
Epoch 4/100
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 26s - loss: 0.0468 - val_loss: 0.0466
Epoch 6/100
7493/7493 - 12s - loss: 0.0468 - val_loss: 0.0466
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 15s - loss: 0.0468 - val_loss: 0.0467
Epoch 8/100
7493/7493 - 13s - loss: 0.0467 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 16s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 19s - loss: 0.0467 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 16s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 14s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 16s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 14s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 14s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 28s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 29s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 21s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 33/100
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0466
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.3
Model: "functional_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_36 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_37 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_38 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_39 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_40 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_41 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 13s - loss: 0.0486 - val_loss: 0.0475
Epoch 2/100
7493/7493 - 12s - loss: 0.0472 - val_loss: 0.0470
Epoch 3/100
7493/7493 - 12s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 12s - loss: 0.0469 - val_loss: 0.0468
Epoch 5/100
7493/7493 - 13s - loss: 0.0468 - val_loss: 0.0468
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 12s - loss: 0.0468 - val_loss: 0.0471
Epoch 7/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 16s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 11s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 26s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 12s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 25s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 14s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 13s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 17s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 16s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 14s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 15s - loss: 0.0466 - val_loss: 0.0466
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.3
Model: "functional_15"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_42 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_43 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_44 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_45 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_46 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_47 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 16s - loss: 0.0485 - val_loss: 0.0473
Epoch 2/100
7493/7493 - 15s - loss: 0.0471 - val_loss: 0.0478
Epoch 3/100
7493/7493 - 12s - loss: 0.0470 - val_loss: 0.0473
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 16s - loss: 0.0469 - val_loss: 0.0477
Epoch 5/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0470
Epoch 6/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 7/100
7493/7493 - 29s - loss: 0.0466 - val_loss: 0.0467
Epoch 8/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 29s - loss: 0.0466 - val_loss: 0.0467
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 26s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.3
Model: "functional_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_48 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_49 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_50 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_51 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_52 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_53 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 41s - loss: 0.0486 - val_loss: 0.0567
Epoch 2/100
7493/7493 - 36s - loss: 0.0472 - val_loss: 0.0470
Epoch 3/100
7493/7493 - 36s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 37s - loss: 0.0469 - val_loss: 0.0471
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 31s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 33s - loss: 0.0467 - val_loss: 0.0465
Epoch 9/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0467 - val_loss: 0.0465
Epoch 11/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 12/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 14/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 15/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4145fc0d08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917962 events, validating on 1917961

training QR for quantile 0.3
Model: "functional_19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_54 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_55 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_56 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_57 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_58 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_59 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0484 - val_loss: 0.0531
Epoch 2/100
7493/7493 - 37s - loss: 0.0470 - val_loss: 0.0487
Epoch 3/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0472
Epoch 5/100
7493/7493 - 40s - loss: 0.0468 - val_loss: 0.0482
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0481
Epoch 7/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0467
Epoch 11/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 28s - loss: 0.0465 - val_loss: 0.0467
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 42s - loss: 0.0465 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 39s - loss: 0.0465 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0465 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 41s - loss: 0.0465 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 42s - loss: 0.0465 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 31s - loss: 0.0465 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 29s - loss: 0.0465 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 20s - loss: 0.0465 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 40s - loss: 0.0465 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 41s - loss: 0.0465 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 28s - loss: 0.0465 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 29s - loss: 0.0465 - val_loss: 0.0466
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4145eb1510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67808628e+00 4.28678336e-04 1.67774975e+00
  1.67889237e+00]
 [1.28750000e+03 1.70597386e+00 1.40575788e-04 1.70581210e+00
  1.70618248e+00]
 [1.35350000e+03 1.73659704e+00 5.13546095e-04 1.73609495e+00
  1.73732376e+00]
 [1.42200000e+03 1.76890321e+00 4.27251712e-04 1.76840127e+00
  1.76942444e+00]
 [1.49300000e+03 1.80239856e+00 4.67732799e-04 1.80192399e+00
  1.80328083e+00]
 [1.56650000e+03 1.83835971e+00 4.76500297e-04 1.83779824e+00
  1.83920574e+00]
 [1.64250000e+03 1.87591956e+00 5.34681406e-04 1.87516022e+00
  1.87672913e+00]
 [1.72100000e+03 1.91636624e+00 4.68686439e-04 1.91552103e+00
  1.91686320e+00]
 [1.80250000e+03 1.96024671e+00 3.65184590e-04 1.95989609e+00
  1.96084356e+00]
 [1.88700000e+03 2.00714645e+00 5.62737097e-04 2.00613666e+00
  2.00761032e+00]
 [1.97450000e+03 2.05782075e+00 9.62195078e-04 2.05613971e+00
  2.05878210e+00]
 [2.06500000e+03 2.11326818e+00 1.21154188e-03 2.11104083e+00
  2.11439800e+00]
 [2.15850000e+03 2.17373524e+00 1.41240211e-03 2.17138457e+00
  2.17571664e+00]
 [2.25550000e+03 2.23945947e+00 1.28580146e-03 2.23771763e+00
  2.24097633e+00]
 [2.35550000e+03 2.31040182e+00 1.36973183e-03 2.30841899e+00
  2.31254220e+00]
 [2.45900000e+03 2.38750644e+00 1.50426233e-03 2.38495469e+00
  2.38941836e+00]
 [2.56600000e+03 2.47144041e+00 1.29008896e-03 2.46901059e+00
  2.47240067e+00]
 [2.67650000e+03 2.56348777e+00 3.19641957e-03 2.55904770e+00
  2.56766796e+00]
 [2.79100000e+03 2.66478658e+00 5.28305656e-03 2.65743923e+00
  2.67213154e+00]
 [2.90900000e+03 2.77742243e+00 3.84527932e-03 2.77394772e+00
  2.78331685e+00]
 [3.03100000e+03 2.90156331e+00 2.96625915e-03 2.89587927e+00
  2.90395522e+00]
 [3.15700000e+03 3.03550515e+00 5.68462714e-03 3.02749991e+00
  3.04214454e+00]
 [3.28700000e+03 3.18025398e+00 7.68673623e-03 3.17050719e+00
  3.18765020e+00]
 [3.42150000e+03 3.33971000e+00 5.73351984e-03 3.32830024e+00
  3.34364486e+00]
 [3.56100000e+03 3.51594334e+00 1.01199733e-02 3.50657558e+00
  3.53417349e+00]
 [3.70500000e+03 3.71291838e+00 1.49594533e-02 3.69332528e+00
  3.73413229e+00]
 [3.85300000e+03 3.92714639e+00 1.80817069e-02 3.90111732e+00
  3.94605899e+00]
 [4.00600000e+03 4.15487680e+00 1.90581794e-02 4.13022900e+00
  4.17865515e+00]
 [4.16450000e+03 4.39630575e+00 2.25398889e-02 4.36350203e+00
  4.42157078e+00]
 [4.32800000e+03 4.64872408e+00 3.19186443e-02 4.60954285e+00
  4.67545366e+00]
 [4.49700000e+03 4.91224985e+00 4.63558880e-02 4.84843349e+00
  4.96766090e+00]
 [4.67150000e+03 5.18825274e+00 6.62708762e-02 5.09540558e+00
  5.28187656e+00]
 [4.85150000e+03 5.47967415e+00 9.69790963e-02 5.35039520e+00
  5.63483334e+00]
 [5.03750000e+03 5.79540615e+00 1.54773303e-01 5.61405182e+00
  6.07122135e+00]
 [5.22950000e+03 6.13595877e+00 2.43616443e-01 5.88633251e+00
  6.59410095e+00]
 [5.45050000e+03 6.52851028e+00 3.48773118e-01 6.19981956e+00
  7.19816542e+00]
 [5.65550000e+03 6.89301558e+00 4.47550488e-01 6.49063110e+00
  7.75993872e+00]
 [5.84400000e+03 7.22820110e+00 5.39015301e-01 6.75801229e+00
  8.27736568e+00]
 [6.06200000e+03 7.61566639e+00 6.45372531e-01 7.06715727e+00
  8.87654591e+00]
 [6.28750000e+03 8.01584978e+00 7.56098017e-01 7.38677168e+00
  9.49705887e+00]
 [6.52000000e+03 8.42729454e+00 8.71154991e-01 7.71602392e+00
  1.01374617e+01]
 [6.76000000e+03 8.85011368e+00 9.91144869e-01 8.05543232e+00
  1.07991199e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917961 events, validating on 1917962

training QR for quantile 0.5
Model: "functional_21"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_11 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_60 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_61 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_62 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_63 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_64 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_65 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0562 - val_loss: 0.0540
Epoch 2/100
7493/7493 - 36s - loss: 0.0546 - val_loss: 0.0543
Epoch 3/100
7493/7493 - 35s - loss: 0.0544 - val_loss: 0.0540
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0541
Epoch 5/100
7493/7493 - 35s - loss: 0.0540 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 35s - loss: 0.0540 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 36s - loss: 0.0540 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 31s - loss: 0.0540 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0539
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.5
Model: "functional_23"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_66 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_67 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_68 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_69 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_70 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_71 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0559 - val_loss: 0.0540
Epoch 2/100
7493/7493 - 35s - loss: 0.0544 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 32s - loss: 0.0542 - val_loss: 0.0540
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 8/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 33/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.5
Model: "functional_25"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_13 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_72 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_73 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_74 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_75 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_76 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_77 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0561 - val_loss: 0.0548
Epoch 2/100
7493/7493 - 41s - loss: 0.0545 - val_loss: 0.0546
Epoch 3/100
7493/7493 - 44s - loss: 0.0543 - val_loss: 0.0543
Epoch 4/100
7493/7493 - 45s - loss: 0.0541 - val_loss: 0.0551
Epoch 5/100
7493/7493 - 37s - loss: 0.0541 - val_loss: 0.0542
Epoch 6/100
7493/7493 - 35s - loss: 0.0540 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 35s - loss: 0.0540 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 42s - loss: 0.0540 - val_loss: 0.0541
Epoch 9/100
7493/7493 - 31s - loss: 0.0540 - val_loss: 0.0540
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0540 - val_loss: 0.0540
Epoch 11/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 33/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.5
Model: "functional_27"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_14 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_78 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_79 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_80 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_81 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_82 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_83 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 39s - loss: 0.0562 - val_loss: 0.0585
Epoch 2/100
7493/7493 - 47s - loss: 0.0546 - val_loss: 0.0560
Epoch 3/100
7493/7493 - 44s - loss: 0.0544 - val_loss: 0.0550
Epoch 4/100
7493/7493 - 40s - loss: 0.0543 - val_loss: 0.0539
Epoch 5/100
7493/7493 - 31s - loss: 0.0542 - val_loss: 0.0538
Epoch 6/100
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0547
Epoch 7/100
7493/7493 - 32s - loss: 0.0541 - val_loss: 0.0539
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0541 - val_loss: 0.0538
Epoch 9/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 27s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 29s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 33/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 34/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 36/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 37/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 00037: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4155630510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917962 events, validating on 1917961

training QR for quantile 0.5
Model: "functional_29"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_84 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_85 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_86 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_87 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_88 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_89 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0561 - val_loss: 0.0542
Epoch 2/100
7493/7493 - 35s - loss: 0.0544 - val_loss: 0.0543
Epoch 3/100
7493/7493 - 33s - loss: 0.0542 - val_loss: 0.0548
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0545
Epoch 5/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0541
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4153f9d1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73766840e+00 3.35568514e-04 1.73723471e+00
  1.73820019e+00]
 [1.28750000e+03 1.76600630e+00 2.53486983e-04 1.76571691e+00
  1.76647496e+00]
 [1.35350000e+03 1.79753458e+00 2.64367418e-04 1.79720902e+00
  1.79794621e+00]
 [1.42200000e+03 1.83146660e+00 5.19304474e-04 1.83062899e+00
  1.83213472e+00]
 [1.49300000e+03 1.86671724e+00 2.40346308e-04 1.86657250e+00
  1.86719561e+00]
 [1.56650000e+03 1.90570571e+00 4.92083988e-04 1.90516937e+00
  1.90638256e+00]
 [1.64250000e+03 1.94614837e+00 7.06380700e-04 1.94558167e+00
  1.94751000e+00]
 [1.72100000e+03 1.99085557e+00 4.69814061e-04 1.98998249e+00
  1.99138713e+00]
 [1.80250000e+03 2.03986282e+00 6.10204031e-04 2.03927541e+00
  2.04061389e+00]
 [1.88700000e+03 2.09277191e+00 5.92483277e-04 2.09181547e+00
  2.09357905e+00]
 [1.97450000e+03 2.14992132e+00 1.15062367e-03 2.14826751e+00
  2.15184116e+00]
 [2.06500000e+03 2.21239343e+00 1.45782539e-03 2.21078277e+00
  2.21461463e+00]
 [2.15850000e+03 2.28098612e+00 1.17894659e-03 2.27946854e+00
  2.28250122e+00]
 [2.25550000e+03 2.35618372e+00 9.81386100e-04 2.35481596e+00
  2.35775709e+00]
 [2.35550000e+03 2.43765216e+00 1.47179123e-03 2.43642974e+00
  2.44021535e+00]
 [2.45900000e+03 2.52661729e+00 2.29238656e-03 2.52367020e+00
  2.52977824e+00]
 [2.56600000e+03 2.62403765e+00 3.21330686e-03 2.61869407e+00
  2.62784529e+00]
 [2.67650000e+03 2.73173280e+00 4.28888343e-03 2.72460961e+00
  2.73736024e+00]
 [2.79100000e+03 2.84955473e+00 5.27711385e-03 2.84393048e+00
  2.85688782e+00]
 [2.90900000e+03 2.97661653e+00 6.28929836e-03 2.96833587e+00
  2.98447728e+00]
 [3.03100000e+03 3.11673660e+00 9.03336610e-03 3.10263777e+00
  3.12675595e+00]
 [3.15700000e+03 3.27167139e+00 1.41481195e-02 3.24675345e+00
  3.28827810e+00]
 [3.28700000e+03 3.44452157e+00 1.44474480e-02 3.42255402e+00
  3.46281195e+00]
 [3.42150000e+03 3.63446693e+00 1.03533172e-02 3.62152410e+00
  3.65282321e+00]
 [3.56100000e+03 3.84098697e+00 9.76168185e-03 3.82690072e+00
  3.85223436e+00]
 [3.70500000e+03 4.06428490e+00 1.57689521e-02 4.03635931e+00
  4.08351707e+00]
 [3.85300000e+03 4.31085930e+00 1.60924863e-02 4.28061152e+00
  4.32858133e+00]
 [4.00600000e+03 4.57530193e+00 1.67799302e-02 4.54403973e+00
  4.59244537e+00]
 [4.16450000e+03 4.85558910e+00 2.30339658e-02 4.81952906e+00
  4.89154243e+00]
 [4.32800000e+03 5.14928198e+00 3.26883477e-02 5.10447121e+00
  5.20524025e+00]
 [4.49700000e+03 5.45805120e+00 4.45331688e-02 5.39932919e+00
  5.53162861e+00]
 [4.67150000e+03 5.78542433e+00 6.40534824e-02 5.70403862e+00
  5.86943245e+00]
 [4.85150000e+03 6.12697268e+00 9.26270421e-02 6.01857615e+00
  6.25380564e+00]
 [5.03750000e+03 6.48034172e+00 1.25120887e-01 6.34384298e+00
  6.67300844e+00]
 [5.22950000e+03 6.84541960e+00 1.60145455e-01 6.67994833e+00
  7.10724592e+00]
 [5.45050000e+03 7.26594572e+00 2.01504482e-01 7.06755447e+00
  7.60871935e+00]
 [5.65550000e+03 7.65640211e+00 2.40477183e-01 7.42843437e+00
  8.07532978e+00]
 [5.84400000e+03 8.01594582e+00 2.76552174e-01 7.76247644e+00
  8.50557041e+00]
 [6.06200000e+03 8.43288174e+00 3.18103500e-01 8.15401459e+00
  9.00457478e+00]
 [6.28750000e+03 8.86664047e+00 3.59953254e-01 8.57071495e+00
  9.52234268e+00]
 [6.52000000e+03 9.31907368e+00 3.99493980e-01 8.97802162e+00
  1.00563011e+01]
 [6.76000000e+03 9.79367237e+00 4.36103474e-01 9.39821529e+00
  1.06066008e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917961 events, validating on 1917962

training QR for quantile 0.7
Model: "functional_31"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_90 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_91 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_92 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_93 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_94 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_95 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0514 - val_loss: 0.0503
Epoch 2/100
7493/7493 - 34s - loss: 0.0485 - val_loss: 0.0490
Epoch 3/100
7493/7493 - 35s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 37s - loss: 0.0481 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0509
Epoch 6/100
7493/7493 - 44s - loss: 0.0480 - val_loss: 0.0495
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 30s - loss: 0.0480 - val_loss: 0.0480
Epoch 8/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 42s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.7
Model: "functional_33"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_17 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_96 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_97 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_98 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_99 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_100 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_101 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0505 - val_loss: 0.0556
Epoch 2/100
7493/7493 - 34s - loss: 0.0484 - val_loss: 0.0490
Epoch 3/100
7493/7493 - 33s - loss: 0.0482 - val_loss: 0.0483
Epoch 4/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 46s - loss: 0.0481 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 37s - loss: 0.0480 - val_loss: 0.0481
Epoch 7/100
7493/7493 - 45s - loss: 0.0480 - val_loss: 0.0483
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0480 - val_loss: 0.0483
Epoch 9/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0479
Epoch 11/100
7493/7493 - 32s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 28s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 34/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0478
Epoch 35/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.7
Model: "functional_35"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_102 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_103 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_104 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_105 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_106 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_107 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0501 - val_loss: 0.0498
Epoch 2/100
7493/7493 - 36s - loss: 0.0483 - val_loss: 0.0479
Epoch 3/100
7493/7493 - 40s - loss: 0.0482 - val_loss: 0.0479
Epoch 4/100
7493/7493 - 34s - loss: 0.0481 - val_loss: 0.0480
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 28s - loss: 0.0478 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 27s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.7
Model: "functional_37"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_19 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_108 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_109 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_110 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_111 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_112 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_113 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0500 - val_loss: 0.0615
Epoch 2/100
7493/7493 - 36s - loss: 0.0485 - val_loss: 0.0504
Epoch 3/100
7493/7493 - 35s - loss: 0.0483 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 31s - loss: 0.0482 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0478
Epoch 6/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0477
Epoch 7/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0478
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0480 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0477
Epoch 10/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 15/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 48s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 26/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 27/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0477
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 29/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0477
Epoch 30/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f414827bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917962 events, validating on 1917961

training QR for quantile 0.7
Model: "functional_39"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_20 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_114 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_115 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_116 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_117 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_118 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_119 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 40s - loss: 0.0498 - val_loss: 0.0481
Epoch 2/100
7493/7493 - 37s - loss: 0.0483 - val_loss: 0.0479
Epoch 3/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0490
Epoch 4/100
7493/7493 - 43s - loss: 0.0480 - val_loss: 0.0479
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 30s - loss: 0.0478 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0479
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0479
Epoch 9/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 46s - loss: 0.0477 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 32s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 28s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 47s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 29s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 44s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 46s - loss: 0.0477 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f414605a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79567349e+00 3.53086019e-04 1.79515517e+00
  1.79625571e+00]
 [1.28750000e+03 1.82488275e+00 1.65188945e-04 1.82460654e+00
  1.82502735e+00]
 [1.35350000e+03 1.85851443e+00 2.21969938e-04 1.85807645e+00
  1.85866439e+00]
 [1.42200000e+03 1.89450622e+00 5.41000683e-04 1.89379227e+00
  1.89514649e+00]
 [1.49300000e+03 1.93341746e+00 3.43162491e-04 1.93294716e+00
  1.93377483e+00]
 [1.56650000e+03 1.97592673e+00 3.02631817e-04 1.97551763e+00
  1.97632372e+00]
 [1.64250000e+03 2.02093587e+00 8.03118561e-04 2.02014232e+00
  2.02194595e+00]
 [1.72100000e+03 2.07087903e+00 6.08810596e-04 2.06990767e+00
  2.07158518e+00]
 [1.80250000e+03 2.12574449e+00 3.44068446e-04 2.12542534e+00
  2.12640810e+00]
 [1.88700000e+03 2.18566408e+00 5.12949150e-04 2.18505025e+00
  2.18646431e+00]
 [1.97450000e+03 2.25116568e+00 8.44393109e-04 2.25007248e+00
  2.25264430e+00]
 [2.06500000e+03 2.32277369e+00 9.80720722e-04 2.32160449e+00
  2.32439232e+00]
 [2.15850000e+03 2.40070682e+00 1.23843269e-03 2.39962220e+00
  2.40232849e+00]
 [2.25550000e+03 2.48652997e+00 1.20900613e-03 2.48537612e+00
  2.48869610e+00]
 [2.35550000e+03 2.58083515e+00 1.66448251e-03 2.57859755e+00
  2.58354020e+00]
 [2.45900000e+03 2.68407674e+00 3.70348357e-03 2.67803860e+00
  2.68877244e+00]
 [2.56600000e+03 2.79601693e+00 5.93723365e-03 2.78659034e+00
  2.80323482e+00]
 [2.67650000e+03 2.91867213e+00 7.13702385e-03 2.90999532e+00
  2.92907882e+00]
 [2.79100000e+03 3.05337706e+00 7.37723917e-03 3.04329300e+00
  3.06542015e+00]
 [2.90900000e+03 3.19984312e+00 6.28075435e-03 3.18963432e+00
  3.20888162e+00]
 [3.03100000e+03 3.36236253e+00 3.68021196e-03 3.35738611e+00
  3.36786485e+00]
 [3.15700000e+03 3.54234676e+00 1.26461139e-02 3.52708006e+00
  3.56065369e+00]
 [3.28700000e+03 3.73764539e+00 1.85892280e-02 3.71866536e+00
  3.76666474e+00]
 [3.42150000e+03 3.95659757e+00 1.67695893e-02 3.93625498e+00
  3.98434615e+00]
 [3.56100000e+03 4.19910917e+00 1.60754352e-02 4.17938948e+00
  4.21851969e+00]
 [3.70500000e+03 4.46391525e+00 2.13341891e-02 4.43579912e+00
  4.49376011e+00]
 [3.85300000e+03 4.75052919e+00 2.81858588e-02 4.71030807e+00
  4.78484297e+00]
 [4.00600000e+03 5.06083107e+00 2.85349816e-02 5.02408552e+00
  5.09455442e+00]
 [4.16450000e+03 5.38737736e+00 2.92019347e-02 5.33857155e+00
  5.41912746e+00]
 [4.32800000e+03 5.72663937e+00 3.23184076e-02 5.66536045e+00
  5.75555277e+00]
 [4.49700000e+03 6.07894793e+00 3.75303248e-02 6.00441170e+00
  6.10429525e+00]
 [4.67150000e+03 6.44390783e+00 4.43160171e-02 6.35536194e+00
  6.46980619e+00]
 [4.85150000e+03 6.82109680e+00 5.22070553e-02 6.71794891e+00
  6.86192989e+00]
 [5.03750000e+03 7.21140089e+00 6.10250330e-02 7.09290361e+00
  7.26758862e+00]
 [5.22950000e+03 7.61462545e+00 7.05833986e-02 7.48008776e+00
  7.68664026e+00]
 [5.45050000e+03 8.07912750e+00 8.20455128e-02 7.92576027e+00
  8.16922283e+00]
 [5.65550000e+03 8.51055412e+00 9.31592043e-02 8.33905792e+00
  8.61699772e+00]
 [5.84400000e+03 8.90800056e+00 1.03917474e-01 8.71893024e+00
  9.02878571e+00]
 [6.06200000e+03 9.36803589e+00 1.16831352e-01 9.15798473e+00
  9.50502968e+00]
 [6.28750000e+03 9.84368725e+00 1.30459857e-01 9.61166000e+00
  9.99763298e+00]
 [6.52000000e+03 1.03338440e+01 1.44780728e-01 1.00788088e+01
  1.05054693e+01]
 [6.76000000e+03 1.08395044e+01 1.59832487e-01 1.05603247e+01
  1.10295916e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917961 events, validating on 1917962

training QR for quantile 0.9
Model: "functional_41"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_21 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_120 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_121 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_122 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_123 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_124 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_125 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0279 - val_loss: 0.0268
Epoch 2/100
7493/7493 - 29s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 45s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0257
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0255 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.9
Model: "functional_43"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_22 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_126 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_127 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_128 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_129 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_130 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_131 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0272 - val_loss: 0.0265
Epoch 2/100
7493/7493 - 34s - loss: 0.0256 - val_loss: 0.0255
Epoch 3/100
7493/7493 - 39s - loss: 0.0255 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 40s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 29s - loss: 0.0254 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0254
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 28s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 35/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 37/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 00037: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.9
Model: "functional_45"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_132 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_133 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_134 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_135 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_136 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_137 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0274 - val_loss: 0.0274
Epoch 2/100
7493/7493 - 45s - loss: 0.0258 - val_loss: 0.0258
Epoch 3/100
7493/7493 - 28s - loss: 0.0256 - val_loss: 0.0253
Epoch 4/100
7493/7493 - 44s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0255
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 26s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.9
Model: "functional_47"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_24 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_138 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_139 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_140 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_141 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_142 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_143 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0282 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 28s - loss: 0.0257 - val_loss: 0.0278
Epoch 3/100
7493/7493 - 39s - loss: 0.0256 - val_loss: 0.0258
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0255 - val_loss: 0.0258
Epoch 5/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 27s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 28s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 12/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 29s - loss: 0.0253 - val_loss: 0.0252
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 28s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 18/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0252
Epoch 19/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0252
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0252
Epoch 21/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 22/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0252
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 24/100
7493/7493 - 27s - loss: 0.0253 - val_loss: 0.0252
Epoch 25/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0252
Epoch 27/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0252
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f415371c620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917962 events, validating on 1917961

training QR for quantile 0.9
Model: "functional_49"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_25 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_144 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_145 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_146 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_147 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_148 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_149 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0275 - val_loss: 0.0269
Epoch 2/100
7493/7493 - 35s - loss: 0.0257 - val_loss: 0.0284
Epoch 3/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0260
Epoch 4/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0257
Epoch 5/100
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 30s - loss: 0.0254 - val_loss: 0.0254
Epoch 7/100
7493/7493 - 34s - loss: 0.0254 - val_loss: 0.0257
Epoch 8/100
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0257
Epoch 10/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 31s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 28s - loss: 0.0252 - val_loss: 0.0253
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4148de1e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88112941e+00 2.35609957e-04 1.88070011e+00
  1.88140237e+00]
 [1.28750000e+03 1.91358833e+00 3.76705834e-04 1.91309249e+00
  1.91423762e+00]
 [1.35350000e+03 1.95135567e+00 3.37219717e-04 1.95102441e+00
  1.95200479e+00]
 [1.42200000e+03 1.99147737e+00 6.06273953e-04 1.99082398e+00
  1.99225461e+00]
 [1.49300000e+03 2.03740125e+00 6.27948108e-04 2.03669977e+00
  2.03846192e+00]
 [1.56650000e+03 2.08706131e+00 4.77311390e-04 2.08656001e+00
  2.08794665e+00]
 [1.64250000e+03 2.14021573e+00 1.29332887e-03 2.13800335e+00
  2.14196229e+00]
 [1.72100000e+03 2.20012031e+00 9.32179577e-04 2.19843793e+00
  2.20129776e+00]
 [1.80250000e+03 2.26680293e+00 9.11431085e-04 2.26551723e+00
  2.26820540e+00]
 [1.88700000e+03 2.33906460e+00 9.41709880e-04 2.33796525e+00
  2.34066176e+00]
 [1.97450000e+03 2.41752295e+00 1.53377935e-03 2.41582823e+00
  2.42003679e+00]
 [2.06500000e+03 2.50427475e+00 1.22420681e-03 2.50300956e+00
  2.50636816e+00]
 [2.15850000e+03 2.59950395e+00 1.87524836e-03 2.59738517e+00
  2.60238004e+00]
 [2.25550000e+03 2.70377526e+00 1.91460517e-03 2.70157051e+00
  2.70694304e+00]
 [2.35550000e+03 2.81813941e+00 1.08157411e-03 2.81613088e+00
  2.81930518e+00]
 [2.45900000e+03 2.94272914e+00 3.60101103e-03 2.93841410e+00
  2.94838810e+00]
 [2.56600000e+03 3.07903342e+00 6.62967279e-03 3.07163453e+00
  3.08838010e+00]
 [2.67650000e+03 3.23051033e+00 8.43954598e-03 3.21718597e+00
  3.24045467e+00]
 [2.79100000e+03 3.39808073e+00 1.00634501e-02 3.37910533e+00
  3.40675807e+00]
 [2.90900000e+03 3.58170462e+00 7.84426116e-03 3.56786370e+00
  3.58934999e+00]
 [3.03100000e+03 3.78655400e+00 7.06813116e-03 3.77753854e+00
  3.79875016e+00]
 [3.15700000e+03 4.01407046e+00 1.53070100e-02 3.99357533e+00
  4.03380919e+00]
 [3.28700000e+03 4.25908813e+00 2.31363266e-02 4.23058462e+00
  4.28463650e+00]
 [3.42150000e+03 4.52478676e+00 2.55503610e-02 4.48086166e+00
  4.55172157e+00]
 [3.56100000e+03 4.81076365e+00 2.56514826e-02 4.76253939e+00
  4.83826733e+00]
 [3.70500000e+03 5.12309523e+00 1.93826618e-02 5.09209108e+00
  5.15216017e+00]
 [3.85300000e+03 5.46504602e+00 2.59037777e-02 5.43884563e+00
  5.50957203e+00]
 [4.00600000e+03 5.84784880e+00 3.68966904e-02 5.80131006e+00
  5.91226149e+00]
 [4.16450000e+03 6.25679655e+00 5.92637858e-02 6.18013430e+00
  6.34715319e+00]
 [4.32800000e+03 6.68213768e+00 8.77852860e-02 6.57388735e+00
  6.80495930e+00]
 [4.49700000e+03 7.12357130e+00 1.18039456e-01 6.98396158e+00
  7.27992916e+00]
 [4.67150000e+03 7.58096180e+00 1.49285898e-01 7.41095161e+00
  7.77125978e+00]
 [4.85150000e+03 8.05402460e+00 1.81652081e-01 7.85385752e+00
  8.27855682e+00]
 [5.03750000e+03 8.54410534e+00 2.15096969e-01 8.31433582e+00
  8.80718422e+00]
 [5.22950000e+03 9.05138645e+00 2.49024854e-01 8.79463768e+00
  9.36619186e+00]
 [5.45050000e+03 9.63724651e+00 2.85250712e-01 9.35973072e+00
  1.00104504e+01]
 [5.65550000e+03 1.01845852e+01 3.13550665e-01 9.90858746e+00
  1.06085482e+01]
 [5.84400000e+03 1.06940237e+01 3.32689050e-01 1.03875027e+01
  1.11589088e+01]
 [6.06200000e+03 1.12820898e+01 3.53076965e-01 1.09269896e+01
  1.17958946e+01]
 [6.28750000e+03 1.18847832e+01 3.73570738e-01 1.14850130e+01
  1.24554062e+01]
 [6.52000000e+03 1.25053314e+01 3.99106859e-01 1.20602942e+01
  1.31360483e+01]
 [6.76000000e+03 1.31459373e+01 4.29997643e-01 1.26540251e+01
  1.38385010e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917961 events, validating on 1917962

training QR for quantile 0.99
Model: "functional_51"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_26 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_150 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_151 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_152 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_153 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_154 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_155 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0063 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 48s - loss: 0.0044 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 45s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 37s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 00021: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_10_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917961

training QR for quantile 0.99
Model: "functional_53"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_27 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_156 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_157 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_158 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_159 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_160 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_161 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0059 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 32s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 33s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_10_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917961 events, validating on 1917962

training QR for quantile 0.99
Model: "functional_55"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_28 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_162 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_163 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_164 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_165 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_166 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_167 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0058 - val_loss: 0.0044
Epoch 2/100
7493/7493 - 33s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 28s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_10_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
training on 1917962 events, validating on 1917962

training QR for quantile 0.99
Model: "functional_57"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_29 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_168 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_169 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_170 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_171 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_172 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_173 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0054 - val_loss: 0.0050
Epoch 2/100
7493/7493 - 36s - loss: 0.0045 - val_loss: 0.0047
Epoch 3/100
7493/7493 - 43s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0046
Epoch 5/100
7493/7493 - 40s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 31s - loss: 0.0044 - val_loss: 0.0044
Epoch 7/100
7493/7493 - 31s - loss: 0.0044 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 24s - loss: 0.0044 - val_loss: 0.0044
Epoch 9/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_10_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41537c68c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917962 events, validating on 1917961

training QR for quantile 0.99
Model: "functional_59"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_30 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_174 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_175 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_176 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_177 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_178 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_179 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0057 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 32s - loss: 0.0044 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 32s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 29s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_10_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41537c60d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.99: 
[[1.22750000e+03 2.01592255e+00 6.38255148e-04 2.01496482e+00
  2.01662636e+00]
 [1.28750000e+03 2.05644174e+00 1.03464088e-03 2.05530238e+00
  2.05814147e+00]
 [1.35350000e+03 2.10466013e+00 7.34422040e-04 2.10387826e+00
  2.10588312e+00]
 [1.42200000e+03 2.15483418e+00 9.78774187e-04 2.15318894e+00
  2.15575242e+00]
 [1.49300000e+03 2.21213999e+00 1.29559525e-03 2.21048975e+00
  2.21440244e+00]
 [1.56650000e+03 2.27602592e+00 1.60094499e-03 2.27378035e+00
  2.27842569e+00]
 [1.64250000e+03 2.34645929e+00 3.17650027e-03 2.34356093e+00
  2.35106015e+00]
 [1.72100000e+03 2.42363629e+00 2.22156242e-03 2.42149687e+00
  2.42695570e+00]
 [1.80250000e+03 2.50990262e+00 1.67333199e-03 2.50766468e+00
  2.51226377e+00]
 [1.88700000e+03 2.60477438e+00 2.48061691e-03 2.60155487e+00
  2.60897017e+00]
 [1.97450000e+03 2.70860248e+00 2.30457832e-03 2.70501423e+00
  2.71204376e+00]
 [2.06500000e+03 2.82204995e+00 3.43191604e-03 2.81761742e+00
  2.82698178e+00]
 [2.15850000e+03 2.94741101e+00 6.44807916e-03 2.93873334e+00
  2.95596743e+00]
 [2.25550000e+03 3.08845301e+00 6.82644224e-03 3.07819200e+00
  3.09607553e+00]
 [2.35550000e+03 3.24422979e+00 4.47068455e-03 3.23885655e+00
  3.25102353e+00]
 [2.45900000e+03 3.41488109e+00 6.01383508e-03 3.40832782e+00
  3.42413545e+00]
 [2.56600000e+03 3.59926834e+00 9.42014504e-03 3.58898354e+00
  3.61469579e+00]
 [2.67650000e+03 3.79536300e+00 1.18482140e-02 3.78348517e+00
  3.81568980e+00]
 [2.79100000e+03 4.00649109e+00 1.43444102e-02 3.99271822e+00
  4.03033543e+00]
 [2.90900000e+03 4.23895540e+00 1.90065673e-02 4.21640635e+00
  4.26546001e+00]
 [3.03100000e+03 4.49791346e+00 2.81511110e-02 4.45832729e+00
  4.52958488e+00]
 [3.15700000e+03 4.79140205e+00 3.62969199e-02 4.72254753e+00
  4.82322884e+00]
 [3.28700000e+03 5.11613550e+00 5.45770771e-02 5.00836182e+00
  5.15288210e+00]
 [3.42150000e+03 5.46241322e+00 7.55019888e-02 5.32239914e+00
  5.53598404e+00]
 [3.56100000e+03 5.83610783e+00 8.35147970e-02 5.69632101e+00
  5.93694592e+00]
 [3.70500000e+03 6.24281206e+00 7.43620628e-02 6.14727640e+00
  6.35261869e+00]
 [3.85300000e+03 6.67921238e+00 6.85698954e-02 6.58929825e+00
  6.78097439e+00]
 [4.00600000e+03 7.15102129e+00 8.75676033e-02 7.01647758e+00
  7.26488924e+00]
 [4.16450000e+03 7.65419216e+00 1.15296229e-01 7.52382231e+00
  7.85896826e+00]
 [4.32800000e+03 8.18178806e+00 1.53251575e-01 8.03715038e+00
  8.47360325e+00]
 [4.49700000e+03 8.73182220e+00 2.00777681e-01 8.50949478e+00
  9.11034584e+00]
 [4.67150000e+03 9.30409813e+00 2.52593057e-01 8.99757195e+00
  9.76893425e+00]
 [4.85150000e+03 9.89257145e+00 3.08509843e-01 9.50131607e+00
  1.04488688e+01]
 [5.03750000e+03 1.05010195e+01 3.67445843e-01 1.00220728e+01
  1.11518221e+01]
 [5.22950000e+03 1.11294426e+01 4.29131431e-01 1.05598011e+01
  1.18778553e+01]
 [5.45050000e+03 1.18532152e+01 5.01022272e-01 1.11789227e+01
  1.27141981e+01]
 [5.65550000e+03 1.25252747e+01 5.68433726e-01 1.17533665e+01
  1.34908190e+01]
 [5.84400000e+03 1.31440928e+01 6.31089218e-01 1.22816944e+01
  1.42059326e+01]
 [6.06200000e+03 1.38611483e+01 7.04546719e-01 1.28928699e+01
  1.50345840e+01]
 [6.28750000e+03 1.46048862e+01 7.81773171e-01 1.35253239e+01
  1.58934221e+01]
 [6.52000000e+03 1.53744707e+01 8.62817434e-01 1.41778078e+01
  1.67798519e+01]
 [6.76000000e+03 1.61735804e+01 9.48960780e-01 1.48519926e+01
  1.76961746e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.1
Model: "functional_61"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_31 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_180 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_181 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_182 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_183 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_184 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_185 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0249 - val_loss: 0.0237
Epoch 2/100
7493/7493 - 34s - loss: 0.0240 - val_loss: 0.0242
Epoch 3/100
7493/7493 - 38s - loss: 0.0239 - val_loss: 0.0240
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0240
Epoch 5/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 00022: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_20_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.1
Model: "functional_63"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_32 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_186 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_187 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_188 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_189 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_190 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_191 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 47s - loss: 0.0247 - val_loss: 0.0244
Epoch 2/100
7493/7493 - 36s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 37s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0245
Epoch 6/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 32/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 34/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 35/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 37/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0237
Epoch 00037: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_20_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.1
Model: "functional_65"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_33 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_192 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_193 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_194 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_195 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_196 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_197 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0249 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 37s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 37s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0238
Epoch 9/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_20_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.1
Model: "functional_67"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_34 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_198 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_199 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_200 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_201 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_202 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_203 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 43s - loss: 0.0250 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 37s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 33s - loss: 0.0239 - val_loss: 0.0240
Epoch 4/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0236
Epoch 6/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0238
Epoch 7/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0236
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0238 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 31/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 34/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_20_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4152d58ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.1
Model: "functional_69"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_35 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_204 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_205 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_206 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_207 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_208 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_209 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0249 - val_loss: 0.0239
Epoch 2/100
7493/7493 - 34s - loss: 0.0240 - val_loss: 0.0239
Epoch 3/100
7493/7493 - 34s - loss: 0.0239 - val_loss: 0.0240
Epoch 4/100
7493/7493 - 29s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100
7493/7493 - 28s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0238
Epoch 9/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0238
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 40s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 42s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 32/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0237
Epoch 33/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 35/100
7493/7493 - 41s - loss: 0.0236 - val_loss: 0.0237
Epoch 36/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 37/100

Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0237
Epoch 38/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 39/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 40/100

Epoch 00040: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 41/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 42/100
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0237
Epoch 43/100

Epoch 00043: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0237
Epoch 44/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 00044: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_20_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f415030f268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58445156e+00 3.80517419e-04 1.58389521e+00
  1.58491623e+00]
 [1.28750000e+03 1.61232750e+00 3.57604126e-04 1.61169887e+00
  1.61276805e+00]
 [1.35350000e+03 1.64304731e+00 3.08610131e-04 1.64267087e+00
  1.64343977e+00]
 [1.42200000e+03 1.67537723e+00 5.10092961e-04 1.67478228e+00
  1.67601049e+00]
 [1.49300000e+03 1.70748868e+00 4.31130962e-04 1.70672500e+00
  1.70796108e+00]
 [1.56650000e+03 1.74191811e+00 4.48888093e-04 1.74138236e+00
  1.74247539e+00]
 [1.64250000e+03 1.77664919e+00 3.90281242e-04 1.77626169e+00
  1.77719903e+00]
 [1.72100000e+03 1.81297200e+00 4.99607794e-04 1.81239712e+00
  1.81354713e+00]
 [1.80250000e+03 1.85174935e+00 4.78337322e-04 1.85105515e+00
  1.85240293e+00]
 [1.88700000e+03 1.89299662e+00 4.53111336e-04 1.89235163e+00
  1.89368761e+00]
 [1.97450000e+03 1.93670578e+00 3.62358221e-04 1.93601394e+00
  1.93703115e+00]
 [2.06500000e+03 1.98347423e+00 8.62355207e-04 1.98244810e+00
  1.98464119e+00]
 [2.15850000e+03 2.03370347e+00 1.76321861e-03 2.03185606e+00
  2.03667068e+00]
 [2.25550000e+03 2.08777175e+00 1.82993623e-03 2.08528113e+00
  2.09059143e+00]
 [2.35550000e+03 2.14613767e+00 1.06058283e-03 2.14464331e+00
  2.14706612e+00]
 [2.45900000e+03 2.21004558e+00 1.02911039e-03 2.20887136e+00
  2.21146059e+00]
 [2.56600000e+03 2.27942019e+00 1.76873481e-03 2.27734780e+00
  2.28154945e+00]
 [2.67650000e+03 2.35477376e+00 2.51030163e-03 2.35171366e+00
  2.35735726e+00]
 [2.79100000e+03 2.43687119e+00 3.33318841e-03 2.43187213e+00
  2.44197011e+00]
 [2.90900000e+03 2.52625990e+00 4.64839470e-03 2.51854014e+00
  2.53317475e+00]
 [3.03100000e+03 2.62568259e+00 5.56676530e-03 2.61541867e+00
  2.63164687e+00]
 [3.15700000e+03 2.73465505e+00 5.49455910e-03 2.72532344e+00
  2.73993778e+00]
 [3.28700000e+03 2.85514064e+00 6.03207333e-03 2.84808087e+00
  2.86620069e+00]
 [3.42150000e+03 2.98971648e+00 1.02172629e-02 2.97913432e+00
  3.00388122e+00]
 [3.56100000e+03 3.13745060e+00 1.81043478e-02 3.11259007e+00
  3.15749383e+00]
 [3.70500000e+03 3.29674115e+00 2.29547302e-02 3.26294541e+00
  3.32324314e+00]
 [3.85300000e+03 3.46890502e+00 2.04609929e-02 3.44431925e+00
  3.49594712e+00]
 [4.00600000e+03 3.65090418e+00 1.78419305e-02 3.62180805e+00
  3.67631745e+00]
 [4.16450000e+03 3.84135222e+00 1.81745133e-02 3.80985975e+00
  3.86467123e+00]
 [4.32800000e+03 4.03970404e+00 2.09707857e-02 4.01004744e+00
  4.06636238e+00]
 [4.49700000e+03 4.24720440e+00 2.51364671e-02 4.22186804e+00
  4.28869820e+00]
 [4.67150000e+03 4.46544037e+00 3.23335608e-02 4.42362928e+00
  4.51853514e+00]
 [4.85150000e+03 4.69708576e+00 5.02842357e-02 4.63192511e+00
  4.75578499e+00]
 [5.03750000e+03 4.93920317e+00 7.88678546e-02 4.84726715e+00
  5.05792856e+00]
 [5.22950000e+03 5.19031496e+00 1.12969786e-01 5.06961155e+00
  5.37872553e+00]
 [5.45050000e+03 5.48131580e+00 1.57010318e-01 5.32556629e+00
  5.75785255e+00]
 [5.65550000e+03 5.75097847e+00 1.98155616e-01 5.56298208e+00
  6.10839891e+00]
 [5.84400000e+03 5.99701872e+00 2.32865620e-01 5.78125668e+00
  6.42134333e+00]
 [6.06200000e+03 6.27822170e+00 2.67067685e-01 6.03362274e+00
  6.76654816e+00]
 [6.28750000e+03 6.56337795e+00 2.91783064e-01 6.29455948e+00
  7.09388733e+00]
 [6.52000000e+03 6.84776869e+00 2.98733264e-01 6.56343222e+00
  7.37805033e+00]
 [6.76000000e+03 7.12426271e+00 2.74908373e-01 6.84072828e+00
  7.57096148e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.3
Model: "functional_71"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_36 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_210 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_211 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_212 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_213 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_214 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_215 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0487 - val_loss: 0.0474
Epoch 2/100
7493/7493 - 45s - loss: 0.0472 - val_loss: 0.0471
Epoch 3/100
7493/7493 - 48s - loss: 0.0470 - val_loss: 0.0473
Epoch 4/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0468
Epoch 5/100
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 44s - loss: 0.0468 - val_loss: 0.0471
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0468 - val_loss: 0.0468
Epoch 9/100
7493/7493 - 32s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 47s - loss: 0.0467 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0466
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_20_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.3
Model: "functional_73"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_37 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_216 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_217 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_218 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_219 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_220 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_221 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0485 - val_loss: 0.0469
Epoch 2/100
7493/7493 - 32s - loss: 0.0471 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 32s - loss: 0.0469 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 29s - loss: 0.0468 - val_loss: 0.0468
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0468 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 24s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 33/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_20_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.3
Model: "functional_75"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_38 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_222 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_223 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_224 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_225 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_226 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_227 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 41s - loss: 0.0485 - val_loss: 0.0467
Epoch 2/100
7493/7493 - 36s - loss: 0.0471 - val_loss: 0.0472
Epoch 3/100
7493/7493 - 33s - loss: 0.0470 - val_loss: 0.0477
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0478
Epoch 5/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0468
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 00021: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_20_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.3
Model: "functional_77"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_39 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_228 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_229 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_230 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_231 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_232 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_233 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0489 - val_loss: 0.0469
Epoch 2/100
7493/7493 - 41s - loss: 0.0472 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 46s - loss: 0.0470 - val_loss: 0.0466
Epoch 4/100
7493/7493 - 41s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0466
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 39s - loss: 0.0469 - val_loss: 0.0468
Epoch 7/100
7493/7493 - 47s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 33s - loss: 0.0467 - val_loss: 0.0465
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0467 - val_loss: 0.0465
Epoch 10/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 11/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 13/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 14/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0465
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 29s - loss: 0.0466 - val_loss: 0.0465
Epoch 31/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0465
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_20_loss_rk5_05_20210922_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4164c9b6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.3
Model: "functional_79"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_40 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_234 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_235 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_236 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_237 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_238 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_239 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0483 - val_loss: 0.0470
Epoch 2/100
7493/7493 - 46s - loss: 0.0470 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 34s - loss: 0.0468 - val_loss: 0.0469
Epoch 4/100
7493/7493 - 41s - loss: 0.0468 - val_loss: 0.0468
Epoch 5/100
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0469
Epoch 6/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0474
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0469
Epoch 8/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0467
Epoch 11/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0467
Epoch 12/100
7493/7493 - 40s - loss: 0.0465 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0467
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 42s - loss: 0.0465 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 30s - loss: 0.0465 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 32s - loss: 0.0465 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 42s - loss: 0.0465 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 43s - loss: 0.0465 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 31/100
7493/7493 - 41s - loss: 0.0465 - val_loss: 0.0466
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_20_loss_rk5_05_20210922_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4145e27268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67787924e+00 1.51935135e-04 1.67759585e+00
  1.67803800e+00]
 [1.28750000e+03 1.70599043e+00 1.68594019e-04 1.70573485e+00
  1.70624459e+00]
 [1.35350000e+03 1.73658879e+00 4.24117513e-04 1.73617017e+00
  1.73715234e+00]
 [1.42200000e+03 1.76872578e+00 4.34346583e-04 1.76820958e+00
  1.76923621e+00]
 [1.49300000e+03 1.80209880e+00 3.10209579e-04 1.80178130e+00
  1.80266988e+00]
 [1.56650000e+03 1.83826373e+00 4.39437273e-04 1.83758771e+00
  1.83882713e+00]
 [1.64250000e+03 1.87604241e+00 4.12363074e-04 1.87543786e+00
  1.87660050e+00]
 [1.72100000e+03 1.91623194e+00 6.70170057e-04 1.91491628e+00
  1.91670966e+00]
 [1.80250000e+03 1.95999715e+00 7.07402161e-04 1.95887291e+00
  1.96085918e+00]
 [1.88700000e+03 2.00760422e+00 8.92625515e-04 2.00609159e+00
  2.00858021e+00]
 [1.97450000e+03 2.05820622e+00 1.13869613e-03 2.05595613e+00
  2.05894232e+00]
 [2.06500000e+03 2.11286211e+00 1.13724714e-03 2.11085749e+00
  2.11429548e+00]
 [2.15850000e+03 2.17324915e+00 1.17637565e-03 2.17125154e+00
  2.17451215e+00]
 [2.25550000e+03 2.23968930e+00 8.79730485e-04 2.23818684e+00
  2.24080634e+00]
 [2.35550000e+03 2.31084523e+00 8.89061144e-04 2.30962443e+00
  2.31202650e+00]
 [2.45900000e+03 2.38750305e+00 9.31450898e-04 2.38616681e+00
  2.38874006e+00]
 [2.56600000e+03 2.47091827e+00 1.05472260e-03 2.46894145e+00
  2.47172689e+00]
 [2.67650000e+03 2.56255403e+00 2.99055235e-03 2.55774522e+00
  2.56575680e+00]
 [2.79100000e+03 2.66422863e+00 5.18306762e-03 2.65722752e+00
  2.66993880e+00]
 [2.90900000e+03 2.77753463e+00 6.01103200e-03 2.76866984e+00
  2.78339410e+00]
 [3.03100000e+03 2.90163298e+00 5.12128808e-03 2.89482522e+00
  2.90897846e+00]
 [3.15700000e+03 3.03608708e+00 5.05001280e-03 3.03055406e+00
  3.04402733e+00]
 [3.28700000e+03 3.17978449e+00 9.01402880e-03 3.16881752e+00
  3.19291806e+00]
 [3.42150000e+03 3.33629823e+00 1.13683775e-02 3.32221937e+00
  3.35587478e+00]
 [3.56100000e+03 3.51470776e+00 9.39416523e-03 3.50438547e+00
  3.52876163e+00]
 [3.70500000e+03 3.71061506e+00 1.52350921e-02 3.68361807e+00
  3.72741318e+00]
 [3.85300000e+03 3.92299981e+00 2.13334169e-02 3.88832736e+00
  3.94329333e+00]
 [4.00600000e+03 4.15725842e+00 1.77837781e-02 4.13062477e+00
  4.17829132e+00]
 [4.16450000e+03 4.40784245e+00 2.25522022e-02 4.36755419e+00
  4.43158579e+00]
 [4.32800000e+03 4.66950865e+00 3.73624715e-02 4.61298180e+00
  4.71541977e+00]
 [4.49700000e+03 4.94503546e+00 5.80034221e-02 4.86726379e+00
  5.01739168e+00]
 [4.67150000e+03 5.23534555e+00 8.56462338e-02 5.13020372e+00
  5.34069824e+00]
 [4.85150000e+03 5.53572330e+00 1.15473992e-01 5.40169096e+00
  5.69016790e+00]
 [5.03750000e+03 5.84671497e+00 1.46903626e-01 5.68240547e+00
  6.05235147e+00]
 [5.22950000e+03 6.16820030e+00 1.79733631e-01 5.97229385e+00
  6.42701006e+00]
 [5.45050000e+03 6.53863955e+00 2.17862180e-01 6.30604887e+00
  6.85892773e+00]
 [5.65550000e+03 6.88249216e+00 2.53486102e-01 6.61567831e+00
  7.25996304e+00]
 [5.84400000e+03 7.19877481e+00 2.86406620e-01 6.90039682e+00
  7.62887764e+00]
 [6.06200000e+03 7.56464539e+00 3.24611751e-01 7.22966433e+00
  8.05552197e+00]
 [6.28750000e+03 7.94309740e+00 3.64146271e-01 7.57024145e+00
  8.49658012e+00]
 [6.52000000e+03 8.33313808e+00 4.04773217e-01 7.92135096e+00
  8.95065784e+00]
 [6.76000000e+03 8.73543377e+00 4.46367110e-01 8.28374577e+00
  9.41800976e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.5
Model: "functional_81"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_41 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_240 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_241 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_242 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_243 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_244 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_245 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0562 - val_loss: 0.0549
Epoch 2/100
7493/7493 - 33s - loss: 0.0546 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 37s - loss: 0.0544 - val_loss: 0.0543
Epoch 4/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 31s - loss: 0.0542 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 44s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 36s - loss: 0.0540 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_20_loss_rk5_05_20210922_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.5
Model: "functional_83"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_42 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_246 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_247 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_248 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_249 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_250 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_251 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0564 - val_loss: 0.0546
Epoch 2/100
7493/7493 - 34s - loss: 0.0544 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 34s - loss: 0.0543 - val_loss: 0.0544
Epoch 4/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0541
Epoch 5/100
7493/7493 - 37s - loss: 0.0542 - val_loss: 0.0538
Epoch 6/100
7493/7493 - 47s - loss: 0.0541 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 32s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 43s - loss: 0.0541 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0540
Epoch 12/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_20_loss_rk5_05_20210922_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.5
Model: "functional_85"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_43 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_252 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_253 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_254 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_255 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_256 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_257 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0561 - val_loss: 0.0565
Epoch 2/100
7493/7493 - 30s - loss: 0.0545 - val_loss: 0.0541
Epoch 3/100
7493/7493 - 29s - loss: 0.0542 - val_loss: 0.0540
Epoch 4/100
7493/7493 - 30s - loss: 0.0541 - val_loss: 0.0546
Epoch 5/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 47s - loss: 0.0541 - val_loss: 0.0546
Epoch 7/100
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 40s - loss: 0.0540 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_20_loss_rk5_05_20210922_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.5
Model: "functional_87"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_44 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_258 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_259 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_260 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_261 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_262 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_263 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0568 - val_loss: 0.0549
Epoch 2/100
7493/7493 - 47s - loss: 0.0545 - val_loss: 0.0564
Epoch 3/100
7493/7493 - 36s - loss: 0.0543 - val_loss: 0.0540
Epoch 4/100
7493/7493 - 44s - loss: 0.0542 - val_loss: 0.0538
Epoch 5/100
7493/7493 - 32s - loss: 0.0542 - val_loss: 0.0541
Epoch 6/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0538
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0541 - val_loss: 0.0539
Epoch 8/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 9/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0538
Epoch 11/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_20_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41539c79d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.5
Model: "functional_89"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_45 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_264 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_265 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_266 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_267 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_268 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_269 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 47s - loss: 0.0561 - val_loss: 0.0546
Epoch 2/100
7493/7493 - 45s - loss: 0.0544 - val_loss: 0.0544
Epoch 3/100
7493/7493 - 37s - loss: 0.0542 - val_loss: 0.0547
Epoch 4/100
7493/7493 - 45s - loss: 0.0541 - val_loss: 0.0541
Epoch 5/100
7493/7493 - 35s - loss: 0.0540 - val_loss: 0.0540
Epoch 6/100
7493/7493 - 41s - loss: 0.0540 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 47s - loss: 0.0540 - val_loss: 0.0543
Epoch 8/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0541
Epoch 10/100
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 46s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_20_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f415371c400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73783350e+00 4.13959039e-04 1.73726094e+00
  1.73853064e+00]
 [1.28750000e+03 1.76617565e+00 1.86170433e-04 1.76594186e+00
  1.76648748e+00]
 [1.35350000e+03 1.79733515e+00 3.21726487e-04 1.79694057e+00
  1.79784727e+00]
 [1.42200000e+03 1.83150499e+00 4.52044673e-04 1.83081210e+00
  1.83214164e+00]
 [1.49300000e+03 1.86694319e+00 8.82065584e-04 1.86618984e+00
  1.86862934e+00]
 [1.56650000e+03 1.90555470e+00 4.86331946e-04 1.90478146e+00
  1.90627241e+00]
 [1.64250000e+03 1.94647951e+00 3.04860129e-04 1.94590938e+00
  1.94681621e+00]
 [1.72100000e+03 1.99060893e+00 6.64313311e-04 1.98974144e+00
  1.99133885e+00]
 [1.80250000e+03 2.03955140e+00 2.75581005e-04 2.03907156e+00
  2.03985167e+00]
 [1.88700000e+03 2.09272122e+00 6.78690671e-04 2.09159636e+00
  2.09370470e+00]
 [1.97450000e+03 2.15028343e+00 1.18056690e-03 2.14847016e+00
  2.15204811e+00]
 [2.06500000e+03 2.21255670e+00 1.32941011e-03 2.21040583e+00
  2.21440125e+00]
 [2.15850000e+03 2.28087869e+00 1.38660009e-03 2.27866149e+00
  2.28290296e+00]
 [2.25550000e+03 2.35592394e+00 1.28889839e-03 2.35407710e+00
  2.35744834e+00]
 [2.35550000e+03 2.43753586e+00 1.54658137e-03 2.43638229e+00
  2.44059014e+00]
 [2.45900000e+03 2.52657466e+00 2.69674694e-03 2.52294850e+00
  2.53124666e+00]
 [2.56600000e+03 2.62416377e+00 3.71318011e-03 2.61814380e+00
  2.62941313e+00]
 [2.67650000e+03 2.73132701e+00 4.30524757e-03 2.72447395e+00
  2.73713589e+00]
 [2.79100000e+03 2.84943185e+00 3.87927444e-03 2.84485865e+00
  2.85623622e+00]
 [2.90900000e+03 2.97800202e+00 3.41946738e-03 2.97264433e+00
  2.98240066e+00]
 [3.03100000e+03 3.11918802e+00 7.95106110e-03 3.10766387e+00
  3.12876654e+00]
 [3.15700000e+03 3.27314754e+00 1.42409031e-02 3.25732636e+00
  3.29208374e+00]
 [3.28700000e+03 3.44251895e+00 1.56558371e-02 3.42681265e+00
  3.46545410e+00]
 [3.42150000e+03 3.63110738e+00 1.21611140e-02 3.61755371e+00
  3.64948058e+00]
 [3.56100000e+03 3.83857560e+00 1.24886260e-02 3.82207775e+00
  3.85616779e+00]
 [3.70500000e+03 4.06359053e+00 2.32765657e-02 4.02313280e+00
  4.08877897e+00]
 [3.85300000e+03 4.31117001e+00 3.34989967e-02 4.24910831e+00
  4.34402847e+00]
 [4.00600000e+03 4.58175135e+00 3.47337928e-02 4.52134848e+00
  4.62005329e+00]
 [4.16450000e+03 4.86707897e+00 3.73444277e-02 4.81007528e+00
  4.91120434e+00]
 [4.32800000e+03 5.16389704e+00 4.25784869e-02 5.10873127e+00
  5.21662569e+00]
 [4.49700000e+03 5.47201052e+00 4.94974165e-02 5.41802216e+00
  5.53447294e+00]
 [4.67150000e+03 5.79102163e+00 5.76033297e-02 5.73780966e+00
  5.86385584e+00]
 [4.85150000e+03 6.12070551e+00 6.66295505e-02 6.05995560e+00
  6.20447159e+00]
 [5.03750000e+03 6.46180897e+00 7.64283652e-02 6.38440800e+00
  6.55704451e+00]
 [5.22950000e+03 6.81419315e+00 8.69262368e-02 6.71915245e+00
  6.92142582e+00]
 [5.45050000e+03 7.21999254e+00 9.93996827e-02 7.10407543e+00
  7.34121418e+00]
 [5.65550000e+03 7.59648771e+00 1.11291728e-01 7.46065855e+00
  7.73085022e+00]
 [5.84400000e+03 7.94267044e+00 1.22483634e-01 7.78803444e+00
  8.08925343e+00]
 [6.06200000e+03 8.34293194e+00 1.35734172e-01 8.16588783e+00
  8.50380230e+00]
 [6.28750000e+03 8.75675812e+00 1.49840650e-01 8.55563164e+00
  8.93263912e+00]
 [6.52000000e+03 9.18305855e+00 1.64935218e-01 8.95586109e+00
  9.37477589e+00]
 [6.76000000e+03 9.62241859e+00 1.81383041e-01 9.36653519e+00
  9.83114624e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.7
Model: "functional_91"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_46 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_270 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_271 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_272 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_273 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_274 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_275 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0503 - val_loss: 0.0496
Epoch 2/100
7493/7493 - 43s - loss: 0.0484 - val_loss: 0.0480
Epoch 3/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0483
Epoch 4/100
7493/7493 - 46s - loss: 0.0481 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0480
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0479
Epoch 9/100
7493/7493 - 44s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0480
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_20_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.7
Model: "functional_93"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_47 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_276 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_277 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_278 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_279 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_280 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_281 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0507 - val_loss: 0.0484
Epoch 2/100
7493/7493 - 35s - loss: 0.0484 - val_loss: 0.0483
Epoch 3/100
7493/7493 - 34s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0489
Epoch 6/100
7493/7493 - 31s - loss: 0.0481 - val_loss: 0.0483
Epoch 7/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 32s - loss: 0.0480 - val_loss: 0.0482
Epoch 9/100
7493/7493 - 45s - loss: 0.0480 - val_loss: 0.0482
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0480 - val_loss: 0.0480
Epoch 11/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 35/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_20_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.7
Model: "functional_95"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_48 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_282 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_283 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_284 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_285 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_286 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_287 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 38s - loss: 0.0502 - val_loss: 0.0498
Epoch 2/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0491
Epoch 3/100
7493/7493 - 33s - loss: 0.0482 - val_loss: 0.0481
Epoch 4/100
7493/7493 - 30s - loss: 0.0481 - val_loss: 0.0483
Epoch 5/100
7493/7493 - 40s - loss: 0.0480 - val_loss: 0.0482
Epoch 6/100
7493/7493 - 43s - loss: 0.0480 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0482
Epoch 8/100
7493/7493 - 34s - loss: 0.0480 - val_loss: 0.0481
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0484
Epoch 10/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0479
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0479
Epoch 13/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_20_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.7
Model: "functional_97"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_49 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_288 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_289 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_290 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_291 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_292 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_293 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0508 - val_loss: 0.0500
Epoch 2/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0481
Epoch 3/100
7493/7493 - 45s - loss: 0.0483 - val_loss: 0.0485
Epoch 4/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0481
Epoch 6/100
7493/7493 - 34s - loss: 0.0481 - val_loss: 0.0479
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0481 - val_loss: 0.0483
Epoch 8/100
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0477
Epoch 9/100
7493/7493 - 41s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 28s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100
7493/7493 - 30s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0477
Epoch 25/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0477
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 27/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 28/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 30/100
7493/7493 - 28s - loss: 0.0478 - val_loss: 0.0477
Epoch 31/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0477
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_20_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4155185f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.7
Model: "functional_99"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_50 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_294 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_295 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_296 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_297 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_298 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_299 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0509 - val_loss: 0.0484
Epoch 2/100
7493/7493 - 32s - loss: 0.0483 - val_loss: 0.0479
Epoch 3/100
7493/7493 - 32s - loss: 0.0481 - val_loss: 0.0488
Epoch 4/100
7493/7493 - 34s - loss: 0.0480 - val_loss: 0.0488
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0479
Epoch 9/100
7493/7493 - 43s - loss: 0.0477 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 39s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 38s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 47s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 46s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 43s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_20_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4154f861e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79570940e+00 3.68353907e-04 1.79503775e+00
  1.79612863e+00]
 [1.28750000e+03 1.82495065e+00 3.34570649e-04 1.82445228e+00
  1.82535529e+00]
 [1.35350000e+03 1.85842817e+00 3.25656727e-04 1.85796678e+00
  1.85884154e+00]
 [1.42200000e+03 1.89431612e+00 5.79171959e-04 1.89331579e+00
  1.89487779e+00]
 [1.49300000e+03 1.93350041e+00 3.40265689e-04 1.93305397e+00
  1.93402338e+00]
 [1.56650000e+03 1.97620289e+00 4.23303258e-04 1.97569454e+00
  1.97680819e+00]
 [1.64250000e+03 2.02108645e+00 6.83555605e-04 2.02008820e+00
  2.02219725e+00]
 [1.72100000e+03 2.07048225e+00 6.95603470e-04 2.06944680e+00
  2.07121968e+00]
 [1.80250000e+03 2.12613401e+00 4.22015395e-04 2.12559867e+00
  2.12666750e+00]
 [1.88700000e+03 2.18579335e+00 4.99838382e-04 2.18509483e+00
  2.18643832e+00]
 [1.97450000e+03 2.25087914e+00 6.98350274e-04 2.24982691e+00
  2.25164461e+00]
 [2.06500000e+03 2.32247109e+00 1.07385106e-03 2.32102942e+00
  2.32433963e+00]
 [2.15850000e+03 2.40089703e+00 1.48228799e-03 2.39993834e+00
  2.40381432e+00]
 [2.25550000e+03 2.48710828e+00 1.65579728e-03 2.48493838e+00
  2.48968458e+00]
 [2.35550000e+03 2.58065104e+00 2.38235564e-03 2.57761478e+00
  2.58430958e+00]
 [2.45900000e+03 2.68267074e+00 3.40237081e-03 2.67816997e+00
  2.68653464e+00]
 [2.56600000e+03 2.79516292e+00 4.31415964e-03 2.78877020e+00
  2.80119896e+00]
 [2.67650000e+03 2.91816072e+00 5.89654566e-03 2.91146159e+00
  2.92811799e+00]
 [2.79100000e+03 3.05359325e+00 8.08869893e-03 3.04420066e+00
  3.06486702e+00]
 [2.90900000e+03 3.20194407e+00 1.07700459e-02 3.18531132e+00
  3.21637106e+00]
 [3.03100000e+03 3.36701217e+00 9.08905989e-03 3.35805058e+00
  3.38434696e+00]
 [3.15700000e+03 3.54482818e+00 1.17108877e-02 3.52737188e+00
  3.56374598e+00]
 [3.28700000e+03 3.73856039e+00 1.03006866e-02 3.72502136e+00
  3.75409722e+00]
 [3.42150000e+03 3.95078263e+00 6.40722830e-03 3.94003105e+00
  3.95751143e+00]
 [3.56100000e+03 4.18520336e+00 1.14363622e-02 4.17084980e+00
  4.20338440e+00]
 [3.70500000e+03 4.44621325e+00 1.90247140e-02 4.41919947e+00
  4.47021770e+00]
 [3.85300000e+03 4.73466711e+00 2.43527753e-02 4.69115162e+00
  4.76010180e+00]
 [4.00600000e+03 5.04833536e+00 2.92310616e-02 5.00155258e+00
  5.07599020e+00]
 [4.16450000e+03 5.38515339e+00 3.02798538e-02 5.33749199e+00
  5.42557478e+00]
 [4.32800000e+03 5.73632622e+00 4.40373465e-02 5.65837860e+00
  5.78820515e+00]
 [4.49700000e+03 6.10244617e+00 6.36953220e-02 5.99037695e+00
  6.16462183e+00]
 [4.67150000e+03 6.48486233e+00 8.55926829e-02 6.33338165e+00
  6.57830667e+00]
 [4.85150000e+03 6.88304586e+00 1.09863228e-01 6.68730736e+00
  7.00861216e+00]
 [5.03750000e+03 7.29493294e+00 1.36279278e-01 7.05309534e+00
  7.45288849e+00]
 [5.22950000e+03 7.72006807e+00 1.63963022e-01 7.43072081e+00
  7.90944910e+00]
 [5.45050000e+03 8.20796757e+00 1.94583574e-01 7.86540890e+00
  8.42640686e+00]
 [5.65550000e+03 8.65730953e+00 2.19880121e-01 8.26863384e+00
  8.88895512e+00]
 [5.84400000e+03 9.06604061e+00 2.39345209e-01 8.63940525e+00
  9.29127979e+00]
 [6.06200000e+03 9.51830654e+00 2.49235007e-01 9.06819916e+00
  9.76206779e+00]
 [6.28750000e+03 9.97103214e+00 2.70473055e-01 9.51173973e+00
  1.03019943e+01]
 [6.52000000e+03 1.04373598e+01 3.15788874e-01 9.96904182e+00
  1.08611841e+01]
 [6.76000000e+03 1.09179142e+01 3.79279162e-01 1.04410887e+01
  1.14396753e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.9
Model: "functional_101"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_51 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_300 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_301 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_302 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_303 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_304 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_305 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0279 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 43s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 48s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 45s - loss: 0.0255 - val_loss: 0.0261
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0255 - val_loss: 0.0261
Epoch 6/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_20_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.9
Model: "functional_103"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_52 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_306 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_307 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_308 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_309 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_310 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_311 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0277 - val_loss: 0.0256
Epoch 2/100
7493/7493 - 45s - loss: 0.0258 - val_loss: 0.0256
Epoch 3/100
7493/7493 - 33s - loss: 0.0256 - val_loss: 0.0257
Epoch 4/100
7493/7493 - 32s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7493/7493 - 34s - loss: 0.0255 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 33s - loss: 0.0254 - val_loss: 0.0254
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0254 - val_loss: 0.0254
Epoch 8/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 48s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 35/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 36/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 37/100

Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 38/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 39/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 40/100

Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 00040: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_20_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.9
Model: "functional_105"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_53 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_312 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_313 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_314 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_315 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_316 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_317 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 48s - loss: 0.0275 - val_loss: 0.0254
Epoch 2/100
7493/7493 - 34s - loss: 0.0257 - val_loss: 0.0265
Epoch 3/100
7493/7493 - 35s - loss: 0.0256 - val_loss: 0.0263
Epoch 4/100
7493/7493 - 39s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7493/7493 - 38s - loss: 0.0255 - val_loss: 0.0254
Epoch 6/100
7493/7493 - 36s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0254 - val_loss: 0.0254
Epoch 8/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_20_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.9
Model: "functional_107"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_54 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_318 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_319 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_320 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_321 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_322 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_323 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0276 - val_loss: 0.0256
Epoch 2/100
7493/7493 - 33s - loss: 0.0257 - val_loss: 0.0257
Epoch 3/100
7493/7493 - 34s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 46s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 33s - loss: 0.0255 - val_loss: 0.0260
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0260
Epoch 7/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_20_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4154329158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.9
Model: "functional_109"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_55 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_324 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_325 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_326 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_327 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_328 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_329 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0270 - val_loss: 0.0253
Epoch 2/100
7493/7493 - 32s - loss: 0.0257 - val_loss: 0.0253
Epoch 3/100
7493/7493 - 43s - loss: 0.0255 - val_loss: 0.0253
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0261
Epoch 5/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 31s - loss: 0.0252 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 31s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_20_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41538529d8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88121321e+00 3.96012308e-04 1.88062251e+00
  1.88167238e+00]
 [1.28750000e+03 1.91374085e+00 3.47080670e-04 1.91318023e+00
  1.91424155e+00]
 [1.35350000e+03 1.95144188e+00 2.92756891e-04 1.95097029e+00
  1.95175445e+00]
 [1.42200000e+03 1.99117432e+00 4.57796195e-04 1.99041677e+00
  1.99172080e+00]
 [1.49300000e+03 2.03763094e+00 7.91250750e-04 2.03647184e+00
  2.03878570e+00]
 [1.56650000e+03 2.08673253e+00 1.05192158e-03 2.08470035e+00
  2.08758497e+00]
 [1.64250000e+03 2.14047775e+00 8.96397201e-04 2.13926959e+00
  2.14186835e+00]
 [1.72100000e+03 2.20011234e+00 4.61059147e-04 2.19941449e+00
  2.20059323e+00]
 [1.80250000e+03 2.26633625e+00 7.31882773e-04 2.26548409e+00
  2.26762342e+00]
 [1.88700000e+03 2.33869247e+00 1.12175891e-03 2.33748460e+00
  2.34034896e+00]
 [1.97450000e+03 2.41810126e+00 1.14834696e-03 2.41663337e+00
  2.41954327e+00]
 [2.06500000e+03 2.50445685e+00 1.41045471e-03 2.50237751e+00
  2.50674772e+00]
 [2.15850000e+03 2.59850359e+00 1.62980222e-03 2.59697127e+00
  2.60153294e+00]
 [2.25550000e+03 2.70334234e+00 1.41651859e-03 2.70172048e+00
  2.70590138e+00]
 [2.35550000e+03 2.81847262e+00 1.57256655e-03 2.81563091e+00
  2.81989455e+00]
 [2.45900000e+03 2.94330649e+00 4.05566042e-03 2.93763542e+00
  2.94692421e+00]
 [2.56600000e+03 3.07977133e+00 6.98999630e-03 3.07061577e+00
  3.08671141e+00]
 [2.67650000e+03 3.23089204e+00 1.00532909e-02 3.21524096e+00
  3.24218750e+00]
 [2.79100000e+03 3.39965525e+00 1.17172398e-02 3.38040113e+00
  3.41524863e+00]
 [2.90900000e+03 3.58578844e+00 1.02364792e-02 3.57460618e+00
  3.60414481e+00]
 [3.03100000e+03 3.78928590e+00 1.14130410e-02 3.77770281e+00
  3.81021738e+00]
 [3.15700000e+03 4.01191273e+00 1.36648851e-02 3.99240255e+00
  4.03171301e+00]
 [3.28700000e+03 4.25373259e+00 1.30367817e-02 4.23387527e+00
  4.26968336e+00]
 [3.42150000e+03 4.52177200e+00 8.44675004e-03 4.50931835e+00
  4.53479433e+00]
 [3.56100000e+03 4.82002201e+00 2.04510190e-02 4.78705215e+00
  4.84850550e+00]
 [3.70500000e+03 5.14252739e+00 3.86259470e-02 5.08689547e+00
  5.19428015e+00]
 [3.85300000e+03 5.48746071e+00 5.52702836e-02 5.40488768e+00
  5.55453491e+00]
 [4.00600000e+03 5.86219454e+00 7.10700395e-02 5.74807596e+00
  5.94415760e+00]
 [4.16450000e+03 6.26414557e+00 8.25897516e-02 6.13883591e+00
  6.37958574e+00]
 [4.32800000e+03 6.68439894e+00 9.18410770e-02 6.56093979e+00
  6.83198595e+00]
 [4.49700000e+03 7.12092857e+00 1.03374511e-01 7.00127792e+00
  7.30249739e+00]
 [4.67150000e+03 7.57326145e+00 1.16804427e-01 7.45967340e+00
  7.78999615e+00]
 [4.85150000e+03 8.04075861e+00 1.31925224e-01 7.93463326e+00
  8.29344463e+00]
 [5.03750000e+03 8.52456398e+00 1.48440062e-01 8.42079449e+00
  8.81385231e+00]
 [5.22950000e+03 9.02443428e+00 1.65973872e-01 8.90150261e+00
  9.35048389e+00]
 [5.45050000e+03 9.59943256e+00 1.85939196e-01 9.45479107e+00
  9.96536446e+00]
 [5.65550000e+03 1.01312483e+01 2.01998876e-01 9.96789265e+00
  1.05277748e+01]
 [5.84400000e+03 1.06167400e+01 2.10455887e-01 1.04395075e+01
  1.10275450e+01]
 [6.06200000e+03 1.11699539e+01 2.05126724e-01 1.09846239e+01
  1.15648460e+01]
 [6.28750000e+03 1.17371147e+01 1.91551364e-01 1.15480738e+01
  1.20962648e+01]
 [6.52000000e+03 1.23182037e+01 1.73462431e-01 1.21284790e+01
  1.26274099e+01]
 [6.76000000e+03 1.29175390e+01 1.58097059e-01 1.27269382e+01
  1.31755676e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1917983 events, validating on 1917984

training QR for quantile 0.99
Model: "functional_111"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_56 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_330 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_331 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_332 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_333 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_334 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_335 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0054 - val_loss: 0.0047
Epoch 2/100
7493/7493 - 43s - loss: 0.0044 - val_loss: 0.0045
Epoch 3/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 26s - loss: 0.0043 - val_loss: 0.0044
Epoch 7/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 28s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 27s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 28s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 28s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_20_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917983

training QR for quantile 0.99
Model: "functional_113"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_57 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_336 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_337 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_338 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_339 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_340 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_341 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 30s - loss: 0.0057 - val_loss: 0.0044
Epoch 2/100
7493/7493 - 34s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 30s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_20_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917983 events, validating on 1917984

training QR for quantile 0.99
Model: "functional_115"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_58 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_342 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_343 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_344 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_345 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_346 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_347 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 30s - loss: 0.0057 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 36s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 48s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_20_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
training on 1917984 events, validating on 1917984

training QR for quantile 0.99
Model: "functional_117"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_59 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_348 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_349 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_350 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_351 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_352 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_353 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 42s - loss: 0.0058 - val_loss: 0.0050
Epoch 2/100
7493/7493 - 35s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 45s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 46s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 47s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 42s - loss: 0.0044 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 26s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 33/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_20_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4152d99f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1917984 events, validating on 1917983

training QR for quantile 0.99
Model: "functional_119"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_60 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_354 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_355 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_356 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_357 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_358 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_359 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0062 - val_loss: 0.0045
Epoch 2/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 46s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0044 - val_loss: 0.0045
Epoch 6/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0044
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 00022: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_20_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f414b2461e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.99: 
[[1.22750000e+03 2.01617751e+00 5.51693598e-04 2.01530266e+00
  2.01683187e+00]
 [1.28750000e+03 2.05638528e+00 9.78154753e-04 2.05521035e+00
  2.05785704e+00]
 [1.35350000e+03 2.10464087e+00 5.85837203e-04 2.10371900e+00
  2.10535169e+00]
 [1.42200000e+03 2.15442610e+00 8.84257515e-04 2.15315390e+00
  2.15538359e+00]
 [1.49300000e+03 2.21220937e+00 1.62941562e-03 2.21060824e+00
  2.21522880e+00]
 [1.56650000e+03 2.27615399e+00 1.71038856e-03 2.27379751e+00
  2.27872229e+00]
 [1.64250000e+03 2.34622216e+00 2.66331647e-03 2.34349751e+00
  2.35033607e+00]
 [1.72100000e+03 2.42265592e+00 3.97760135e-03 2.41615248e+00
  2.42752242e+00]
 [1.80250000e+03 2.50962119e+00 1.26943461e-03 2.50793481e+00
  2.51164103e+00]
 [1.88700000e+03 2.60475845e+00 3.17482804e-03 2.60010576e+00
  2.60831261e+00]
 [1.97450000e+03 2.70810127e+00 2.79178541e-03 2.70365334e+00
  2.71202779e+00]
 [2.06500000e+03 2.82175469e+00 3.40141287e-03 2.81702232e+00
  2.82583237e+00]
 [2.15850000e+03 2.94748712e+00 6.33490515e-03 2.93739223e+00
  2.95680332e+00]
 [2.25550000e+03 3.08760653e+00 7.96965107e-03 3.07487369e+00
  3.09810233e+00]
 [2.35550000e+03 3.24223819e+00 8.61989283e-03 3.22938609e+00
  3.25321746e+00]
 [2.45900000e+03 3.41108890e+00 8.91592016e-03 3.40192389e+00
  3.42583036e+00]
 [2.56600000e+03 3.59389281e+00 9.50221065e-03 3.58372498e+00
  3.61015940e+00]
 [2.67650000e+03 3.79179902e+00 1.25470468e-02 3.77338839e+00
  3.81023598e+00]
 [2.79100000e+03 4.00840616e+00 2.02790575e-02 3.97671413e+00
  4.03346586e+00]
 [2.90900000e+03 4.24125137e+00 2.93490260e-02 4.19731712e+00
  4.27132320e+00]
 [3.03100000e+03 4.49840832e+00 3.56400285e-02 4.44295168e+00
  4.53364992e+00]
 [3.15700000e+03 4.78689184e+00 3.59765179e-02 4.72528934e+00
  4.82695436e+00]
 [3.28700000e+03 5.11090202e+00 3.42287905e-02 5.05176258e+00
  5.15041590e+00]
 [3.42150000e+03 5.46510534e+00 4.43205616e-02 5.42714071e+00
  5.54718351e+00]
 [3.56100000e+03 5.85686798e+00 6.13712625e-02 5.76896763e+00
  5.95993805e+00]
 [3.70500000e+03 6.27504072e+00 8.56742778e-02 6.12624121e+00
  6.38692570e+00]
 [3.85300000e+03 6.72181911e+00 9.88617596e-02 6.55239058e+00
  6.82670403e+00]
 [4.00600000e+03 7.19365826e+00 1.10294589e-01 7.01765871e+00
  7.33590937e+00]
 [4.16450000e+03 7.69191084e+00 1.19566225e-01 7.52550316e+00
  7.88006210e+00]
 [4.32800000e+03 8.21410294e+00 1.28706762e-01 8.07792568e+00
  8.44430542e+00]
 [4.49700000e+03 8.75852280e+00 1.45513350e-01 8.65010929e+00
  9.02954483e+00]
 [4.67150000e+03 9.32224998e+00 1.67850888e-01 9.18937206e+00
  9.63578796e+00]
 [4.85150000e+03 9.90288486e+00 1.95707302e-01 9.73057365e+00
  1.02636995e+01]
 [5.03750000e+03 1.04989002e+01 2.32399939e-01 1.02671766e+01
  1.09166155e+01]
 [5.22950000e+03 1.10994644e+01 2.91543541e-01 1.07467833e+01
  1.15977411e+01]
 [5.45050000e+03 1.17577269e+01 4.11691597e-01 1.11414089e+01
  1.23930254e+01]
 [5.65550000e+03 1.23665762e+01 5.33988523e-01 1.14991541e+01
  1.31318779e+01]
 [5.84400000e+03 1.29270430e+01 6.49341619e-01 1.18294153e+01
  1.38114815e+01]
 [6.06200000e+03 1.35763336e+01 7.84303601e-01 1.22135210e+01
  1.45978479e+01]
 [6.28750000e+03 1.42501284e+01 9.24056846e-01 1.26157427e+01
  1.54119968e+01]
 [6.52000000e+03 1.49476519e+01 1.06831935e+00 1.30350704e+01
  1.62526913e+01]
 [6.76000000e+03 1.56694616e+01 1.21713697e+00 1.34719973e+01
  1.71227226e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_121"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_61 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_360 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_361 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_362 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_363 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_364 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_365 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0247 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 46s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 34s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.1
Model: "functional_123"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_62 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_366 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_367 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_368 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_369 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_370 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_371 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0250 - val_loss: 0.0242
Epoch 2/100
7493/7493 - 31s - loss: 0.0240 - val_loss: 0.0239
Epoch 3/100
7493/7493 - 35s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 46s - loss: 0.0238 - val_loss: 0.0240
Epoch 5/100
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 28s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 31s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_125"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_63 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_372 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_373 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_374 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_375 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_376 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_377 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0249 - val_loss: 0.0241
Epoch 2/100
7493/7493 - 34s - loss: 0.0240 - val_loss: 0.0242
Epoch 3/100
7493/7493 - 34s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 32s - loss: 0.0239 - val_loss: 0.0239
Epoch 5/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 45s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 29s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 29/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_127"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_64 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_378 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_379 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_380 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_381 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_382 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_383 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0247 - val_loss: 0.0252
Epoch 2/100
7493/7493 - 39s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 39s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7493/7493 - 39s - loss: 0.0239 - val_loss: 0.0238
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0236
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0238
Epoch 10/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 29s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 27s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f40e43282f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.1
Model: "functional_129"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_65 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_384 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_385 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_386 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_387 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_388 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_389 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0250 - val_loss: 0.0238
Epoch 2/100
7493/7493 - 33s - loss: 0.0240 - val_loss: 0.0239
Epoch 3/100
7493/7493 - 34s - loss: 0.0239 - val_loss: 0.0240
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 30s - loss: 0.0238 - val_loss: 0.0239
Epoch 5/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 29s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 31s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 27s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 33/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 34/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0237
Epoch 36/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4155391378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58451512e+00 3.37122964e-04 1.58412361e+00
  1.58496618e+00]
 [1.28750000e+03 1.61233699e+00 3.41822480e-04 1.61196566e+00
  1.61288798e+00]
 [1.35350000e+03 1.64321518e+00 4.68149175e-04 1.64261329e+00
  1.64397001e+00]
 [1.42200000e+03 1.67527852e+00 4.05697340e-04 1.67490196e+00
  1.67603993e+00]
 [1.49300000e+03 1.70721898e+00 6.37890240e-04 1.70595050e+00
  1.70765185e+00]
 [1.56650000e+03 1.74208801e+00 4.45015406e-04 1.74141622e+00
  1.74270976e+00]
 [1.64250000e+03 1.77670538e+00 6.44607926e-04 1.77607501e+00
  1.77759385e+00]
 [1.72100000e+03 1.81303482e+00 7.11977278e-04 1.81218064e+00
  1.81378520e+00]
 [1.80250000e+03 1.85170581e+00 7.47666966e-04 1.85074568e+00
  1.85255277e+00]
 [1.88700000e+03 1.89277241e+00 8.23737710e-04 1.89130485e+00
  1.89366317e+00]
 [1.97450000e+03 1.93666372e+00 7.34001923e-04 1.93568265e+00
  1.93766201e+00]
 [2.06500000e+03 1.98376153e+00 9.46949979e-04 1.98262882e+00
  1.98504031e+00]
 [2.15850000e+03 2.03410745e+00 1.03826830e-03 2.03264666e+00
  2.03559089e+00]
 [2.25550000e+03 2.08802385e+00 1.10356985e-03 2.08637524e+00
  2.08968782e+00]
 [2.35550000e+03 2.14625578e+00 1.16058292e-03 2.14409804e+00
  2.14753795e+00]
 [2.45900000e+03 2.21010852e+00 1.68621912e-03 2.20740247e+00
  2.21268082e+00]
 [2.56600000e+03 2.27915263e+00 1.72367872e-03 2.27739620e+00
  2.28240585e+00]
 [2.67650000e+03 2.35414557e+00 1.86029032e-03 2.35137749e+00
  2.35638475e+00]
 [2.79100000e+03 2.43686938e+00 4.08544789e-03 2.43154883e+00
  2.44244576e+00]
 [2.90900000e+03 2.52752347e+00 6.15024566e-03 2.51922846e+00
  2.53620934e+00]
 [3.03100000e+03 2.62662477e+00 6.97538409e-03 2.61599874e+00
  2.63622499e+00]
 [3.15700000e+03 2.73537064e+00 6.14059246e-03 2.72400951e+00
  2.74224830e+00]
 [3.28700000e+03 2.85500622e+00 7.08888260e-03 2.84732962e+00
  2.86834502e+00]
 [3.42150000e+03 2.98769031e+00 1.13695528e-02 2.97545958e+00
  3.00806189e+00]
 [3.56100000e+03 3.13473797e+00 1.87384984e-02 3.10676432e+00
  3.15741444e+00]
 [3.70500000e+03 3.29235382e+00 2.63034642e-02 3.25081348e+00
  3.32092857e+00]
 [3.85300000e+03 3.46015053e+00 3.03543564e-02 3.41284585e+00
  3.49799013e+00]
 [4.00600000e+03 3.64220490e+00 2.71132133e-02 3.60452771e+00
  3.68366385e+00]
 [4.16450000e+03 3.83946857e+00 2.04623977e-02 3.82377672e+00
  3.87881160e+00]
 [4.32800000e+03 4.04593248e+00 2.60439355e-02 4.00932455e+00
  4.08167934e+00]
 [4.49700000e+03 4.26082850e+00 4.12008726e-02 4.19967270e+00
  4.31846142e+00]
 [4.67150000e+03 4.48416157e+00 5.95876169e-02 4.39644289e+00
  4.57726288e+00]
 [4.85150000e+03 4.71533766e+00 7.96171147e-02 4.59955835e+00
  4.84501123e+00]
 [5.03750000e+03 4.95459232e+00 1.00904426e-01 4.80953836e+00
  5.12223101e+00]
 [5.22950000e+03 5.20176525e+00 1.23217920e-01 5.02634907e+00
  5.40875435e+00]
 [5.45050000e+03 5.48626471e+00 1.49143676e-01 5.27592659e+00
  5.73880196e+00]
 [5.65550000e+03 5.75004644e+00 1.73335718e-01 5.50742292e+00
  6.04505682e+00]
 [5.84400000e+03 5.99238997e+00 1.95670108e-01 5.72025919e+00
  6.32666922e+00]
 [6.06200000e+03 6.27228355e+00 2.21583729e-01 5.96635628e+00
  6.65224981e+00]
 [6.28750000e+03 6.56123219e+00 2.48492813e-01 6.22085333e+00
  6.98882866e+00]
 [6.52000000e+03 6.85831432e+00 2.76409455e-01 6.48316050e+00
  7.33557987e+00]
 [6.76000000e+03 7.16370916e+00 3.05556136e-01 6.75382423e+00
  7.69316673e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_131"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_66 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_390 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_391 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_392 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_393 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_394 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_395 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 38s - loss: 0.0485 - val_loss: 0.0483
Epoch 2/100
7493/7493 - 35s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 32s - loss: 0.0470 - val_loss: 0.0471
Epoch 4/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0469
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0466
Epoch 7/100
7493/7493 - 34s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.3
Model: "functional_133"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_67 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_396 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_397 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_398 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_399 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_400 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_401 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0488 - val_loss: 0.0470
Epoch 2/100
7493/7493 - 36s - loss: 0.0471 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 34s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 39s - loss: 0.0469 - val_loss: 0.0466
Epoch 5/100
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 45s - loss: 0.0468 - val_loss: 0.0468
Epoch 8/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 33/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 35/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 36/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 37/100

Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 38/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 00038: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_135"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_68 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_402 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_403 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_404 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_405 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_406 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_407 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0487 - val_loss: 0.0471
Epoch 2/100
7493/7493 - 33s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 36s - loss: 0.0470 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 37s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0468 - val_loss: 0.0469
Epoch 6/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 7/100
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0468
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_137"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_69 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_408 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_409 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_410 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_411 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_412 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_413 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0487 - val_loss: 0.0469
Epoch 2/100
7493/7493 - 34s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 31s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 32s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 30s - loss: 0.0469 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0465
Epoch 7/100
7493/7493 - 41s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0466
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 11/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 13/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 14/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0465
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 31/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 32/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 34/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 35/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f415370c378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.3
Model: "functional_139"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_70 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_414 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_415 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_416 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_417 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_418 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_419 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0485 - val_loss: 0.0491
Epoch 2/100
7493/7493 - 44s - loss: 0.0471 - val_loss: 0.0470
Epoch 3/100
7493/7493 - 37s - loss: 0.0469 - val_loss: 0.0474
Epoch 4/100
7493/7493 - 39s - loss: 0.0468 - val_loss: 0.0468
Epoch 5/100
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0468
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0467
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0467
Epoch 13/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 46s - loss: 0.0465 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0465 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 41s - loss: 0.0465 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0465 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 44s - loss: 0.0465 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 47s - loss: 0.0465 - val_loss: 0.0466
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 40s - loss: 0.0465 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 44s - loss: 0.0465 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 39s - loss: 0.0465 - val_loss: 0.0466
Epoch 31/100
7493/7493 - 48s - loss: 0.0465 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 44s - loss: 0.0465 - val_loss: 0.0466
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41539546a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67804527e+00 2.04041205e-04 1.67788708e+00
  1.67844784e+00]
 [1.28750000e+03 1.70587184e+00 2.83726949e-04 1.70548928e+00
  1.70622122e+00]
 [1.35350000e+03 1.73671584e+00 4.96074442e-04 1.73616028e+00
  1.73760009e+00]
 [1.42200000e+03 1.76873727e+00 3.20101450e-04 1.76833463e+00
  1.76918364e+00]
 [1.49300000e+03 1.80243466e+00 4.52334238e-04 1.80173111e+00
  1.80315018e+00]
 [1.56650000e+03 1.83834090e+00 6.49463992e-04 1.83739674e+00
  1.83916390e+00]
 [1.64250000e+03 1.87580466e+00 5.80765338e-04 1.87477088e+00
  1.87645257e+00]
 [1.72100000e+03 1.91633327e+00 5.49122909e-04 1.91538870e+00
  1.91689062e+00]
 [1.80250000e+03 1.96033430e+00 5.92523223e-04 1.95937669e+00
  1.96082401e+00]
 [1.88700000e+03 2.00716887e+00 7.01165744e-04 2.00590324e+00
  2.00792742e+00]
 [1.97450000e+03 2.05774150e+00 7.52070441e-04 2.05652833e+00
  2.05860925e+00]
 [2.06500000e+03 2.11331997e+00 9.21325313e-04 2.11170435e+00
  2.11433196e+00]
 [2.15850000e+03 2.17369437e+00 1.34839772e-03 2.17171049e+00
  2.17586088e+00]
 [2.25550000e+03 2.23903270e+00 1.33938313e-03 2.23713613e+00
  2.24121237e+00]
 [2.35550000e+03 2.30990701e+00 1.28478282e-03 2.30810523e+00
  2.31164527e+00]
 [2.45900000e+03 2.38729544e+00 1.15040399e-03 2.38586044e+00
  2.38909793e+00]
 [2.56600000e+03 2.47186799e+00 1.21063200e-03 2.47009683e+00
  2.47369170e+00]
 [2.67650000e+03 2.56426172e+00 3.31051979e-03 2.55961227e+00
  2.56860518e+00]
 [2.79100000e+03 2.66522598e+00 5.61920850e-03 2.65721989e+00
  2.67251039e+00]
 [2.90900000e+03 2.77595429e+00 6.07786309e-03 2.76683784e+00
  2.78351569e+00]
 [3.03100000e+03 2.90004201e+00 3.32068229e-03 2.89396429e+00
  2.90362477e+00]
 [3.15700000e+03 3.03529520e+00 4.74405862e-03 3.02997780e+00
  3.04191566e+00]
 [3.28700000e+03 3.18150249e+00 6.92263270e-03 3.17395282e+00
  3.19295263e+00]
 [3.42150000e+03 3.34239597e+00 5.66517491e-03 3.33733678e+00
  3.35341144e+00]
 [3.56100000e+03 3.52234054e+00 6.50514250e-03 3.50960803e+00
  3.52804208e+00]
 [3.70500000e+03 3.71840191e+00 1.40443917e-02 3.69620371e+00
  3.73191619e+00]
 [3.85300000e+03 3.93315969e+00 1.45034837e-02 3.91303730e+00
  3.94615149e+00]
 [4.00600000e+03 4.16531239e+00 1.53393496e-02 4.13541031e+00
  4.17720366e+00]
 [4.16450000e+03 4.41015491e+00 2.66802930e-02 4.36956072e+00
  4.45143652e+00]
 [4.32800000e+03 4.66537504e+00 4.11741421e-02 4.61516523e+00
  4.73772717e+00]
 [4.49700000e+03 4.93074751e+00 5.70897121e-02 4.87091208e+00
  5.03512478e+00]
 [4.67150000e+03 5.20575523e+00 7.39936935e-02 5.13595295e+00
  5.34324121e+00]
 [4.85150000e+03 5.49014206e+00 9.16641224e-02 5.41011572e+00
  5.66180182e+00]
 [5.03750000e+03 5.78450871e+00 1.10048886e-01 5.69403219e+00
  5.99149513e+00]
 [5.22950000e+03 6.08870516e+00 1.29096964e-01 5.98705196e+00
  6.33217716e+00]
 [5.45050000e+03 6.43908720e+00 1.51070158e-01 6.31763840e+00
  6.72459602e+00]
 [5.65550000e+03 6.76420555e+00 1.71480178e-01 6.62438536e+00
  7.08876801e+00]
 [5.84400000e+03 7.06316671e+00 1.90264481e-01 6.90647030e+00
  7.42370987e+00]
 [6.06200000e+03 7.40884924e+00 2.11990119e-01 7.23269033e+00
  7.81107426e+00]
 [6.28750000e+03 7.76626320e+00 2.34426221e-01 7.57007790e+00
  8.21162701e+00]
 [6.52000000e+03 8.13453894e+00 2.57527691e-01 7.91784000e+00
  8.62444878e+00]
 [6.76000000e+03 8.51438808e+00 2.81354201e-01 8.27667809e+00
  9.05040741e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_141"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_71 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_420 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_421 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_422 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_423 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_424 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_425 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0561 - val_loss: 0.0543
Epoch 2/100
7493/7493 - 39s - loss: 0.0545 - val_loss: 0.0545
Epoch 3/100
7493/7493 - 34s - loss: 0.0543 - val_loss: 0.0544
Epoch 4/100
7493/7493 - 32s - loss: 0.0542 - val_loss: 0.0541
Epoch 5/100
7493/7493 - 34s - loss: 0.0542 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 32s - loss: 0.0541 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 45s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 43s - loss: 0.0541 - val_loss: 0.0544
Epoch 9/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 27s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 26s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 29s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 29s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 22s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 29s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0539
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.5
Model: "functional_143"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_72 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_426 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_427 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_428 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_429 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_430 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_431 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0561 - val_loss: 0.0553
Epoch 2/100
7493/7493 - 36s - loss: 0.0545 - val_loss: 0.0541
Epoch 3/100
7493/7493 - 31s - loss: 0.0543 - val_loss: 0.0541
Epoch 4/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0540
Epoch 5/100
7493/7493 - 44s - loss: 0.0541 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0539
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0538
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_145"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_73 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_432 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_433 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_434 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_435 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_436 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_437 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0565 - val_loss: 0.0560
Epoch 2/100
7493/7493 - 34s - loss: 0.0545 - val_loss: 0.0543
Epoch 3/100
7493/7493 - 33s - loss: 0.0543 - val_loss: 0.0551
Epoch 4/100
7493/7493 - 32s - loss: 0.0542 - val_loss: 0.0540
Epoch 5/100
7493/7493 - 34s - loss: 0.0541 - val_loss: 0.0542
Epoch 6/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0541 - val_loss: 0.0542
Epoch 8/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0540
Epoch 10/100
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0540
Epoch 12/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 46s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0539
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_147"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_74 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_438 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_439 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_440 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_441 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_442 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_443 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0569 - val_loss: 0.0555
Epoch 2/100
7493/7493 - 35s - loss: 0.0545 - val_loss: 0.0544
Epoch 3/100
7493/7493 - 36s - loss: 0.0543 - val_loss: 0.0550
Epoch 4/100
7493/7493 - 40s - loss: 0.0542 - val_loss: 0.0540
Epoch 5/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0541
Epoch 6/100
7493/7493 - 45s - loss: 0.0542 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 46s - loss: 0.0541 - val_loss: 0.0538
Epoch 8/100
7493/7493 - 41s - loss: 0.0541 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 40s - loss: 0.0541 - val_loss: 0.0544
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0545
Epoch 11/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 33/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 35/100
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0538
Epoch 36/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4145ddfd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.5
Model: "functional_149"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_75 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_444 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_445 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_446 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_447 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_448 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_449 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 48s - loss: 0.0569 - val_loss: 0.0543
Epoch 2/100
7493/7493 - 35s - loss: 0.0545 - val_loss: 0.0560
Epoch 3/100
7493/7493 - 32s - loss: 0.0543 - val_loss: 0.0541
Epoch 4/100
7493/7493 - 31s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 40s - loss: 0.0541 - val_loss: 0.0540
Epoch 6/100
7493/7493 - 37s - loss: 0.0541 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 32s - loss: 0.0540 - val_loss: 0.0544
Epoch 8/100
7493/7493 - 30s - loss: 0.0540 - val_loss: 0.0550
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0540 - val_loss: 0.0540
Epoch 10/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 41s - loss: 0.0538 - val_loss: 0.0540
Epoch 13/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 29s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 34/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0539
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f415561c488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73774676e+00 5.23226309e-04 1.73697841e+00
  1.73848522e+00]
 [1.28750000e+03 1.76615369e+00 1.37840831e-04 1.76601326e+00
  1.76639819e+00]
 [1.35350000e+03 1.79733880e+00 3.76514547e-04 1.79701686e+00
  1.79794252e+00]
 [1.42200000e+03 1.83147321e+00 3.67071357e-04 1.83104801e+00
  1.83214283e+00]
 [1.49300000e+03 1.86693339e+00 1.80945606e-04 1.86674547e+00
  1.86726093e+00]
 [1.56650000e+03 1.90563788e+00 4.50459368e-04 1.90478086e+00
  1.90605712e+00]
 [1.64250000e+03 1.94609623e+00 3.81782006e-04 1.94561470e+00
  1.94678044e+00]
 [1.72100000e+03 1.99081929e+00 7.19108871e-04 1.98943567e+00
  1.99153268e+00]
 [1.80250000e+03 2.04017954e+00 4.48656774e-04 2.03960228e+00
  2.04071450e+00]
 [1.88700000e+03 2.09279766e+00 7.74583027e-04 2.09171057e+00
  2.09413505e+00]
 [1.97450000e+03 2.14948421e+00 1.63113363e-03 2.14736629e+00
  2.15216851e+00]
 [2.06500000e+03 2.21254344e+00 1.25687014e-03 2.21096897e+00
  2.21465850e+00]
 [2.15850000e+03 2.28120222e+00 1.29202986e-03 2.27988601e+00
  2.28313494e+00]
 [2.25550000e+03 2.35626779e+00 9.68243483e-04 2.35459948e+00
  2.35751152e+00]
 [2.35550000e+03 2.43796654e+00 2.01119924e-03 2.43590999e+00
  2.44153905e+00]
 [2.45900000e+03 2.52659903e+00 3.50333967e-03 2.52314973e+00
  2.53239012e+00]
 [2.56600000e+03 2.62362204e+00 3.83191147e-03 2.61871314e+00
  2.62968707e+00]
 [2.67650000e+03 2.73003597e+00 3.40196881e-03 2.72578430e+00
  2.73475075e+00]
 [2.79100000e+03 2.84789910e+00 4.88974967e-03 2.84038973e+00
  2.85467601e+00]
 [2.90900000e+03 2.97779818e+00 8.55099869e-03 2.96191931e+00
  2.98743987e+00]
 [3.03100000e+03 3.12102861e+00 1.00040617e-02 3.10334754e+00
  3.13380122e+00]
 [3.15700000e+03 3.27825689e+00 8.59168612e-03 3.26999187e+00
  3.29055548e+00]
 [3.28700000e+03 3.44632707e+00 9.36259279e-03 3.43242359e+00
  3.45757198e+00]
 [3.42150000e+03 3.62900519e+00 8.79065843e-03 3.61318374e+00
  3.63926554e+00]
 [3.56100000e+03 3.83361340e+00 7.37946593e-03 3.82491970e+00
  3.84637690e+00]
 [3.70500000e+03 4.06360302e+00 1.40337615e-02 4.04822350e+00
  4.08217239e+00]
 [3.85300000e+03 4.30958986e+00 2.30383584e-02 4.28200483e+00
  4.33778763e+00]
 [4.00600000e+03 4.57521524e+00 2.52731041e-02 4.54506397e+00
  4.61280537e+00]
 [4.16450000e+03 4.85825300e+00 2.68300755e-02 4.82042503e+00
  4.90254402e+00]
 [4.32800000e+03 5.15668316e+00 3.28407721e-02 5.10586500e+00
  5.20685101e+00]
 [4.49700000e+03 5.47170115e+00 4.66056441e-02 5.40197372e+00
  5.52808952e+00]
 [4.67150000e+03 5.79947271e+00 6.53249602e-02 5.70865202e+00
  5.87626505e+00]
 [4.85150000e+03 6.13991175e+00 8.71334061e-02 6.02588034e+00
  6.24805450e+00]
 [5.03750000e+03 6.49456873e+00 1.12160337e-01 6.35457993e+00
  6.63266134e+00]
 [5.22950000e+03 6.86332140e+00 1.40370482e-01 6.69485378e+00
  7.02991867e+00]
 [5.45050000e+03 7.28828478e+00 1.73240916e-01 7.08735180e+00
  7.48737574e+00]
 [5.65550000e+03 7.68252840e+00 2.04007546e-01 7.45137405e+00
  7.91856098e+00]
 [5.84400000e+03 8.04498482e+00 2.32494494e-01 7.78594971e+00
  8.31890297e+00]
 [6.06200000e+03 8.46400681e+00 2.65650482e-01 8.17261791e+00
  8.78188419e+00]
 [6.28750000e+03 8.89717484e+00 3.00183168e-01 8.57004642e+00
  9.26070881e+00]
 [6.52000000e+03 9.34339123e+00 3.36050587e-01 8.97424030e+00
  9.75423050e+00]
 [6.76000000e+03 9.80346260e+00 3.73395028e-01 9.39059258e+00
  1.02634296e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_151"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_76 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_450 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_451 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_452 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_453 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_454 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_455 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 15s - loss: 0.0504 - val_loss: 0.0483
Epoch 2/100
7493/7493 - 12s - loss: 0.0484 - val_loss: 0.0478
Epoch 3/100
7493/7493 - 15s - loss: 0.0482 - val_loss: 0.0480
Epoch 4/100
7493/7493 - 15s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 16s - loss: 0.0481 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 15s - loss: 0.0479 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 16s - loss: 0.0479 - val_loss: 0.0479
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 15s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 14s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.7
Model: "functional_153"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_77 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_456 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_457 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_458 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_459 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_460 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_461 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 15s - loss: 0.0504 - val_loss: 0.0482
Epoch 2/100
7493/7493 - 16s - loss: 0.0484 - val_loss: 0.0491
Epoch 3/100
7493/7493 - 17s - loss: 0.0482 - val_loss: 0.0483
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 16s - loss: 0.0482 - val_loss: 0.0484
Epoch 5/100
7493/7493 - 15s - loss: 0.0479 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 17s - loss: 0.0479 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 17s - loss: 0.0479 - val_loss: 0.0480
Epoch 8/100
7493/7493 - 17s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 14s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 19s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 14s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_155"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_78 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_462 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_463 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_464 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_465 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_466 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_467 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 14s - loss: 0.0512 - val_loss: 0.0486
Epoch 2/100
7493/7493 - 12s - loss: 0.0484 - val_loss: 0.0486
Epoch 3/100
7493/7493 - 14s - loss: 0.0482 - val_loss: 0.0484
Epoch 4/100
7493/7493 - 16s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 15s - loss: 0.0480 - val_loss: 0.0486
Epoch 6/100
7493/7493 - 14s - loss: 0.0480 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 15s - loss: 0.0480 - val_loss: 0.0480
Epoch 8/100
7493/7493 - 15s - loss: 0.0480 - val_loss: 0.0479
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 15s - loss: 0.0480 - val_loss: 0.0482
Epoch 10/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0479
Epoch 11/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0479
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0479
Epoch 13/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 17s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 13s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 14s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 14s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 16s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 15s - loss: 0.0478 - val_loss: 0.0478
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_157"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_79 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_468 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_469 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_470 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_471 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_472 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_473 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 14s - loss: 0.0501 - val_loss: 0.0480
Epoch 2/100
7493/7493 - 15s - loss: 0.0484 - val_loss: 0.0485
Epoch 3/100
7493/7493 - 16s - loss: 0.0482 - val_loss: 0.0479
Epoch 4/100
7493/7493 - 16s - loss: 0.0481 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 15s - loss: 0.0481 - val_loss: 0.0479
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 14s - loss: 0.0480 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 16s - loss: 0.0479 - val_loss: 0.0477
Epoch 8/100
7493/7493 - 15s - loss: 0.0479 - val_loss: 0.0477
Epoch 9/100
7493/7493 - 11s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 11s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 11s - loss: 0.0478 - val_loss: 0.0477
Epoch 12/100
7493/7493 - 11s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 27s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0477
Epoch 15/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0477
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41551f2f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.7
Model: "functional_159"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_80 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_474 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_475 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_476 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_477 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_478 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_479 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0502 - val_loss: 0.0480
Epoch 2/100
7493/7493 - 41s - loss: 0.0483 - val_loss: 0.0479
Epoch 3/100
7493/7493 - 32s - loss: 0.0481 - val_loss: 0.0485
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0480 - val_loss: 0.0482
Epoch 5/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 27s - loss: 0.0478 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0479
Epoch 8/100
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0479
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 12s - loss: 0.0478 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 13s - loss: 0.0477 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 12s - loss: 0.0477 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 11s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 11s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 11s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 11s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 12s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 14s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 12s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 12s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 11s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 13s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 15s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 28s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 43s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 42s - loss: 0.0477 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f414b61a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79555755e+00 4.08777012e-04 1.79491031e+00
  1.79617381e+00]
 [1.28750000e+03 1.82498350e+00 1.99793210e-04 1.82474136e+00
  1.82531095e+00]
 [1.35350000e+03 1.85838263e+00 3.12026781e-04 1.85799265e+00
  1.85879111e+00]
 [1.42200000e+03 1.89446228e+00 4.99915373e-04 1.89370668e+00
  1.89508486e+00]
 [1.49300000e+03 1.93341944e+00 4.40974240e-04 1.93288326e+00
  1.93399942e+00]
 [1.56650000e+03 1.97575483e+00 3.27593022e-04 1.97528327e+00
  1.97627974e+00]
 [1.64250000e+03 2.02097802e+00 5.36501275e-04 2.02029419e+00
  2.02187061e+00]
 [1.72100000e+03 2.07077332e+00 6.43192144e-04 2.06973481e+00
  2.07151532e+00]
 [1.80250000e+03 2.12581244e+00 5.98316687e-04 2.12489676e+00
  2.12652111e+00]
 [1.88700000e+03 2.18586574e+00 5.38853767e-04 2.18525410e+00
  2.18666744e+00]
 [1.97450000e+03 2.25120826e+00 1.09975545e-03 2.25009179e+00
  2.25314713e+00]
 [2.06500000e+03 2.32259159e+00 1.52300843e-03 2.32083464e+00
  2.32494855e+00]
 [2.15850000e+03 2.40094857e+00 1.47558728e-03 2.39869976e+00
  2.40274334e+00]
 [2.25550000e+03 2.48695283e+00 8.78754470e-04 2.48578048e+00
  2.48847747e+00]
 [2.35550000e+03 2.58058829e+00 2.34446518e-03 2.57777095e+00
  2.58365774e+00]
 [2.45900000e+03 2.68307233e+00 3.97205875e-03 2.67666030e+00
  2.68829250e+00]
 [2.56600000e+03 2.79518332e+00 5.63805784e-03 2.78568316e+00
  2.80292606e+00]
 [2.67650000e+03 2.91886806e+00 6.11370739e-03 2.91201210e+00
  2.92892623e+00]
 [2.79100000e+03 3.05500174e+00 7.47344307e-03 3.04552245e+00
  3.06473684e+00]
 [2.90900000e+03 3.20352163e+00 7.90529967e-03 3.19022322e+00
  3.21297145e+00]
 [3.03100000e+03 3.36643877e+00 4.93845876e-03 3.36039281e+00
  3.37482095e+00]
 [3.15700000e+03 3.54213710e+00 6.61722212e-03 3.53174663e+00
  3.54953170e+00]
 [3.28700000e+03 3.73568339e+00 6.44313845e-03 3.72672105e+00
  3.74371028e+00]
 [3.42150000e+03 3.95128379e+00 1.15422582e-02 3.93866897e+00
  3.96843076e+00]
 [3.56100000e+03 4.19602442e+00 1.92736256e-02 4.16867542e+00
  4.22097063e+00]
 [3.70500000e+03 4.46138620e+00 3.15543344e-02 4.40769482e+00
  4.50140715e+00]
 [3.85300000e+03 4.74621439e+00 3.71314109e-02 4.68468761e+00
  4.79387331e+00]
 [4.00600000e+03 5.05982084e+00 3.08414808e-02 5.02131128e+00
  5.10139513e+00]
 [4.16450000e+03 5.39166384e+00 3.20237223e-02 5.33666372e+00
  5.42279148e+00]
 [4.32800000e+03 5.73608866e+00 4.06526810e-02 5.65814209e+00
  5.77620888e+00]
 [4.49700000e+03 6.09409599e+00 5.28056577e-02 5.99500322e+00
  6.15145779e+00]
 [4.67150000e+03 6.46621008e+00 6.49171198e-02 6.35128736e+00
  6.54039717e+00]
 [4.85150000e+03 6.85202818e+00 7.66826202e-02 6.72601557e+00
  6.94293642e+00]
 [5.03750000e+03 7.25105085e+00 8.97455171e-02 7.11401987e+00
  7.36022854e+00]
 [5.22950000e+03 7.66309109e+00 1.03708488e-01 7.51516581e+00
  7.79201508e+00]
 [5.45050000e+03 8.13748465e+00 1.20123505e-01 7.97750807e+00
  8.29016590e+00]
 [5.65550000e+03 8.57763348e+00 1.35579732e-01 8.40684223e+00
  8.75324726e+00]
 [5.84400000e+03 8.98247643e+00 1.50015377e-01 8.80191231e+00
  9.18004608e+00]
 [6.06200000e+03 9.45073299e+00 1.66885650e-01 9.25907993e+00
  9.67451191e+00]
 [6.28750000e+03 9.93502903e+00 1.84387090e-01 9.73224735e+00
  1.01864872e+01]
 [6.52000000e+03 1.04342674e+01 2.02477066e-01 1.02203703e+01
  1.07149458e+01]
 [6.76000000e+03 1.09494236e+01 2.21086664e-01 1.07245255e+01
  1.12607956e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_161"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_81 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_480 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_481 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_482 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_483 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_484 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_485 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0274 - val_loss: 0.0259
Epoch 2/100
7493/7493 - 32s - loss: 0.0257 - val_loss: 0.0260
Epoch 3/100
7493/7493 - 32s - loss: 0.0256 - val_loss: 0.0253
Epoch 4/100
7493/7493 - 46s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0254
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.9
Model: "functional_163"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_82 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_486 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_487 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_488 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_489 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_490 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_491 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0283 - val_loss: 0.0254
Epoch 2/100
7493/7493 - 43s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 35s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_165"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_83 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_492 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_493 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_494 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_495 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_496 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_497 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0273 - val_loss: 0.0268
Epoch 2/100
7493/7493 - 31s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 34s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 44s - loss: 0.0255 - val_loss: 0.0258
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0255 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0254
Epoch 7/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0256
Epoch 8/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 48s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 13s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 15s - loss: 0.0253 - val_loss: 0.0253
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_167"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_84 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_498 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_499 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_500 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_501 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_502 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_503 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 17s - loss: 0.0273 - val_loss: 0.0256
Epoch 2/100
7493/7493 - 17s - loss: 0.0257 - val_loss: 0.0256
Epoch 3/100
7493/7493 - 17s - loss: 0.0256 - val_loss: 0.0255
Epoch 4/100
7493/7493 - 16s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 15s - loss: 0.0255 - val_loss: 0.0254
Epoch 6/100
7493/7493 - 16s - loss: 0.0255 - val_loss: 0.0255
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 15s - loss: 0.0255 - val_loss: 0.0255
Epoch 8/100
7493/7493 - 16s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 17s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 16s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 15s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 16s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 15s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 19s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 15s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 13s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 13s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 12s - loss: 0.0253 - val_loss: 0.0253
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f414b27ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.9
Model: "functional_169"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_85 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_504 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_505 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_506 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_507 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_508 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_509 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 12s - loss: 0.0274 - val_loss: 0.0264
Epoch 2/100
7493/7493 - 11s - loss: 0.0257 - val_loss: 0.0255
Epoch 3/100
7493/7493 - 12s - loss: 0.0255 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 14s - loss: 0.0255 - val_loss: 0.0255
Epoch 5/100
7493/7493 - 12s - loss: 0.0254 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 36s - loss: 0.0254 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0254 - val_loss: 0.0254
Epoch 9/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 44s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 45s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 43s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f41552431e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88108296e+00 5.13077722e-04 1.88031626e+00
  1.88171673e+00]
 [1.28750000e+03 1.91365407e+00 3.96105268e-04 1.91306555e+00
  1.91406918e+00]
 [1.35350000e+03 1.95131958e+00 4.91743547e-04 1.95038009e+00
  1.95175946e+00]
 [1.42200000e+03 1.99138544e+00 4.01635167e-04 1.99074018e+00
  1.99198651e+00]
 [1.49300000e+03 2.03747215e+00 3.99890851e-04 2.03676939e+00
  2.03799272e+00]
 [1.56650000e+03 2.08703642e+00 6.36624242e-04 2.08625078e+00
  2.08784056e+00]
 [1.64250000e+03 2.14009485e+00 9.81789976e-04 2.13844585e+00
  2.14110112e+00]
 [1.72100000e+03 2.20016675e+00 1.14721173e-03 2.19856644e+00
  2.20165277e+00]
 [1.80250000e+03 2.26633186e+00 4.52377435e-04 2.26577997e+00
  2.26714206e+00]
 [1.88700000e+03 2.33885350e+00 1.33773148e-03 2.33685160e+00
  2.34103942e+00]
 [1.97450000e+03 2.41804194e+00 1.28074360e-03 2.41625595e+00
  2.41988277e+00]
 [2.06500000e+03 2.50450940e+00 1.41101728e-03 2.50237656e+00
  2.50648427e+00]
 [2.15850000e+03 2.59956331e+00 1.91767294e-03 2.59604740e+00
  2.60164881e+00]
 [2.25550000e+03 2.70418019e+00 1.38625786e-03 2.70304585e+00
  2.70664692e+00]
 [2.35550000e+03 2.81771178e+00 1.92826934e-03 2.81476903e+00
  2.82015157e+00]
 [2.45900000e+03 2.94132743e+00 3.41235450e-03 2.93628836e+00
  2.94629455e+00]
 [2.56600000e+03 3.07813463e+00 4.91507828e-03 3.07242656e+00
  3.08559728e+00]
 [2.67650000e+03 3.23256512e+00 7.67455126e-03 3.21894813e+00
  3.24197936e+00]
 [2.79100000e+03 3.40228677e+00 1.14757856e-02 3.38372254e+00
  3.41700935e+00]
 [2.90900000e+03 3.58581929e+00 1.40032851e-02 3.57019687e+00
  3.60673141e+00]
 [3.03100000e+03 3.78581920e+00 1.55355972e-02 3.76188231e+00
  3.80958796e+00]
 [3.15700000e+03 4.00914345e+00 1.27906591e-02 3.99248910e+00
  4.02563620e+00]
 [3.28700000e+03 4.25697699e+00 1.29858731e-02 4.24434233e+00
  4.28113317e+00]
 [3.42150000e+03 4.53305435e+00 1.13864057e-02 4.52390099e+00
  4.55445051e+00]
 [3.56100000e+03 4.83609667e+00 1.41405832e-02 4.81125879e+00
  4.84975243e+00]
 [3.70500000e+03 5.15705290e+00 2.86066244e-02 5.11217403e+00
  5.19469213e+00]
 [3.85300000e+03 5.49609365e+00 4.48338315e-02 5.42711973e+00
  5.55987215e+00]
 [4.00600000e+03 5.86083260e+00 6.07331014e-02 5.76085472e+00
  5.94637346e+00]
 [4.16450000e+03 6.25026884e+00 7.86949796e-02 6.12012339e+00
  6.36233139e+00]
 [4.32800000e+03 6.66542673e+00 9.54233955e-02 6.51550198e+00
  6.80341101e+00]
 [4.49700000e+03 7.10423889e+00 1.12894524e-01 6.94160604e+00
  7.26117039e+00]
 [4.67150000e+03 7.55948286e+00 1.32507345e-01 7.38784456e+00
  7.73470068e+00]
 [4.85150000e+03 8.03149424e+00 1.52384321e-01 7.85986519e+00
  8.22355747e+00]
 [5.03750000e+03 8.52294960e+00 1.71481555e-01 8.36810017e+00
  8.73641777e+00]
 [5.22950000e+03 9.03026180e+00 1.93986607e-01 8.85045624e+00
  9.28427505e+00]
 [5.45050000e+03 9.61161537e+00 2.24652850e-01 9.39277267e+00
  9.91505146e+00]
 [5.65550000e+03 1.01460295e+01 2.59010678e-01 9.86216545e+00
  1.05000162e+01]
 [5.84400000e+03 1.06298267e+01 2.98927504e-01 1.02605619e+01
  1.10375919e+01]
 [6.06200000e+03 1.11803856e+01 3.53475379e-01 1.06924839e+01
  1.16586685e+01]
 [6.28750000e+03 1.17428013e+01 4.09357603e-01 1.11382380e+01
  1.23000803e+01]
 [6.52000000e+03 1.23110521e+01 4.65785776e-01 1.15962763e+01
  1.29598827e+01]
 [6.76000000e+03 1.28942936e+01 5.29222589e-01 1.20670500e+01
  1.36391668e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_171"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_86 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_510 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_511 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_512 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_513 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_514 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_515 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0057 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 37s - loss: 0.0044 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 32s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 43s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.99
Model: "functional_173"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_87 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_516 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_517 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_518 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_519 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_520 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_521 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0054 - val_loss: 0.0053
Epoch 2/100
7493/7493 - 26s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 44s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 22s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 24s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 33/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 34/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 36/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 37/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 38/100

Epoch 00038: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 39/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 40/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 41/100

Epoch 00041: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 00041: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_175"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_88 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_522 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_523 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_524 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_525 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_526 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_527 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0064 - val_loss: 0.0051
Epoch 2/100
7493/7493 - 33s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0045
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 45s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_177"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_89 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_528 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_529 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_530 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_531 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_532 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_533 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0064 - val_loss: 0.0050
Epoch 2/100
7493/7493 - 34s - loss: 0.0045 - val_loss: 0.0046
Epoch 3/100
7493/7493 - 30s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100
7493/7493 - 39s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 24s - loss: 0.0044 - val_loss: 0.0044
Epoch 7/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_10/envelope/GtoWW35na/xsec_20/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4146008ae8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.99
Model: "functional_179"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_90 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_534 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_535 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_536 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_537 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_538 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_539 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0060 - val_loss: 0.0045
Epoch 2/100
7493/7493 - 33s - loss: 0.0044 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 38s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 48s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 33/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 34/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 36/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 00036: early stopping
