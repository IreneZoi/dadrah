setGPU: Setting GPU to: 0
bin centers:  [1227.5, 1287.5, 1353.5, 1422.0, 1493.0, 1566.5, 1642.5, 1721.0, 1802.5, 1887.0, 1974.5, 2065.0, 2158.5, 2255.5, 2355.5, 2459.0, 2566.0, 2676.5, 2791.0, 2909.0, 3031.0, 3157.0, 3287.0, 3421.5, 3561.0, 3705.0, 3853.0, 4006.0, 4164.5, 4328.0, 4497.0, 4671.5, 4851.5, 5037.5, 5229.5, 5450.5, 5655.5, 5844.0, 6062.0, 6287.5, 6520.0, 6760.0]
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
4793609 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
4796089 events read in 10 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_EXT_signalregion_parts
qcd all: min mjj = 1200.0001220703125, max mjj = 7285.58154296875
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
531825 events read in 2 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
training on 1918006 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense (Dense)                (None, 60)                120       
_________________________________________________________________
dense_1 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_2 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_3 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_4 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0248 - val_loss: 0.0242
Epoch 2/100
7493/7493 - 32s - loss: 0.0240 - val_loss: 0.0237
Epoch 3/100
7493/7493 - 35s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0241
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 7/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0238
Epoch 9/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 41s - loss: 0.0236 - val_loss: 0.0236
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.1
Model: "functional_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_6 (Dense)              (None, 60)                120       
_________________________________________________________________
dense_7 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_8 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_9 (Dense)              (None, 60)                3660      
_________________________________________________________________
dense_10 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_11 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0250 - val_loss: 0.0242
Epoch 2/100
7493/7493 - 32s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 30s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0240
Epoch 6/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 42s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 29s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 29s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_3 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_12 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_13 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_14 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_15 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_16 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_17 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0251 - val_loss: 0.0241
Epoch 2/100
7493/7493 - 34s - loss: 0.0241 - val_loss: 0.0240
Epoch 3/100
7493/7493 - 48s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 45s - loss: 0.0239 - val_loss: 0.0250
Epoch 5/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100
7493/7493 - 30s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0238 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 48s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.1
Model: "functional_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_4 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_18 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_19 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_20 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_21 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_22 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_23 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0254 - val_loss: 0.0244
Epoch 2/100
7493/7493 - 40s - loss: 0.0240 - val_loss: 0.0240
Epoch 3/100
7493/7493 - 31s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 48s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 38s - loss: 0.0238 - val_loss: 0.0238
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 47s - loss: 0.0238 - val_loss: 0.0240
Epoch 7/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 29s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 31/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 34/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.1
Model: "functional_9"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_5 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_24 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_25 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_26 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_27 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_28 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_29 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 42s - loss: 0.0251 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 31s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 42s - loss: 0.0239 - val_loss: 0.0240
Epoch 4/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 47s - loss: 0.0236 - val_loss: 0.0236
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cef6a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58435054e+00 7.79810135e-05 1.58426940e+00
  1.58445299e+00]
 [1.28750000e+03 1.61256268e+00 1.23901192e-04 1.61242735e+00
  1.61271358e+00]
 [1.35350000e+03 1.64306881e+00 2.28554191e-04 1.64279842e+00
  1.64340591e+00]
 [1.42200000e+03 1.67536447e+00 4.27215215e-04 1.67457390e+00
  1.67586386e+00]
 [1.49300000e+03 1.70717130e+00 6.63372859e-04 1.70623600e+00
  1.70802391e+00]
 [1.56650000e+03 1.74194331e+00 5.47738423e-04 1.74150300e+00
  1.74301016e+00]
 [1.64250000e+03 1.77678657e+00 5.14379689e-04 1.77621722e+00
  1.77751541e+00]
 [1.72100000e+03 1.81314991e+00 3.54933727e-04 1.81264174e+00
  1.81364381e+00]
 [1.80250000e+03 1.85145440e+00 5.48945109e-04 1.85071707e+00
  1.85235453e+00]
 [1.88700000e+03 1.89290347e+00 6.16622149e-04 1.89200521e+00
  1.89374840e+00]
 [1.97450000e+03 1.93681283e+00 9.27176266e-04 1.93542218e+00
  1.93777955e+00]
 [2.06500000e+03 1.98344507e+00 1.27610393e-03 1.98147213e+00
  1.98489463e+00]
 [2.15850000e+03 2.03383074e+00 1.29050608e-03 2.03133821e+00
  2.03509068e+00]
 [2.25550000e+03 2.08823109e+00 1.29759452e-03 2.08580470e+00
  2.08920550e+00]
 [2.35550000e+03 2.14673648e+00 1.89141772e-03 2.14420915e+00
  2.14990401e+00]
 [2.45900000e+03 2.20992265e+00 2.06802105e-03 2.20796824e+00
  2.21385741e+00]
 [2.56600000e+03 2.27905374e+00 1.57649225e-03 2.27682734e+00
  2.28113818e+00]
 [2.67650000e+03 2.35417991e+00 1.77280212e-03 2.35263300e+00
  2.35755539e+00]
 [2.79100000e+03 2.43545141e+00 3.12257287e-03 2.43043017e+00
  2.43971276e+00]
 [2.90900000e+03 2.52449260e+00 4.07946427e-03 2.51687956e+00
  2.52766109e+00]
 [3.03100000e+03 2.62469039e+00 5.40954007e-03 2.61621165e+00
  2.63026667e+00]
 [3.15700000e+03 2.73488908e+00 7.33806831e-03 2.72606683e+00
  2.74361062e+00]
 [3.28700000e+03 2.85724545e+00 4.82267159e-03 2.85172939e+00
  2.86469984e+00]
 [3.42150000e+03 2.99283748e+00 3.56576479e-03 2.98741198e+00
  2.99638534e+00]
 [3.56100000e+03 3.14115195e+00 8.18754908e-03 3.12598109e+00
  3.14871907e+00]
 [3.70500000e+03 3.30002775e+00 1.04762397e-02 3.28342438e+00
  3.31079054e+00]
 [3.85300000e+03 3.46691880e+00 1.21227105e-02 3.45136905e+00
  3.48050594e+00]
 [4.00600000e+03 3.64241629e+00 1.35923532e-02 3.62866688e+00
  3.65894628e+00]
 [4.16450000e+03 3.82734771e+00 1.51876002e-02 3.81207585e+00
  3.84742117e+00]
 [4.32800000e+03 4.02014866e+00 1.80677879e-02 3.99586964e+00
  4.04300070e+00]
 [4.49700000e+03 4.22140665e+00 2.36917813e-02 4.18491936e+00
  4.24566412e+00]
 [4.67150000e+03 4.43125143e+00 3.34132009e-02 4.38036633e+00
  4.47353458e+00]
 [4.85150000e+03 4.65114565e+00 5.01028513e-02 4.58216333e+00
  4.73089790e+00]
 [5.03750000e+03 4.88539839e+00 8.12321706e-02 4.79082775e+00
  5.03194571e+00]
 [5.22950000e+03 5.13230219e+00 1.24694492e-01 5.00632143e+00
  5.36849928e+00]
 [5.45050000e+03 5.41632919e+00 1.75777516e-01 5.25443792e+00
  5.75576639e+00]
 [5.65550000e+03 5.67948599e+00 2.23374511e-01 5.48462963e+00
  6.11439991e+00]
 [5.84400000e+03 5.92107887e+00 2.67013138e-01 5.69630432e+00
  6.44324732e+00]
 [6.06200000e+03 6.19983511e+00 3.17022811e-01 5.94109488e+00
  6.82180595e+00]
 [6.28750000e+03 6.48714018e+00 3.67804557e-01 6.19427490e+00
  7.21030760e+00]
 [6.52000000e+03 6.78183270e+00 4.18713365e-01 6.45525789e+00
  7.60631943e+00]
 [6.76000000e+03 7.08370714e+00 4.69203035e-01 6.72457695e+00
  8.00852299e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_11"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_6 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_30 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_31 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_32 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_33 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_34 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_35 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0521
Epoch 2/100
7493/7493 - 46s - loss: 0.0471 - val_loss: 0.0476
Epoch 3/100
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0470
Epoch 4/100
7493/7493 - 34s - loss: 0.0468 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 39s - loss: 0.0468 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 32s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 49s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 46s - loss: 0.0465 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 38s - loss: 0.0465 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 48s - loss: 0.0465 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 46s - loss: 0.0465 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 41s - loss: 0.0465 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0465 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 30s - loss: 0.0465 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 38s - loss: 0.0465 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0465 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.3
Model: "functional_13"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_36 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_37 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_38 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_39 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_40 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_41 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0484 - val_loss: 0.0483
Epoch 2/100
7493/7493 - 33s - loss: 0.0471 - val_loss: 0.0468
Epoch 3/100
7493/7493 - 34s - loss: 0.0470 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 33s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 34s - loss: 0.0468 - val_loss: 0.0471
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0467
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 27s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_15"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_42 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_43 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_44 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_45 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_46 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_47 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0484 - val_loss: 0.0475
Epoch 2/100
7493/7493 - 38s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 31s - loss: 0.0470 - val_loss: 0.0469
Epoch 4/100
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0468 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0469
Epoch 9/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 28s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.3
Model: "functional_17"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_9 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_48 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_49 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_50 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_51 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_52 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_53 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0484 - val_loss: 0.0475
Epoch 2/100
7493/7493 - 38s - loss: 0.0472 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 43s - loss: 0.0470 - val_loss: 0.0470
Epoch 4/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 35s - loss: 0.0469 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0466
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0468 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 47s - loss: 0.0467 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0468
Epoch 10/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 39s - loss: 0.0467 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0467 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 33/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda20e7bd08> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.3
Model: "functional_19"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_10 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_54 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_55 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_56 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_57 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_58 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_59 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0486 - val_loss: 0.0483
Epoch 2/100
7493/7493 - 33s - loss: 0.0471 - val_loss: 0.0466
Epoch 3/100
7493/7493 - 43s - loss: 0.0470 - val_loss: 0.0470
Epoch 4/100
7493/7493 - 37s - loss: 0.0469 - val_loss: 0.0473
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 41s - loss: 0.0467 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 9/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0465
Epoch 13/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0465
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0465
Epoch 15/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda20d6d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67794986e+00 1.19088675e-04 1.67776716e+00
  1.67811382e+00]
 [1.28750000e+03 1.70615854e+00 3.70916753e-04 1.70558739e+00
  1.70654118e+00]
 [1.35350000e+03 1.73648303e+00 5.10013317e-04 1.73581314e+00
  1.73717594e+00]
 [1.42200000e+03 1.76911502e+00 1.46991843e-04 1.76892769e+00
  1.76929510e+00]
 [1.49300000e+03 1.80227289e+00 6.76962967e-04 1.80132568e+00
  1.80324483e+00]
 [1.56650000e+03 1.83841062e+00 4.14246178e-04 1.83765697e+00
  1.83890426e+00]
 [1.64250000e+03 1.87575958e+00 7.06932397e-04 1.87451231e+00
  1.87662375e+00]
 [1.72100000e+03 1.91624737e+00 4.17213269e-04 1.91560161e+00
  1.91680408e+00]
 [1.80250000e+03 1.96031260e+00 3.52479297e-04 1.95983195e+00
  1.96081197e+00]
 [1.88700000e+03 2.00720091e+00 4.61984448e-04 2.00669026e+00
  2.00805306e+00]
 [1.97450000e+03 2.05793500e+00 3.22608830e-04 2.05742097e+00
  2.05830574e+00]
 [2.06500000e+03 2.11326489e+00 6.42907970e-04 2.11253667e+00
  2.11446524e+00]
 [2.15850000e+03 2.17346711e+00 9.97888588e-04 2.17225862e+00
  2.17519236e+00]
 [2.25550000e+03 2.23893318e+00 9.59743270e-04 2.23797417e+00
  2.24047327e+00]
 [2.35550000e+03 2.31005988e+00 1.03677273e-03 2.30830097e+00
  2.31136990e+00]
 [2.45900000e+03 2.38783059e+00 1.52080299e-03 2.38511872e+00
  2.38934016e+00]
 [2.56600000e+03 2.47242122e+00 1.89920000e-03 2.46893239e+00
  2.47473216e+00]
 [2.67650000e+03 2.56466579e+00 2.94975829e-03 2.55990577e+00
  2.56778097e+00]
 [2.79100000e+03 2.66577373e+00 5.43426426e-03 2.65902376e+00
  2.67429233e+00]
 [2.90900000e+03 2.77593656e+00 6.93076106e-03 2.76817966e+00
  2.78728890e+00]
 [3.03100000e+03 2.89724054e+00 7.39744078e-03 2.88856578e+00
  2.90809798e+00]
 [3.15700000e+03 3.03059483e+00 1.05263059e-02 3.01620269e+00
  3.04096580e+00]
 [3.28700000e+03 3.17744608e+00 1.24299012e-02 3.16130424e+00
  3.19415569e+00]
 [3.42150000e+03 3.34326982e+00 9.34485022e-03 3.32994676e+00
  3.35634112e+00]
 [3.56100000e+03 3.52896166e+00 4.94450923e-03 3.52166700e+00
  3.53703523e+00]
 [3.70500000e+03 3.72857461e+00 1.01066049e-02 3.71698475e+00
  3.74079180e+00]
 [3.85300000e+03 3.94149790e+00 1.48551093e-02 3.92406249e+00
  3.96007848e+00]
 [4.00600000e+03 4.16794710e+00 1.75023142e-02 4.14399099e+00
  4.18807411e+00]
 [4.16450000e+03 4.40555983e+00 2.06923458e-02 4.37339735e+00
  4.42575979e+00]
 [4.32800000e+03 4.65348101e+00 2.59588668e-02 4.61085033e+00
  4.67498970e+00]
 [4.49700000e+03 4.91088181e+00 3.31163203e-02 4.85689163e+00
  4.94851398e+00]
 [4.67150000e+03 5.17733431e+00 4.15140798e-02 5.11140633e+00
  5.23223972e+00]
 [4.85150000e+03 5.45270205e+00 5.08146004e-02 5.37430525e+00
  5.52590036e+00]
 [5.03750000e+03 5.73762398e+00 6.08446353e-02 5.64625216e+00
  5.83010530e+00]
 [5.22950000e+03 6.03198109e+00 7.14895711e-02 5.92718935e+00
  6.14470005e+00]
 [5.45050000e+03 6.37097139e+00 8.39941840e-02 6.25074768e+00
  6.50732040e+00]
 [5.65550000e+03 6.68548050e+00 9.57634853e-02 6.55099392e+00
  6.84400845e+00]
 [5.84400000e+03 6.97465630e+00 1.06697303e-01 6.82712936e+00
  7.15378094e+00]
 [6.06200000e+03 7.30897884e+00 1.19471764e-01 7.14645100e+00
  7.51217699e+00]
 [6.28750000e+03 7.65459137e+00 1.32825632e-01 7.47668409e+00
  7.88299704e+00]
 [6.52000000e+03 8.01060982e+00 1.46756534e-01 7.81706285e+00
  8.26539230e+00]
 [6.76000000e+03 8.37763996e+00 1.61351098e-01 8.16827297e+00
  8.66016006e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_21"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_11 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_60 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_61 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_62 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_63 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_64 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_65 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0565 - val_loss: 0.0549
Epoch 2/100
7493/7493 - 43s - loss: 0.0544 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 33s - loss: 0.0542 - val_loss: 0.0541
Epoch 4/100
7493/7493 - 32s - loss: 0.0541 - val_loss: 0.0543
Epoch 5/100
7493/7493 - 38s - loss: 0.0540 - val_loss: 0.0540
Epoch 6/100
7493/7493 - 37s - loss: 0.0540 - val_loss: 0.0543
Epoch 7/100
7493/7493 - 31s - loss: 0.0540 - val_loss: 0.0543
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0540 - val_loss: 0.0549
Epoch 9/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0540
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 32/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 34/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 35/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 37/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 38/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 39/100

Epoch 00039: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 41s - loss: 0.0538 - val_loss: 0.0539
Epoch 40/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 41/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 42/100

Epoch 00042: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 00042: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.5
Model: "functional_23"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_12 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_66 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_67 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_68 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_69 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_70 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_71 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0564 - val_loss: 0.0544
Epoch 2/100
7493/7493 - 37s - loss: 0.0545 - val_loss: 0.0548
Epoch 3/100
7493/7493 - 33s - loss: 0.0543 - val_loss: 0.0542
Epoch 4/100
7493/7493 - 33s - loss: 0.0542 - val_loss: 0.0543
Epoch 5/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0542
Epoch 6/100
7493/7493 - 45s - loss: 0.0541 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 47s - loss: 0.0541 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 37s - loss: 0.0541 - val_loss: 0.0542
Epoch 11/100
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0541
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0541 - val_loss: 0.0541
Epoch 13/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0540
Epoch 14/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 32/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 34/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 35/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_25"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_13 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_72 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_73 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_74 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_75 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_76 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_77 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0563 - val_loss: 0.0553
Epoch 2/100
7493/7493 - 34s - loss: 0.0545 - val_loss: 0.0559
Epoch 3/100
7493/7493 - 36s - loss: 0.0544 - val_loss: 0.0539
Epoch 4/100
7493/7493 - 46s - loss: 0.0542 - val_loss: 0.0548
Epoch 5/100
7493/7493 - 47s - loss: 0.0542 - val_loss: 0.0543
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.5
Model: "functional_27"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_14 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_78 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_79 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_80 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_81 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_82 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_83 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0559 - val_loss: 0.0552
Epoch 2/100
7493/7493 - 32s - loss: 0.0545 - val_loss: 0.0545
Epoch 3/100
7493/7493 - 34s - loss: 0.0543 - val_loss: 0.0544
Epoch 4/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0539
Epoch 5/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 42s - loss: 0.0541 - val_loss: 0.0539
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 38s - loss: 0.0541 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0538
Epoch 33/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda305032f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.5
Model: "functional_29"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_15 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_84 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_85 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_86 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_87 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_88 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_89 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0562 - val_loss: 0.0562
Epoch 2/100
7493/7493 - 32s - loss: 0.0546 - val_loss: 0.0540
Epoch 3/100
7493/7493 - 42s - loss: 0.0543 - val_loss: 0.0544
Epoch 4/100
7493/7493 - 36s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 28s - loss: 0.0541 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0543
Epoch 7/100
7493/7493 - 43s - loss: 0.0540 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0540 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 28s - loss: 0.0538 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 31s - loss: 0.0538 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 41s - loss: 0.0538 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0538
Epoch 28/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0538
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2ef331e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73773003e+00 1.89130458e-04 1.73738623e+00
  1.73796630e+00]
 [1.28750000e+03 1.76614273e+00 1.49575255e-04 1.76600945e+00
  1.76642191e+00]
 [1.35350000e+03 1.79732168e+00 3.53226077e-04 1.79670787e+00
  1.79765308e+00]
 [1.42200000e+03 1.83157554e+00 1.74201484e-04 1.83130479e+00
  1.83178651e+00]
 [1.49300000e+03 1.86678743e+00 4.96147602e-04 1.86625695e+00
  1.86768436e+00]
 [1.56650000e+03 1.90567925e+00 4.66113575e-04 1.90498650e+00
  1.90641713e+00]
 [1.64250000e+03 1.94613733e+00 3.27787080e-04 1.94581556e+00
  1.94672632e+00]
 [1.72100000e+03 1.99095583e+00 5.89839870e-04 1.99005795e+00
  1.99183571e+00]
 [1.80250000e+03 2.03981190e+00 6.54615423e-04 2.03896117e+00
  2.04083037e+00]
 [1.88700000e+03 2.09258847e+00 5.51694398e-04 2.09183717e+00
  2.09349489e+00]
 [1.97450000e+03 2.14981380e+00 9.42345778e-04 2.14861870e+00
  2.15104914e+00]
 [2.06500000e+03 2.21271181e+00 1.08060259e-03 2.21088171e+00
  2.21422935e+00]
 [2.15850000e+03 2.28155661e+00 9.13296853e-04 2.28009200e+00
  2.28284812e+00]
 [2.25550000e+03 2.35656128e+00 5.60726287e-04 2.35561562e+00
  2.35725212e+00]
 [2.35550000e+03 2.43746581e+00 8.14419714e-04 2.43591857e+00
  2.43816137e+00]
 [2.45900000e+03 2.52568378e+00 1.96548967e-03 2.52184081e+00
  2.52706957e+00]
 [2.56600000e+03 2.62349358e+00 2.73417602e-03 2.61888480e+00
  2.62738633e+00]
 [2.67650000e+03 2.73145766e+00 2.28544033e-03 2.72933888e+00
  2.73567486e+00]
 [2.79100000e+03 2.84928360e+00 3.03765402e-03 2.84504795e+00
  2.85270882e+00]
 [2.90900000e+03 2.97872181e+00 4.12914635e-03 2.97082591e+00
  2.98204637e+00]
 [3.03100000e+03 3.12233081e+00 6.72624021e-03 3.10966873e+00
  3.12945747e+00]
 [3.15700000e+03 3.27881875e+00 1.00048578e-02 3.26240253e+00
  3.29256940e+00]
 [3.28700000e+03 3.44583964e+00 1.21583647e-02 3.42847514e+00
  3.46312499e+00]
 [3.42150000e+03 3.62652731e+00 1.07850384e-02 3.61440134e+00
  3.64236999e+00]
 [3.56100000e+03 3.82997141e+00 7.41546049e-03 3.81865954e+00
  3.83988261e+00]
 [3.70500000e+03 4.05724745e+00 2.22211683e-02 4.02744818e+00
  4.09620857e+00]
 [3.85300000e+03 4.30845118e+00 3.00981786e-02 4.27170229e+00
  4.36322212e+00]
 [4.00600000e+03 4.57962255e+00 3.31252430e-02 4.55093718e+00
  4.64271593e+00]
 [4.16450000e+03 4.86564970e+00 3.57996796e-02 4.83490515e+00
  4.93490744e+00]
 [4.32800000e+03 5.16491632e+00 3.81832658e-02 5.12708712e+00
  5.23791075e+00]
 [4.49700000e+03 5.47992878e+00 4.27782528e-02 5.43019867e+00
  5.55216599e+00]
 [4.67150000e+03 5.80747366e+00 5.25087536e-02 5.74400520e+00
  5.87746191e+00]
 [4.85150000e+03 6.14616442e+00 6.57902065e-02 6.06833076e+00
  6.23310518e+00]
 [5.03750000e+03 6.49670353e+00 8.13840576e-02 6.40394163e+00
  6.61819553e+00]
 [5.22950000e+03 6.85894613e+00 9.85623326e-02 6.75070953e+00
  7.01622391e+00]
 [5.45050000e+03 7.27622795e+00 1.19087523e-01 7.15008497e+00
  7.47475624e+00]
 [5.65550000e+03 7.66348457e+00 1.38539121e-01 7.52062130e+00
  7.90027475e+00]
 [5.84400000e+03 8.01964169e+00 1.56638711e-01 7.86127806e+00
  8.29159451e+00]
 [6.06200000e+03 8.43153210e+00 1.77739209e-01 8.25503922e+00
  8.74412441e+00]
 [6.28750000e+03 8.85748577e+00 1.99706386e-01 8.66192913e+00
  9.21211910e+00]
 [6.52000000e+03 9.29641819e+00 2.22490592e-01 9.08075809e+00
  9.69446945e+00]
 [6.76000000e+03 9.74906845e+00 2.46174374e-01 9.51200867e+00
  1.01921406e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_31"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_16 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_90 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_91 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_92 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_93 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_94 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_95 (Dense)             (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0501 - val_loss: 0.0479
Epoch 2/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0483
Epoch 3/100
7493/7493 - 34s - loss: 0.0481 - val_loss: 0.0479
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0486
Epoch 5/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0482
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 27s - loss: 0.0478 - val_loss: 0.0479
Epoch 8/100
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0479
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0479
Epoch 12/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 44s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 38s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 33s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 29s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 31s - loss: 0.0477 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.7
Model: "functional_33"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_17 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_96 (Dense)             (None, 60)                120       
_________________________________________________________________
dense_97 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_98 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_99 (Dense)             (None, 60)                3660      
_________________________________________________________________
dense_100 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_101 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0499 - val_loss: 0.0494
Epoch 2/100
7493/7493 - 38s - loss: 0.0484 - val_loss: 0.0487
Epoch 3/100
7493/7493 - 47s - loss: 0.0482 - val_loss: 0.0479
Epoch 4/100
7493/7493 - 41s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 40s - loss: 0.0481 - val_loss: 0.0482
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 47s - loss: 0.0480 - val_loss: 0.0482
Epoch 7/100
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 31s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 30s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 49s - loss: 0.0478 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_35"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_18 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_102 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_103 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_104 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_105 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_106 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_107 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 42s - loss: 0.0500 - val_loss: 0.0479
Epoch 2/100
7493/7493 - 31s - loss: 0.0484 - val_loss: 0.0478
Epoch 3/100
7493/7493 - 39s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0483
Epoch 5/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 6/100
7493/7493 - 38s - loss: 0.0479 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 32s - loss: 0.0479 - val_loss: 0.0480
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0479 - val_loss: 0.0479
Epoch 9/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 30s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.7
Model: "functional_37"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_19 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_108 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_109 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_110 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_111 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_112 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_113 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0502 - val_loss: 0.0482
Epoch 2/100
7493/7493 - 30s - loss: 0.0485 - val_loss: 0.0480
Epoch 3/100
7493/7493 - 32s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 32s - loss: 0.0481 - val_loss: 0.0482
Epoch 5/100
7493/7493 - 44s - loss: 0.0481 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 43s - loss: 0.0481 - val_loss: 0.0479
Epoch 7/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0488
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 28s - loss: 0.0480 - val_loss: 0.0483
Epoch 9/100
7493/7493 - 29s - loss: 0.0479 - val_loss: 0.0480
Epoch 10/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 18s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 34/100
7493/7493 - 27s - loss: 0.0478 - val_loss: 0.0478
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 36/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda24501f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.7
Model: "functional_39"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_20 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_114 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_115 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_116 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_117 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_118 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_119 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0504 - val_loss: 0.0483
Epoch 2/100
7493/7493 - 40s - loss: 0.0484 - val_loss: 0.0487
Epoch 3/100
7493/7493 - 44s - loss: 0.0482 - val_loss: 0.0480
Epoch 4/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0480
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0480 - val_loss: 0.0480
Epoch 7/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 12/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 29s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0477
Epoch 25/100
7493/7493 - 48s - loss: 0.0478 - val_loss: 0.0477
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 27/100
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0477
Epoch 28/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda20b271e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79573224e+00 1.04920630e-04 1.79553425e+00
  1.79584527e+00]
 [1.28750000e+03 1.82495284e+00 2.61746380e-04 1.82469952e+00
  1.82535136e+00]
 [1.35350000e+03 1.85847034e+00 3.68000976e-04 1.85812843e+00
  1.85914958e+00]
 [1.42200000e+03 1.89460018e+00 3.65453349e-04 1.89413357e+00
  1.89522421e+00]
 [1.49300000e+03 1.93333230e+00 6.11612070e-04 1.93233502e+00
  1.93386173e+00]
 [1.56650000e+03 1.97605882e+00 8.26121221e-04 1.97504032e+00
  1.97724223e+00]
 [1.64250000e+03 2.02104840e+00 1.80177992e-04 2.02082658e+00
  2.02129149e+00]
 [1.72100000e+03 2.07067156e+00 5.55625278e-04 2.07011771e+00
  2.07166290e+00]
 [1.80250000e+03 2.12574973e+00 3.96143288e-04 2.12517548e+00
  2.12616229e+00]
 [1.88700000e+03 2.18576365e+00 8.63802580e-04 2.18470812e+00
  2.18725085e+00]
 [1.97450000e+03 2.25132284e+00 1.23235074e-03 2.24947691e+00
  2.25303817e+00]
 [2.06500000e+03 2.32287517e+00 1.54594963e-03 2.31982517e+00
  2.32406425e+00]
 [2.15850000e+03 2.40093465e+00 1.82877289e-03 2.39841628e+00
  2.40245891e+00]
 [2.25550000e+03 2.48656602e+00 2.28310426e-03 2.48268700e+00
  2.48856783e+00]
 [2.35550000e+03 2.58061681e+00 1.07272468e-03 2.57935858e+00
  2.58212543e+00]
 [2.45900000e+03 2.68307419e+00 1.25609607e-03 2.68108582e+00
  2.68492079e+00]
 [2.56600000e+03 2.79521108e+00 1.90041554e-03 2.79286814e+00
  2.79862165e+00]
 [2.67650000e+03 2.91843929e+00 3.45601870e-03 2.91252518e+00
  2.92271471e+00]
 [2.79100000e+03 3.05376019e+00 4.15711478e-03 3.04574060e+00
  3.05739021e+00]
 [2.90900000e+03 3.20173812e+00 4.65703785e-03 3.19243121e+00
  3.20423484e+00]
 [3.03100000e+03 3.36475048e+00 5.91845498e-03 3.35326385e+00
  3.36932683e+00]
 [3.15700000e+03 3.54347043e+00 6.99702058e-03 3.53203082e+00
  3.55204463e+00]
 [3.28700000e+03 3.74075727e+00 8.31521961e-03 3.73296261e+00
  3.75575805e+00]
 [3.42150000e+03 3.95641427e+00 6.63918808e-03 3.95242834e+00
  3.96964526e+00]
 [3.56100000e+03 4.19260225e+00 5.68703025e-03 4.18401432e+00
  4.20007610e+00]
 [3.70500000e+03 4.45418005e+00 1.66529595e-02 4.43810034e+00
  4.47967720e+00]
 [3.85300000e+03 4.73868303e+00 3.10175947e-02 4.69752502e+00
  4.78588200e+00]
 [4.00600000e+03 5.04442234e+00 4.55800722e-02 4.96972275e+00
  5.10423708e+00]
 [4.16450000e+03 5.36749477e+00 6.01824057e-02 5.26244640e+00
  5.43531895e+00]
 [4.32800000e+03 5.70862741e+00 6.62905641e-02 5.59559393e+00
  5.77768564e+00]
 [4.49700000e+03 6.07611475e+00 5.32924387e-02 6.00951242e+00
  6.13216496e+00]
 [4.67150000e+03 6.45918732e+00 5.00479486e-02 6.37128592e+00
  6.51568508e+00]
 [4.85150000e+03 6.85525408e+00 6.56417440e-02 6.73764515e+00
  6.91574478e+00]
 [5.03750000e+03 7.26517296e+00 9.30037065e-02 7.11613846e+00
  7.38838863e+00]
 [5.22950000e+03 7.68876858e+00 1.26170095e-01 7.50655317e+00
  7.88252115e+00]
 [5.45050000e+03 8.17668629e+00 1.67107886e-01 7.95545959e+00
  8.45284462e+00]
 [5.65550000e+03 8.62948437e+00 2.06525850e-01 8.37136269e+00
  8.98305225e+00]
 [5.84400000e+03 9.04595051e+00 2.43579214e-01 8.75335979e+00
  9.47144222e+00]
 [6.06200000e+03 9.52769547e+00 2.87179192e-01 9.19464207e+00
  1.00372496e+01]
 [6.28750000e+03 1.00261065e+01 3.32996750e-01 9.65056896e+00
  1.06236200e+01]
 [6.52000000e+03 1.05399426e+01 3.80689807e-01 1.01201124e+01
  1.12287149e+01]
 [6.76000000e+03 1.10702370e+01 4.30277100e-01 1.06042538e+01
  1.18536692e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_41"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_21 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_120 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_121 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_122 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_123 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_124 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_125 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0281 - val_loss: 0.0262
Epoch 2/100
7493/7493 - 37s - loss: 0.0257 - val_loss: 0.0260
Epoch 3/100
7493/7493 - 45s - loss: 0.0255 - val_loss: 0.0256
Epoch 4/100
7493/7493 - 37s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 36s - loss: 0.0254 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 33s - loss: 0.0254 - val_loss: 0.0254
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0254 - val_loss: 0.0255
Epoch 8/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 46s - loss: 0.0252 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 29s - loss: 0.0252 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 30s - loss: 0.0252 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 30s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 39s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 47s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 42s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 39s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 32s - loss: 0.0252 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0252 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 45s - loss: 0.0252 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 39s - loss: 0.0252 - val_loss: 0.0253
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 46s - loss: 0.0252 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 34/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 32s - loss: 0.0252 - val_loss: 0.0253
Epoch 36/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 37/100
7493/7493 - 43s - loss: 0.0252 - val_loss: 0.0253
Epoch 38/100

Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 46s - loss: 0.0252 - val_loss: 0.0253
Epoch 00038: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.9
Model: "functional_43"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_22 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_126 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_127 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_128 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_129 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_130 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_131 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 38s - loss: 0.0279 - val_loss: 0.0262
Epoch 2/100
7493/7493 - 36s - loss: 0.0257 - val_loss: 0.0257
Epoch 3/100
7493/7493 - 45s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 30s - loss: 0.0255 - val_loss: 0.0253
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 38s - loss: 0.0254 - val_loss: 0.0256
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_45"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_23 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_132 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_133 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_134 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_135 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_136 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_137 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0281 - val_loss: 0.0262
Epoch 2/100
7493/7493 - 38s - loss: 0.0257 - val_loss: 0.0258
Epoch 3/100
7493/7493 - 43s - loss: 0.0256 - val_loss: 0.0255
Epoch 4/100
7493/7493 - 37s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0256
Epoch 6/100
7493/7493 - 44s - loss: 0.0255 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0255 - val_loss: 0.0254
Epoch 8/100
7493/7493 - 28s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 35/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 36/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 37/100

Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 00037: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.9
Model: "functional_47"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_24 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_138 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_139 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_140 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_141 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_142 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_143 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0277 - val_loss: 0.0254
Epoch 2/100
7493/7493 - 33s - loss: 0.0257 - val_loss: 0.0257
Epoch 3/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cd7b378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.9
Model: "functional_49"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_25 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_144 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_145 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_146 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_147 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_148 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_149 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0277 - val_loss: 0.0265
Epoch 2/100
7493/7493 - 44s - loss: 0.0257 - val_loss: 0.0253
Epoch 3/100
7493/7493 - 37s - loss: 0.0255 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0259
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 47s - loss: 0.0255 - val_loss: 0.0256
Epoch 6/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0252
Epoch 10/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 12/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0252
Epoch 13/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 14/100
7493/7493 - 24s - loss: 0.0253 - val_loss: 0.0252
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0252
Epoch 17/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0252
Epoch 19/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 20/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0252
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 22/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 23/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0252
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 25/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 26/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0252
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 28/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 29/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0252
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 31/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0252
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_30_loss_rk5_05_20210923_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda23c9de18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88113840e+00 3.89989780e-04 1.88042808e+00
  1.88153255e+00]
 [1.28750000e+03 1.91383948e+00 4.80521508e-04 1.91308939e+00
  1.91434145e+00]
 [1.35350000e+03 1.95095665e+00 3.69161685e-04 1.95035458e+00
  1.95148468e+00]
 [1.42200000e+03 1.99178398e+00 7.00948291e-04 1.99054050e+00
  1.99270022e+00]
 [1.49300000e+03 2.03736930e+00 3.48130884e-04 2.03670669e+00
  2.03766656e+00]
 [1.56650000e+03 2.08689117e+00 1.11560387e-03 2.08538389e+00
  2.08868694e+00]
 [1.64250000e+03 2.14019566e+00 4.19957916e-04 2.13973808e+00
  2.14074397e+00]
 [1.72100000e+03 2.19997315e+00 5.48502664e-04 2.19923973e+00
  2.20055461e+00]
 [1.80250000e+03 2.26639037e+00 6.58427194e-04 2.26521969e+00
  2.26716447e+00]
 [1.88700000e+03 2.33891416e+00 4.23932853e-04 2.33835912e+00
  2.33939481e+00]
 [1.97450000e+03 2.41808219e+00 7.26121158e-04 2.41699815e+00
  2.41902304e+00]
 [2.06500000e+03 2.50459213e+00 1.07378173e-03 2.50286984e+00
  2.50555134e+00]
 [2.15850000e+03 2.59907851e+00 1.72052630e-03 2.59603667e+00
  2.60111880e+00]
 [2.25550000e+03 2.70266981e+00 2.01822469e-03 2.69890165e+00
  2.70466566e+00]
 [2.35550000e+03 2.81723051e+00 1.72475162e-03 2.81505752e+00
  2.82033682e+00]
 [2.45900000e+03 2.94458756e+00 2.57262786e-03 2.94019723e+00
  2.94736981e+00]
 [2.56600000e+03 3.08297949e+00 3.74729183e-03 3.07643676e+00
  3.08789086e+00]
 [2.67650000e+03 3.23296618e+00 5.16804860e-03 3.22604990e+00
  3.24045038e+00]
 [2.79100000e+03 3.39635997e+00 8.23687572e-03 3.38854265e+00
  3.41167688e+00]
 [2.90900000e+03 3.57565837e+00 1.08572889e-02 3.56745839e+00
  3.59622121e+00]
 [3.03100000e+03 3.78240118e+00 8.23279313e-03 3.76919341e+00
  3.79410887e+00]
 [3.15700000e+03 4.01820707e+00 8.16916027e-03 4.00797606e+00
  4.02896643e+00]
 [3.28700000e+03 4.27307100e+00 1.28746097e-02 4.24917793e+00
  4.28747749e+00]
 [3.42150000e+03 4.54397879e+00 1.98460636e-02 4.51160431e+00
  4.57001829e+00]
 [3.56100000e+03 4.83021498e+00 3.03551542e-02 4.78809881e+00
  4.87966728e+00]
 [3.70500000e+03 5.13247747e+00 4.27322552e-02 5.08088589e+00
  5.20806551e+00]
 [3.85300000e+03 5.45537777e+00 5.24990409e-02 5.40523386e+00
  5.55334568e+00]
 [4.00600000e+03 5.81323442e+00 5.52575172e-02 5.76105022e+00
  5.91296053e+00]
 [4.16450000e+03 6.20690269e+00 6.95434625e-02 6.11333609e+00
  6.28709745e+00]
 [4.32800000e+03 6.63624601e+00 9.81164657e-02 6.47819996e+00
  6.73708630e+00]
 [4.49700000e+03 7.08288012e+00 1.38837243e-01 6.85842896e+00
  7.22880030e+00]
 [4.67150000e+03 7.54538536e+00 1.85046376e-01 7.25238371e+00
  7.75183249e+00]
 [4.85150000e+03 8.02339401e+00 2.34319388e-01 7.65997982e+00
  8.29185104e+00]
 [5.03750000e+03 8.51797085e+00 2.85771584e-01 8.08251476e+00
  8.84952068e+00]
 [5.22950000e+03 9.02858047e+00 3.39025674e-01 8.51912212e+00
  9.42447567e+00]
 [5.45050000e+03 9.61551361e+00 3.99641940e-01 9.02174282e+00
  1.00847607e+01]
 [5.65550000e+03 1.01575590e+01 4.53592551e-01 9.48779392e+00
  1.06948757e+01]
 [5.84400000e+03 1.06512594e+01 4.98779370e-01 9.91583157e+00
  1.12526665e+01]
 [6.06200000e+03 1.12132772e+01 5.43305725e-01 1.04094734e+01
  1.18916044e+01]
 [6.28750000e+03 1.17932514e+01 5.88391786e-01 1.09166250e+01
  1.25412560e+01]
 [6.52000000e+03 1.23875875e+01 6.35661229e-01 1.14305286e+01
  1.31909761e+01]
 [6.76000000e+03 1.29858549e+01 6.84979897e-01 1.19379358e+01
  1.38294630e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918006 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_51"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_26 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_150 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_151 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_152 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_153 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_154 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_155 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 38s - loss: 0.0055 - val_loss: 0.0045
Epoch 2/100
7493/7493 - 40s - loss: 0.0044 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 37s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 00020: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918006

training QR for quantile 0.99
Model: "functional_53"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_27 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_156 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_157 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_158 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_159 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_160 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_161 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 44s - loss: 0.0057 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 38s - loss: 0.0045 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0046
Epoch 4/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0044 - val_loss: 0.0045
Epoch 6/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 49s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918006 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_55"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_28 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_162 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_163 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_164 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_165 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_166 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_167 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 48s - loss: 0.0057 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 36s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 43s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 39s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 29s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 48s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 26s - loss: 0.0043 - val_loss: 0.0043
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
training on 1918007 events, validating on 1918007

training QR for quantile 0.99
Model: "functional_57"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_29 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_168 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_169 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_170 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_171 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_172 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_173 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 32s - loss: 0.0057 - val_loss: 0.0044
Epoch 2/100
7493/7493 - 36s - loss: 0.0045 - val_loss: 0.0045
Epoch 3/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0043
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda304711e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918007 events, validating on 1918006

training QR for quantile 0.99
Model: "functional_59"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_30 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_174 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_175 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_176 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_177 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_178 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_179 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0058 - val_loss: 0.0044
Epoch 2/100
7493/7493 - 43s - loss: 0.0045 - val_loss: 0.0045
Epoch 3/100
7493/7493 - 40s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 41s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 33/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 34/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 36/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_30_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda30471510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.99: 
[[1.22750000e+03 2.01610641e+00 1.00178959e-03 2.01493335e+00
  2.01785493e+00]
 [1.28750000e+03 2.05625052e+00 1.24181400e-03 2.05458045e+00
  2.05808234e+00]
 [1.35350000e+03 2.10468669e+00 1.01058367e-03 2.10382247e+00
  2.10648608e+00]
 [1.42200000e+03 2.15475135e+00 9.10308255e-04 2.15370488e+00
  2.15616965e+00]
 [1.49300000e+03 2.21182213e+00 2.25272738e-03 2.20876145e+00
  2.21571398e+00]
 [1.56650000e+03 2.27638321e+00 1.08463497e-03 2.27506876e+00
  2.27787542e+00]
 [1.64250000e+03 2.34617133e+00 1.31466970e-03 2.34454370e+00
  2.34794879e+00]
 [1.72100000e+03 2.42378898e+00 1.45776621e-03 2.42135167e+00
  2.42589235e+00]
 [1.80250000e+03 2.50985332e+00 2.22598713e-03 2.50764441e+00
  2.51329231e+00]
 [1.88700000e+03 2.60479112e+00 1.90165398e-03 2.60226321e+00
  2.60689521e+00]
 [1.97450000e+03 2.70763793e+00 3.23977929e-03 2.70412111e+00
  2.71334743e+00]
 [2.06500000e+03 2.82096477e+00 5.11936259e-03 2.81441784e+00
  2.82882380e+00]
 [2.15850000e+03 2.94829278e+00 5.71001266e-03 2.94049978e+00
  2.95541930e+00]
 [2.25550000e+03 3.08915691e+00 5.97039443e-03 3.08170485e+00
  3.09819245e+00]
 [2.35550000e+03 3.24346600e+00 6.39596043e-03 3.23732519e+00
  3.25507689e+00]
 [2.45900000e+03 3.41231251e+00 6.45734038e-03 3.40134788e+00
  3.42087340e+00]
 [2.56600000e+03 3.59586172e+00 6.06380498e-03 3.58413887e+00
  3.60092974e+00]
 [2.67650000e+03 3.79638162e+00 1.11384941e-02 3.78151846e+00
  3.81323719e+00]
 [2.79100000e+03 4.01205382e+00 2.24462105e-02 3.97930264e+00
  4.04709291e+00]
 [2.90900000e+03 4.24239292e+00 3.35642418e-02 4.19305420e+00
  4.29368019e+00]
 [3.03100000e+03 4.49370708e+00 3.96867136e-02 4.43747663e+00
  4.55406094e+00]
 [3.15700000e+03 4.78222504e+00 3.19813060e-02 4.74652529e+00
  4.82840347e+00]
 [3.28700000e+03 5.11084976e+00 3.91565457e-02 5.04291868e+00
  5.16535044e+00]
 [3.42150000e+03 5.47131977e+00 6.03156456e-02 5.39898443e+00
  5.56998205e+00]
 [3.56100000e+03 5.86471176e+00 8.12558656e-02 5.74781752e+00
  5.99336290e+00]
 [3.70500000e+03 6.27780828e+00 1.12731159e-01 6.09718037e+00
  6.43336439e+00]
 [3.85300000e+03 6.71051779e+00 1.42850053e-01 6.48264551e+00
  6.88847446e+00]
 [4.00600000e+03 7.17276449e+00 1.58696884e-01 6.94138336e+00
  7.36263514e+00]
 [4.16450000e+03 7.66619177e+00 1.64268444e-01 7.47017002e+00
  7.85970736e+00]
 [4.32800000e+03 8.18192921e+00 1.72189016e-01 7.97066212e+00
  8.38049698e+00]
 [4.49700000e+03 8.72183094e+00 1.76729445e-01 8.50196743e+00
  8.93977070e+00]
 [4.67150000e+03 9.27827911e+00 1.85850025e-01 9.06293869e+00
  9.52220726e+00]
 [4.85150000e+03 9.84859428e+00 2.06689016e-01 9.62283707e+00
  1.01239576e+01]
 [5.03750000e+03 1.04415413e+01 2.31919113e-01 1.01393480e+01
  1.07467899e+01]
 [5.22950000e+03 1.10577789e+01 2.56777144e-01 1.06947460e+01
  1.13909283e+01]
 [5.45050000e+03 1.17672028e+01 2.88504732e-01 1.13352652e+01
  1.21342163e+01]
 [5.65550000e+03 1.24253805e+01 3.20240847e-01 1.19298515e+01
  1.28259172e+01]
 [5.84400000e+03 1.30304623e+01 3.50539756e-01 1.24769716e+01
  1.34629498e+01]
 [6.06200000e+03 1.37295938e+01 3.86008893e-01 1.31101847e+01
  1.41997013e+01]
 [6.28750000e+03 1.44513195e+01 4.22316602e-01 1.37657452e+01
  1.49615221e+01]
 [6.52000000e+03 1.51928101e+01 4.58326995e-01 1.44423037e+01
  1.57466516e+01]
 [6.76000000e+03 1.59533312e+01 4.92553083e-01 1.51414471e+01
  1.65567074e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.1
Model: "functional_61"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_31 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_180 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_181 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_182 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_183 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_184 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_185 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0249 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 46s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 37s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 31s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0238 - val_loss: 0.0238
Epoch 8/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0238
Epoch 9/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 48s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 26s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 39s - loss: 0.0236 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 28s - loss: 0.0236 - val_loss: 0.0236
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.1
Model: "functional_63"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_32 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_186 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_187 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_188 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_189 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_190 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_191 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0248 - val_loss: 0.0237
Epoch 2/100
7493/7493 - 46s - loss: 0.0240 - val_loss: 0.0239
Epoch 3/100
7493/7493 - 35s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0238
Epoch 5/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 42s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 40s - loss: 0.0236 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.1
Model: "functional_65"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_33 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_192 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_193 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_194 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_195 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_196 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_197 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0248 - val_loss: 0.0239
Epoch 2/100
7493/7493 - 34s - loss: 0.0240 - val_loss: 0.0240
Epoch 3/100
7493/7493 - 50s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 47s - loss: 0.0238 - val_loss: 0.0239
Epoch 5/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0238 - val_loss: 0.0240
Epoch 7/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 31s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.1
Model: "functional_67"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_34 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_198 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_199 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_200 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_201 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_202 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_203 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0250 - val_loss: 0.0237
Epoch 2/100
7493/7493 - 46s - loss: 0.0240 - val_loss: 0.0241
Epoch 3/100
7493/7493 - 36s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 31s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 6/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 9/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 42s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 42s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 46s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 40s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 39s - loss: 0.0237 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 29s - loss: 0.0237 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 28s - loss: 0.0237 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0236
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2c3ebea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.1
Model: "functional_69"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_35 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_204 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_205 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_206 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_207 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_208 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_209 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0248 - val_loss: 0.0244
Epoch 2/100
7493/7493 - 29s - loss: 0.0240 - val_loss: 0.0237
Epoch 3/100
7493/7493 - 39s - loss: 0.0239 - val_loss: 0.0238
Epoch 4/100
7493/7493 - 37s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 49s - loss: 0.0238 - val_loss: 0.0239
Epoch 6/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0238
Epoch 7/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0237
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 29s - loss: 0.0236 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0236
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 28s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 30s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 31s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 33s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 40s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 31/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2c1e8268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58442831e+00 2.05303760e-04 1.58415020e+00
  1.58466494e+00]
 [1.28750000e+03 1.61246805e+00 2.85267562e-04 1.61203039e+00
  1.61292112e+00]
 [1.35350000e+03 1.64317100e+00 4.04509779e-04 1.64270222e+00
  1.64372444e+00]
 [1.42200000e+03 1.67526298e+00 4.61457953e-04 1.67461801e+00
  1.67595172e+00]
 [1.49300000e+03 1.70731795e+00 8.21058365e-04 1.70598900e+00
  1.70854127e+00]
 [1.56650000e+03 1.74184692e+00 7.04438435e-04 1.74069870e+00
  1.74284363e+00]
 [1.64250000e+03 1.77697821e+00 4.47811695e-04 1.77650702e+00
  1.77766871e+00]
 [1.72100000e+03 1.81290505e+00 3.17325862e-04 1.81240547e+00
  1.81325173e+00]
 [1.80250000e+03 1.85121853e+00 3.53344222e-04 1.85080528e+00
  1.85170615e+00]
 [1.88700000e+03 1.89275012e+00 8.52990309e-04 1.89134324e+00
  1.89360154e+00]
 [1.97450000e+03 1.93694279e+00 1.02446724e-03 1.93566263e+00
  1.93817019e+00]
 [2.06500000e+03 1.98378308e+00 1.03162513e-03 1.98209083e+00
  1.98487568e+00]
 [2.15850000e+03 2.03388734e+00 1.44399394e-03 2.03120041e+00
  2.03518677e+00]
 [2.25550000e+03 2.08804340e+00 1.74144593e-03 2.08494568e+00
  2.08988667e+00]
 [2.35550000e+03 2.14652023e+00 1.40357520e-03 2.14471817e+00
  2.14860368e+00]
 [2.45900000e+03 2.20974479e+00 1.26301860e-03 2.20792603e+00
  2.21182323e+00]
 [2.56600000e+03 2.27830081e+00 1.15141960e-03 2.27624393e+00
  2.27978206e+00]
 [2.67650000e+03 2.35300426e+00 1.07940612e-03 2.35180545e+00
  2.35483456e+00]
 [2.79100000e+03 2.43500853e+00 2.92796952e-03 2.42976570e+00
  2.43817759e+00]
 [2.90900000e+03 2.52510715e+00 4.57645555e-03 2.51646638e+00
  2.52924848e+00]
 [3.03100000e+03 2.62656035e+00 5.97997629e-03 2.61667633e+00
  2.63264966e+00]
 [3.15700000e+03 2.73801208e+00 5.84437673e-03 2.73194385e+00
  2.74601603e+00]
 [3.28700000e+03 2.85919704e+00 4.29608102e-03 2.85287452e+00
  2.86549711e+00]
 [3.42150000e+03 2.99174757e+00 4.72624500e-03 2.98704958e+00
  3.00059104e+00]
 [3.56100000e+03 3.13673172e+00 1.04226986e-02 3.12412977e+00
  3.15261388e+00]
 [3.70500000e+03 3.29343777e+00 1.58758402e-02 3.27222753e+00
  3.31381917e+00]
 [3.85300000e+03 3.46232300e+00 1.74217195e-02 3.43318105e+00
  3.48193598e+00]
 [4.00600000e+03 3.64366817e+00 1.31571975e-02 3.62330008e+00
  3.65782475e+00]
 [4.16450000e+03 3.83827486e+00 1.09056043e-02 3.81741333e+00
  3.84773898e+00]
 [4.32800000e+03 4.04013329e+00 2.41496875e-02 4.00522423e+00
  4.08036184e+00]
 [4.49700000e+03 4.24952507e+00 3.99983585e-02 4.20009613e+00
  4.32104683e+00]
 [4.67150000e+03 4.46616821e+00 5.66939009e-02 4.40163565e+00
  4.56971788e+00]
 [4.85150000e+03 4.68988266e+00 7.40272271e-02 4.60961533e+00
  4.82629919e+00]
 [5.03750000e+03 4.92119226e+00 9.19834628e-02 4.82452011e+00
  5.09143543e+00]
 [5.22950000e+03 5.16001415e+00 1.10534506e-01 5.04628658e+00
  5.36504745e+00]
 [5.45050000e+03 5.43487329e+00 1.31876762e-01 5.30141640e+00
  5.67978096e+00]
 [5.65550000e+03 5.68972979e+00 1.51633384e-01 5.53792620e+00
  5.97142935e+00]
 [5.84400000e+03 5.92390385e+00 1.69729810e-01 5.75526381e+00
  6.23925114e+00]
 [6.06200000e+03 6.19445486e+00 1.90535993e-01 6.00645065e+00
  6.54844522e+00]
 [6.28750000e+03 6.47395239e+00 2.11871314e-01 6.26608467e+00
  6.86750078e+00]
 [6.52000000e+03 6.76165800e+00 2.33609608e-01 6.53357887e+00
  7.19541168e+00]
 [6.76000000e+03 7.05804520e+00 2.55691511e-01 6.80950165e+00
  7.53249407e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.3
Model: "functional_71"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_36 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_210 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_211 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_212 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_213 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_214 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_215 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0470
Epoch 2/100
7493/7493 - 44s - loss: 0.0470 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 37s - loss: 0.0468 - val_loss: 0.0469
Epoch 4/100
7493/7493 - 34s - loss: 0.0468 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 44s - loss: 0.0468 - val_loss: 0.0473
Epoch 6/100
7493/7493 - 38s - loss: 0.0467 - val_loss: 0.0469
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0467 - val_loss: 0.0468
Epoch 8/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 49s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 33s - loss: 0.0465 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 35s - loss: 0.0465 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 32s - loss: 0.0465 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 32s - loss: 0.0465 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 45s - loss: 0.0465 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 42s - loss: 0.0465 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0465 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 31s - loss: 0.0465 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0465 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 46s - loss: 0.0465 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 36s - loss: 0.0465 - val_loss: 0.0466
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.3
Model: "functional_73"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_37 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_216 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_217 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_218 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_219 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_220 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_221 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0485 - val_loss: 0.0480
Epoch 2/100
7493/7493 - 28s - loss: 0.0472 - val_loss: 0.0470
Epoch 3/100
7493/7493 - 34s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 44s - loss: 0.0469 - val_loss: 0.0473
Epoch 5/100
7493/7493 - 44s - loss: 0.0468 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 42s - loss: 0.0468 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 46s - loss: 0.0468 - val_loss: 0.0468
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 38s - loss: 0.0468 - val_loss: 0.0468
Epoch 9/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 25s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.3
Model: "functional_75"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_38 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_222 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_223 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_224 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_225 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_226 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_227 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0488 - val_loss: 0.0468
Epoch 2/100
7493/7493 - 43s - loss: 0.0472 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 46s - loss: 0.0470 - val_loss: 0.0473
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0469 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 46s - loss: 0.0467 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 43s - loss: 0.0467 - val_loss: 0.0469
Epoch 7/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0467 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 26s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0466
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.3
Model: "functional_77"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_39 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_228 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_229 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_230 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_231 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_232 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_233 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 32s - loss: 0.0488 - val_loss: 0.0474
Epoch 2/100
7493/7493 - 34s - loss: 0.0471 - val_loss: 0.0473
Epoch 3/100
7493/7493 - 48s - loss: 0.0470 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 36s - loss: 0.0469 - val_loss: 0.0468
Epoch 5/100
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0467
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 44s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 44s - loss: 0.0467 - val_loss: 0.0466
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cf34730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.3
Model: "functional_79"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_40 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_234 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_235 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_236 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_237 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_238 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_239 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0489 - val_loss: 0.0477
Epoch 2/100
7493/7493 - 37s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 37s - loss: 0.0470 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 33s - loss: 0.0469 - val_loss: 0.0466
Epoch 5/100
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0473
Epoch 6/100
7493/7493 - 44s - loss: 0.0468 - val_loss: 0.0466
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100
7493/7493 - 40s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100
7493/7493 - 24s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0465
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2ea63268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67797165e+00 5.94602553e-05 1.67789853e+00
  1.67805588e+00]
 [1.28750000e+03 1.70610621e+00 3.54870749e-04 1.70570254e+00
  1.70667756e+00]
 [1.35350000e+03 1.73648977e+00 5.72000562e-04 1.73565936e+00
  1.73718750e+00]
 [1.42200000e+03 1.76914065e+00 9.90718448e-05 1.76900375e+00
  1.76926541e+00]
 [1.49300000e+03 1.80199845e+00 5.17426586e-04 1.80128956e+00
  1.80262923e+00]
 [1.56650000e+03 1.83853471e+00 3.74333258e-04 1.83789766e+00
  1.83885348e+00]
 [1.64250000e+03 1.87580497e+00 5.55941131e-04 1.87498832e+00
  1.87661374e+00]
 [1.72100000e+03 1.91615219e+00 4.34083192e-04 1.91562700e+00
  1.91673899e+00]
 [1.80250000e+03 1.96017141e+00 4.22453144e-04 1.95949411e+00
  1.96074998e+00]
 [1.88700000e+03 2.00739636e+00 3.73643999e-04 2.00685191e+00
  2.00798869e+00]
 [1.97450000e+03 2.05809841e+00 7.58946304e-04 2.05686283e+00
  2.05908775e+00]
 [2.06500000e+03 2.11304832e+00 8.58155794e-04 2.11215115e+00
  2.11440730e+00]
 [2.15850000e+03 2.17342234e+00 9.57708220e-04 2.17182016e+00
  2.17463636e+00]
 [2.25550000e+03 2.23893342e+00 1.33068165e-03 2.23723078e+00
  2.24083304e+00]
 [2.35550000e+03 2.31039653e+00 1.78700516e-03 2.30711389e+00
  2.31251764e+00]
 [2.45900000e+03 2.38813491e+00 2.50741308e-03 2.38472867e+00
  2.39106393e+00]
 [2.56600000e+03 2.47253118e+00 2.44504557e-03 2.46956301e+00
  2.47634792e+00]
 [2.67650000e+03 2.56438985e+00 2.97180682e-03 2.56161857e+00
  2.56810641e+00]
 [2.79100000e+03 2.66543493e+00 5.06366202e-03 2.65852213e+00
  2.67397022e+00]
 [2.90900000e+03 2.77646074e+00 6.13879683e-03 2.76963520e+00
  2.78726268e+00]
 [3.03100000e+03 2.89829206e+00 6.75803259e-03 2.88973022e+00
  2.90867782e+00]
 [3.15700000e+03 3.03307862e+00 7.03109017e-03 3.02195001e+00
  3.04118538e+00]
 [3.28700000e+03 3.18107100e+00 8.79731813e-03 3.16956902e+00
  3.18995953e+00]
 [3.42150000e+03 3.34664426e+00 8.16138592e-03 3.33181882e+00
  3.35596681e+00]
 [3.56100000e+03 3.52735348e+00 7.10449878e-03 3.51829600e+00
  3.53496099e+00]
 [3.70500000e+03 3.72049823e+00 8.77283881e-03 3.70858836e+00
  3.73446393e+00]
 [3.85300000e+03 3.92873330e+00 1.09418491e-02 3.91706681e+00
  3.94712687e+00]
 [4.00600000e+03 4.15515089e+00 1.66516181e-02 4.12759113e+00
  4.17916536e+00]
 [4.16450000e+03 4.39766836e+00 2.28577543e-02 4.35588789e+00
  4.42188549e+00]
 [4.32800000e+03 4.65219088e+00 2.74211824e-02 4.60278654e+00
  4.68100786e+00]
 [4.49700000e+03 4.91779280e+00 3.32259216e-02 4.85959911e+00
  4.95882368e+00]
 [4.67150000e+03 5.19503965e+00 4.13185031e-02 5.12538338e+00
  5.24679899e+00]
 [4.85150000e+03 5.48448763e+00 5.35963521e-02 5.39996576e+00
  5.54482508e+00]
 [5.03750000e+03 5.78448534e+00 6.86489180e-02 5.68396568e+00
  5.86845207e+00]
 [5.22950000e+03 6.09451990e+00 8.53179566e-02 5.97726059e+00
  6.20965338e+00]
 [5.45050000e+03 6.45161591e+00 1.05211227e-01 6.31485987e+00
  6.60250568e+00]
 [5.65550000e+03 6.78291731e+00 1.24000289e-01 6.62789774e+00
  6.96680069e+00]
 [5.84400000e+03 7.08748808e+00 1.41406688e-01 6.91554022e+00
  7.30151272e+00]
 [6.06200000e+03 7.43955450e+00 1.61599512e-01 7.24785519e+00
  7.68811560e+00]
 [6.28750000e+03 7.80343437e+00 1.82482448e-01 7.59110689e+00
  8.08725071e+00]
 [6.52000000e+03 8.17815800e+00 2.03959273e-01 7.94433308e+00
  8.49771881e+00]
 [6.76000000e+03 8.56430283e+00 2.26046503e-01 8.30803204e+00
  8.92000103e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.5
Model: "functional_81"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_41 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_240 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_241 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_242 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_243 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_244 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_245 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0560 - val_loss: 0.0540
Epoch 2/100
7493/7493 - 34s - loss: 0.0545 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 35s - loss: 0.0543 - val_loss: 0.0542
Epoch 4/100
7493/7493 - 43s - loss: 0.0541 - val_loss: 0.0539
Epoch 5/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0542
Epoch 6/100
7493/7493 - 35s - loss: 0.0541 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0540 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0540
Epoch 9/100
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 44s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 27s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 18s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 12s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 12s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 11s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 11s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 19s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 30/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0539
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.5
Model: "functional_83"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_42 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_246 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_247 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_248 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_249 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_250 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_251 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 43s - loss: 0.0561 - val_loss: 0.0541
Epoch 2/100
7493/7493 - 37s - loss: 0.0545 - val_loss: 0.0547
Epoch 3/100
7493/7493 - 37s - loss: 0.0543 - val_loss: 0.0542
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0541
Epoch 5/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 42s - loss: 0.0540 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 38s - loss: 0.0540 - val_loss: 0.0540
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 38s - loss: 0.0540 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.5
Model: "functional_85"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_43 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_252 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_253 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_254 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_255 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_256 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_257 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0559 - val_loss: 0.0545
Epoch 2/100
7493/7493 - 36s - loss: 0.0545 - val_loss: 0.0550
Epoch 3/100
7493/7493 - 43s - loss: 0.0543 - val_loss: 0.0540
Epoch 4/100
7493/7493 - 42s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 38s - loss: 0.0542 - val_loss: 0.0541
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0541 - val_loss: 0.0541
Epoch 7/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 8/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0540
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.5
Model: "functional_87"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_44 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_258 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_259 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_260 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_261 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_262 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_263 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0562 - val_loss: 0.0542
Epoch 2/100
7493/7493 - 35s - loss: 0.0545 - val_loss: 0.0543
Epoch 3/100
7493/7493 - 37s - loss: 0.0544 - val_loss: 0.0541
Epoch 4/100
7493/7493 - 46s - loss: 0.0542 - val_loss: 0.0539
Epoch 5/100
7493/7493 - 46s - loss: 0.0542 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0541 - val_loss: 0.0539
Epoch 8/100
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0540
Epoch 11/100
7493/7493 - 43s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 40s - loss: 0.0539 - val_loss: 0.0538
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0538
Epoch 32/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0538
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cd7bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.5
Model: "functional_89"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_45 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_264 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_265 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_266 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_267 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_268 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_269 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0565 - val_loss: 0.0539
Epoch 2/100
7493/7493 - 36s - loss: 0.0545 - val_loss: 0.0542
Epoch 3/100
7493/7493 - 39s - loss: 0.0542 - val_loss: 0.0543
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0541 - val_loss: 0.0540
Epoch 5/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 26s - loss: 0.0539 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 8/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0538
Epoch 9/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0538
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0538
Epoch 11/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 14/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0538
Epoch 17/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 29s - loss: 0.0538 - val_loss: 0.0538
Epoch 20/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0538
Epoch 23/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0538
Epoch 26/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 29/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cfbf2f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73774569e+00 1.45476027e-04 1.73746824e+00
  1.73787534e+00]
 [1.28750000e+03 1.76614041e+00 2.34203676e-04 1.76581049e+00
  1.76650000e+00]
 [1.35350000e+03 1.79740541e+00 4.35931278e-04 1.79695129e+00
  1.79811323e+00]
 [1.42200000e+03 1.83154514e+00 1.97959097e-04 1.83132946e+00
  1.83190584e+00]
 [1.49300000e+03 1.86704893e+00 1.07234268e-04 1.86692274e+00
  1.86719322e+00]
 [1.56650000e+03 1.90558550e+00 2.05439182e-04 1.90531862e+00
  1.90595269e+00]
 [1.64250000e+03 1.94622173e+00 2.60945692e-04 1.94587159e+00
  1.94658184e+00]
 [1.72100000e+03 1.99096589e+00 4.70473537e-04 1.99026048e+00
  1.99174273e+00]
 [1.80250000e+03 2.03973250e+00 5.36297554e-04 2.03915167e+00
  2.04047656e+00]
 [1.88700000e+03 2.09263778e+00 6.11562496e-04 2.09172750e+00
  2.09361911e+00]
 [1.97450000e+03 2.14998941e+00 4.99103779e-04 2.14941859e+00
  2.15064001e+00]
 [2.06500000e+03 2.21266785e+00 1.00883184e-03 2.21103215e+00
  2.21416664e+00]
 [2.15850000e+03 2.28126097e+00 1.41127203e-03 2.27911615e+00
  2.28284168e+00]
 [2.25550000e+03 2.35612960e+00 1.39692348e-03 2.35388780e+00
  2.35794926e+00]
 [2.35550000e+03 2.43725462e+00 1.18546809e-03 2.43546057e+00
  2.43896055e+00]
 [2.45900000e+03 2.52607732e+00 1.53715479e-03 2.52417159e+00
  2.52857494e+00]
 [2.56600000e+03 2.62387462e+00 1.78569431e-03 2.62181330e+00
  2.62697792e+00]
 [2.67650000e+03 2.73122854e+00 1.41878712e-03 2.72917724e+00
  2.73332834e+00]
 [2.79100000e+03 2.84901910e+00 3.83247775e-03 2.84231806e+00
  2.85362411e+00]
 [2.90900000e+03 2.97786570e+00 6.78940938e-03 2.96647477e+00
  2.98560309e+00]
 [3.03100000e+03 3.11997056e+00 8.25667153e-03 3.10723591e+00
  3.12974977e+00]
 [3.15700000e+03 3.27605915e+00 7.87756406e-03 3.26311326e+00
  3.28501415e+00]
 [3.28700000e+03 3.44689374e+00 6.39730037e-03 3.43419003e+00
  3.45129299e+00]
 [3.42150000e+03 3.63388562e+00 5.15417873e-03 3.62652349e+00
  3.63961697e+00]
 [3.56100000e+03 3.83908749e+00 3.69678908e-03 3.83399034e+00
  3.84523702e+00]
 [3.70500000e+03 4.06941338e+00 1.23532664e-02 4.05476904e+00
  4.09024668e+00]
 [3.85300000e+03 4.31751528e+00 2.64311921e-02 4.28880358e+00
  4.36063004e+00]
 [4.00600000e+03 4.58253117e+00 3.91949448e-02 4.53070211e+00
  4.64364052e+00]
 [4.16450000e+03 4.86186323e+00 5.23337736e-02 4.78401232e+00
  4.93863583e+00]
 [4.32800000e+03 5.15379486e+00 6.45346953e-02 5.05371809e+00
  5.24414062e+00]
 [4.49700000e+03 5.46068563e+00 7.27897520e-02 5.34857130e+00
  5.56106758e+00]
 [4.67150000e+03 5.78540964e+00 7.18437649e-02 5.68615198e+00
  5.88911533e+00]
 [4.85150000e+03 6.13398733e+00 6.01934389e-02 6.06164217e+00
  6.22821951e+00]
 [5.03750000e+03 6.49899740e+00 6.78370300e-02 6.39804316e+00
  6.57893801e+00]
 [5.22950000e+03 6.87608490e+00 9.68981244e-02 6.74536610e+00
  7.01124477e+00]
 [5.45050000e+03 7.31029701e+00 1.40453795e-01 7.14507437e+00
  7.54603577e+00]
 [5.65550000e+03 7.71309338e+00 1.84341307e-01 7.51571226e+00
  8.04251575e+00]
 [5.84400000e+03 8.08339720e+00 2.25997425e-01 7.85637379e+00
  8.49921131e+00]
 [6.06200000e+03 8.51147289e+00 2.74988509e-01 8.25014782e+00
  9.02742100e+00]
 [6.28750000e+03 8.95395603e+00 3.26161623e-01 8.65722752e+00
  9.57361412e+00]
 [6.52000000e+03 9.40971851e+00 3.79227408e-01 9.07667351e+00
  1.01364260e+01]
 [6.76000000e+03 9.87953358e+00 4.34214756e-01 9.50932884e+00
  1.07169189e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.7
Model: "functional_91"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_46 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_270 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_271 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_272 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_273 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_274 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_275 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0502 - val_loss: 0.0482
Epoch 2/100
7493/7493 - 32s - loss: 0.0484 - val_loss: 0.0483
Epoch 3/100
7493/7493 - 37s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 43s - loss: 0.0480 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0481
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0481
Epoch 8/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 49s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 38s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 48s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 44s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 44s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 33s - loss: 0.0477 - val_loss: 0.0478
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.7
Model: "functional_93"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_47 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_276 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_277 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_278 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_279 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_280 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_281 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0508 - val_loss: 0.0492
Epoch 2/100
7493/7493 - 34s - loss: 0.0484 - val_loss: 0.0493
Epoch 3/100
7493/7493 - 34s - loss: 0.0483 - val_loss: 0.0484
Epoch 4/100
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0482
Epoch 5/100
7493/7493 - 46s - loss: 0.0481 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 47s - loss: 0.0480 - val_loss: 0.0481
Epoch 7/100
7493/7493 - 32s - loss: 0.0480 - val_loss: 0.0482
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0480 - val_loss: 0.0483
Epoch 9/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0479
Epoch 11/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0479 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.7
Model: "functional_95"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_48 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_282 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_283 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_284 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_285 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_286 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_287 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0502 - val_loss: 0.0480
Epoch 2/100
7493/7493 - 44s - loss: 0.0484 - val_loss: 0.0482
Epoch 3/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0479
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0479
Epoch 8/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 34/100
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 35/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.7
Model: "functional_97"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_49 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_288 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_289 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_290 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_291 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_292 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_293 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0502 - val_loss: 0.0492
Epoch 2/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0485
Epoch 3/100
7493/7493 - 33s - loss: 0.0482 - val_loss: 0.0481
Epoch 4/100
7493/7493 - 37s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 37s - loss: 0.0481 - val_loss: 0.0483
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0480 - val_loss: 0.0480
Epoch 7/100
7493/7493 - 36s - loss: 0.0479 - val_loss: 0.0479
Epoch 8/100
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 46s - loss: 0.0479 - val_loss: 0.0478
Epoch 10/100
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 42s - loss: 0.0479 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 37s - loss: 0.0479 - val_loss: 0.0478
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 46s - loss: 0.0479 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda3001ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.7
Model: "functional_99"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_50 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_294 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_295 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_296 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_297 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_298 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_299 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0505 - val_loss: 0.0489
Epoch 2/100
7493/7493 - 45s - loss: 0.0484 - val_loss: 0.0480
Epoch 3/100
7493/7493 - 35s - loss: 0.0482 - val_loss: 0.0482
Epoch 4/100
7493/7493 - 41s - loss: 0.0481 - val_loss: 0.0481
Epoch 5/100
7493/7493 - 37s - loss: 0.0481 - val_loss: 0.0478
Epoch 6/100
7493/7493 - 35s - loss: 0.0480 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 34s - loss: 0.0480 - val_loss: 0.0478
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0480 - val_loss: 0.0477
Epoch 9/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 10/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 12/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 48s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100
7493/7493 - 48s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 25/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 27/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 28/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0477
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 30/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 31/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0477
Epoch 33/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0477
Epoch 34/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2fe1c1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79579484e+00 1.82227268e-04 1.79554820e+00
  1.79609680e+00]
 [1.28750000e+03 1.82498047e+00 3.30956201e-04 1.82457900e+00
  1.82555604e+00]
 [1.35350000e+03 1.85843410e+00 4.33471352e-04 1.85769856e+00
  1.85888994e+00]
 [1.42200000e+03 1.89472561e+00 2.58856318e-04 1.89435267e+00
  1.89510393e+00]
 [1.49300000e+03 1.93357921e+00 6.83903464e-04 1.93237972e+00
  1.93440664e+00]
 [1.56650000e+03 1.97567782e+00 8.64434195e-04 1.97453022e+00
  1.97671366e+00]
 [1.64250000e+03 2.02139711e+00 4.10942429e-04 2.02098179e+00
  2.02206564e+00]
 [1.72100000e+03 2.07083726e+00 2.67584937e-04 2.07036543e+00
  2.07113552e+00]
 [1.80250000e+03 2.12543855e+00 2.33287267e-04 2.12513852e+00
  2.12572742e+00]
 [1.88700000e+03 2.18602395e+00 2.97435830e-04 2.18571711e+00
  2.18658996e+00]
 [1.97450000e+03 2.25151916e+00 7.60128945e-04 2.25051737e+00
  2.25286508e+00]
 [2.06500000e+03 2.32275662e+00 1.04390007e-03 2.32077599e+00
  2.32382345e+00]
 [2.15850000e+03 2.40084195e+00 1.49940527e-03 2.39815784e+00
  2.40207529e+00]
 [2.25550000e+03 2.48658342e+00 2.20985905e-03 2.48357344e+00
  2.48868823e+00]
 [2.35550000e+03 2.58025484e+00 2.02530429e-03 2.57740092e+00
  2.58256149e+00]
 [2.45900000e+03 2.68259468e+00 1.09605639e-03 2.68101573e+00
  2.68423271e+00]
 [2.56600000e+03 2.79447346e+00 2.56882460e-03 2.79194164e+00
  2.79908013e+00]
 [2.67650000e+03 2.91803422e+00 2.78991732e-03 2.91555119e+00
  2.92151332e+00]
 [2.79100000e+03 3.05433273e+00 3.06254897e-03 3.05011344e+00
  3.05938339e+00]
 [2.90900000e+03 3.20345078e+00 5.60439819e-03 3.19251609e+00
  3.20740151e+00]
 [3.03100000e+03 3.36686420e+00 9.97174504e-03 3.34776449e+00
  3.37546897e+00]
 [3.15700000e+03 3.54645982e+00 8.86753338e-03 3.53289199e+00
  3.55498981e+00]
 [3.28700000e+03 3.74178157e+00 7.63417872e-03 3.72919154e+00
  3.75049305e+00]
 [3.42150000e+03 3.95455594e+00 7.39109067e-03 3.94221091e+00
  3.96173930e+00]
 [3.56100000e+03 4.19218531e+00 1.22189750e-02 4.17836285e+00
  4.20966768e+00]
 [3.70500000e+03 4.45968628e+00 1.65094654e-02 4.43917656e+00
  4.48486519e+00]
 [3.85300000e+03 4.75311747e+00 1.96304822e-02 4.73284101e+00
  4.77885008e+00]
 [4.00600000e+03 5.06489906e+00 2.20347875e-02 5.03896332e+00
  5.09879637e+00]
 [4.16450000e+03 5.39333906e+00 2.64849170e-02 5.36344862e+00
  5.43190098e+00]
 [4.32800000e+03 5.73623886e+00 3.40421091e-02 5.68961143e+00
  5.77661085e+00]
 [4.49700000e+03 6.09334879e+00 4.39464457e-02 6.02737474e+00
  6.13752556e+00]
 [4.67150000e+03 6.46311712e+00 5.51709132e-02 6.37653542e+00
  6.52619839e+00]
 [4.85150000e+03 6.84521103e+00 6.73429683e-02 6.73696852e+00
  6.92776108e+00]
 [5.03750000e+03 7.24046640e+00 8.02761672e-02 7.10959911e+00
  7.34300280e+00]
 [5.22950000e+03 7.64867401e+00 9.37839981e-02 7.49436569e+00
  7.77149963e+00]
 [5.45050000e+03 8.11860371e+00 1.09405785e-01 7.93731785e+00
  8.26436138e+00]
 [5.65550000e+03 8.55444279e+00 1.23901547e-01 8.34821796e+00
  8.72111416e+00]
 [5.84400000e+03 8.95505428e+00 1.37202905e-01 8.72603226e+00
  9.14070034e+00]
 [6.06200000e+03 9.41804905e+00 1.52486117e-01 9.16293335e+00
  9.62526989e+00]
 [6.28750000e+03 9.89637585e+00 1.68013511e-01 9.61479855e+00
  1.01251373e+01]
 [6.52000000e+03 1.03887325e+01 1.83686431e-01 1.00806055e+01
  1.06388807e+01]
 [6.76000000e+03 1.08958740e+01 1.99510966e-01 1.05613289e+01
  1.11673088e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.9
Model: "functional_101"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_51 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_300 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_301 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_302 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_303 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_304 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_305 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0275 - val_loss: 0.0272
Epoch 2/100
7493/7493 - 35s - loss: 0.0257 - val_loss: 0.0261
Epoch 3/100
7493/7493 - 33s - loss: 0.0255 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 48s - loss: 0.0254 - val_loss: 0.0261
Epoch 5/100
7493/7493 - 44s - loss: 0.0254 - val_loss: 0.0255
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0254
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 48s - loss: 0.0253 - val_loss: 0.0254
Epoch 11/100
7493/7493 - 40s - loss: 0.0252 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 46s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 38s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 39s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 47s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 31s - loss: 0.0252 - val_loss: 0.0253
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 47s - loss: 0.0252 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 40s - loss: 0.0252 - val_loss: 0.0253
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.9
Model: "functional_103"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_52 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_306 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_307 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_308 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_309 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_310 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_311 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0274 - val_loss: 0.0256
Epoch 2/100
7493/7493 - 36s - loss: 0.0257 - val_loss: 0.0289
Epoch 3/100
7493/7493 - 33s - loss: 0.0256 - val_loss: 0.0256
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0255 - val_loss: 0.0256
Epoch 5/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 30s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 27s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.9
Model: "functional_105"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_53 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_312 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_313 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_314 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_315 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_316 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_317 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 44s - loss: 0.0276 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 36s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 28s - loss: 0.0256 - val_loss: 0.0256
Epoch 4/100
7493/7493 - 31s - loss: 0.0255 - val_loss: 0.0255
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 29s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 22s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 28s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 25s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 43s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.9
Model: "functional_107"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_54 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_318 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_319 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_320 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_321 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_322 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_323 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0277 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 37s - loss: 0.0257 - val_loss: 0.0256
Epoch 3/100
7493/7493 - 36s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100
7493/7493 - 32s - loss: 0.0255 - val_loss: 0.0253
Epoch 5/100
7493/7493 - 36s - loss: 0.0255 - val_loss: 0.0259
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0255
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 47s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 35/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cd47b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.9
Model: "functional_109"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_55 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_324 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_325 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_326 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_327 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_328 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_329 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0273 - val_loss: 0.0253
Epoch 2/100
7493/7493 - 34s - loss: 0.0258 - val_loss: 0.0260
Epoch 3/100
7493/7493 - 48s - loss: 0.0256 - val_loss: 0.0254
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0255
Epoch 5/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 6/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0252
Epoch 9/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0252
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0252
Epoch 11/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 12/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0252
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 14/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0252
Epoch 15/100
7493/7493 - 48s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 17/100
7493/7493 - 31s - loss: 0.0253 - val_loss: 0.0252
Epoch 18/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0252
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 20/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 21/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0252
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0252
Epoch 23/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0252
Epoch 24/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 44s - loss: 0.0253 - val_loss: 0.0252
Epoch 26/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0252
Epoch 27/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 29/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 30/100
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0252
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0252
Epoch 32/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0252
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda305037b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.9: 
[[1.22750000e+03 1.88121130e+00 1.81740173e-04 1.88090551e+00
  1.88144159e+00]
 [1.28750000e+03 1.91376016e+00 5.24478149e-04 1.91323054e+00
  1.91460860e+00]
 [1.35350000e+03 1.95128982e+00 4.63371551e-04 1.95069730e+00
  1.95207500e+00]
 [1.42200000e+03 1.99158409e+00 4.12700215e-04 1.99099636e+00
  1.99215710e+00]
 [1.49300000e+03 2.03736010e+00 8.00188569e-04 2.03595376e+00
  2.03834438e+00]
 [1.56650000e+03 2.08709521e+00 7.74502105e-04 2.08627844e+00
  2.08856392e+00]
 [1.64250000e+03 2.14031420e+00 9.43530381e-04 2.13890338e+00
  2.14126611e+00]
 [1.72100000e+03 2.20017710e+00 3.92193435e-04 2.19941425e+00
  2.20050693e+00]
 [1.80250000e+03 2.26635318e+00 7.12401918e-04 2.26530695e+00
  2.26751614e+00]
 [1.88700000e+03 2.33869190e+00 2.56428172e-04 2.33827734e+00
  2.33897805e+00]
 [1.97450000e+03 2.41797099e+00 5.95833008e-04 2.41696596e+00
  2.41862440e+00]
 [2.06500000e+03 2.50489659e+00 1.04456128e-03 2.50332665e+00
  2.50609756e+00]
 [2.15850000e+03 2.59930677e+00 1.46830628e-03 2.59663653e+00
  2.60077333e+00]
 [2.25550000e+03 2.70330157e+00 2.65604196e-03 2.69799352e+00
  2.70476222e+00]
 [2.35550000e+03 2.81725402e+00 2.68364965e-03 2.81226516e+00
  2.81962132e+00]
 [2.45900000e+03 2.94265299e+00 2.05334262e-03 2.93998528e+00
  2.94540310e+00]
 [2.56600000e+03 3.07969069e+00 4.71483789e-03 3.07459021e+00
  3.08850956e+00]
 [2.67650000e+03 3.23212790e+00 6.71266864e-03 3.22272944e+00
  3.24064755e+00]
 [2.79100000e+03 3.40041981e+00 9.46588168e-03 3.38732386e+00
  3.41537929e+00]
 [2.90900000e+03 3.58425245e+00 1.16790217e-02 3.56882620e+00
  3.60506177e+00]
 [3.03100000e+03 3.78761029e+00 1.12979580e-02 3.77332616e+00
  3.80704999e+00]
 [3.15700000e+03 4.01103930e+00 1.02439482e-02 3.99246764e+00
  4.02182817e+00]
 [3.28700000e+03 4.25528316e+00 1.67982143e-02 4.22449350e+00
  4.27095795e+00]
 [3.42150000e+03 4.52493820e+00 2.68764411e-02 4.48868179e+00
  4.55191326e+00]
 [3.56100000e+03 4.82503033e+00 4.17862059e-02 4.76215601e+00
  4.88142824e+00]
 [3.70500000e+03 5.14775896e+00 5.82570033e-02 5.05808735e+00
  5.23247814e+00]
 [3.85300000e+03 5.49723215e+00 6.39899664e-02 5.41317797e+00
  5.59917259e+00]
 [4.00600000e+03 5.87995100e+00 5.91677948e-02 5.82630396e+00
  5.98470354e+00]
 [4.16450000e+03 6.28288927e+00 6.85253194e-02 6.20686674e+00
  6.39215231e+00]
 [4.32800000e+03 6.70218916e+00 8.80902802e-02 6.59569979e+00
  6.81598330e+00]
 [4.49700000e+03 7.13923616e+00 1.11939180e-01 7.00021076e+00
  7.28540945e+00]
 [4.67150000e+03 7.59487209e+00 1.37459872e-01 7.42066860e+00
  7.79718924e+00]
 [4.85150000e+03 8.06758261e+00 1.65181703e-01 7.85634232e+00
  8.32506084e+00]
 [5.03750000e+03 8.55624428e+00 1.95418414e-01 8.30768204e+00
  8.86966324e+00]
 [5.22950000e+03 9.06047421e+00 2.27096817e-01 8.77445316e+00
  9.42971897e+00]
 [5.45050000e+03 9.63994656e+00 2.62740477e-01 9.31249905e+00
  1.00692196e+01]
 [5.65550000e+03 1.01756313e+01 2.93372085e-01 9.81209183e+00
  1.06533680e+01]
 [5.84400000e+03 1.06653675e+01 3.17427659e-01 1.02717552e+01
  1.11768227e+01]
 [6.06200000e+03 1.12257643e+01 3.36444974e-01 1.08035660e+01
  1.17532883e+01]
 [6.28750000e+03 1.17933889e+01 3.39124828e-01 1.13537989e+01
  1.22907677e+01]
 [6.52000000e+03 1.23686378e+01 3.31393806e-01 1.19211426e+01
  1.27967911e+01]
 [6.76000000e+03 1.29620100e+01 3.28782502e-01 1.25067225e+01
  1.33193264e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918028 events, validating on 1918029

training QR for quantile 0.99
Model: "functional_111"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_56 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_330 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_331 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_332 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_333 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_334 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_335 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0058 - val_loss: 0.0045
Epoch 2/100
7493/7493 - 47s - loss: 0.0044 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 37s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 00022: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_40_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918028

training QR for quantile 0.99
Model: "functional_113"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_57 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_336 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_337 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_338 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_339 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_340 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_341 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0060 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 37s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100
7493/7493 - 36s - loss: 0.0044 - val_loss: 0.0046
Epoch 6/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 37s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 48s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 00021: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_40_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918028 events, validating on 1918029

training QR for quantile 0.99
Model: "functional_115"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_58 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_342 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_343 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_344 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_345 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_346 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_347 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0066 - val_loss: 0.0045
Epoch 2/100
7493/7493 - 33s - loss: 0.0045 - val_loss: 0.0044
Epoch 3/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0044
Epoch 4/100
7493/7493 - 34s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 35s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0044 - val_loss: 0.0044
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 42s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 40s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 44s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 46s - loss: 0.0043 - val_loss: 0.0043
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0043 - val_loss: 0.0043
Epoch 30/100
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 31/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 36s - loss: 0.0043 - val_loss: 0.0043
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_40_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
training on 1918029 events, validating on 1918029

training QR for quantile 0.99
Model: "functional_117"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_59 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_348 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_349 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_350 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_351 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_352 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_353 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 34s - loss: 0.0054 - val_loss: 0.0048
Epoch 2/100
7493/7493 - 38s - loss: 0.0044 - val_loss: 0.0043
Epoch 3/100
7493/7493 - 44s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 49s - loss: 0.0044 - val_loss: 0.0043
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0044 - val_loss: 0.0044
Epoch 6/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 7/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 32s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 34s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 30s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 39s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 43s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 45s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 35s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 47s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 13s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 12s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 15s - loss: 0.0043 - val_loss: 0.0043
Epoch 27/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 28/100
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 00028: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_40_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda3027bf28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918029 events, validating on 1918028

training QR for quantile 0.99
Model: "functional_119"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_60 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_354 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_355 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_356 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_357 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_358 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_359 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 17s - loss: 0.0063 - val_loss: 0.0046
Epoch 2/100
7493/7493 - 15s - loss: 0.0045 - val_loss: 0.0047
Epoch 3/100
7493/7493 - 15s - loss: 0.0044 - val_loss: 0.0045
Epoch 4/100
7493/7493 - 16s - loss: 0.0044 - val_loss: 0.0044
Epoch 5/100
7493/7493 - 15s - loss: 0.0044 - val_loss: 0.0043
Epoch 6/100
7493/7493 - 14s - loss: 0.0044 - val_loss: 0.0044
Epoch 7/100
7493/7493 - 17s - loss: 0.0044 - val_loss: 0.0043
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
7493/7493 - 15s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
7493/7493 - 12s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 13s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
7493/7493 - 20s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100
7493/7493 - 15s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
7493/7493 - 16s - loss: 0.0043 - val_loss: 0.0043
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 18s - loss: 0.0043 - val_loss: 0.0043
Epoch 24/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 25/100
7493/7493 - 17s - loss: 0.0043 - val_loss: 0.0043
Epoch 00025: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35na_sigx_40_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2534a1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.99: 
[[1.22750000e+03 2.01609387e+00 1.14197963e-03 2.01478815e+00
  2.01796770e+00]
 [1.28750000e+03 2.05643959e+00 7.41868660e-04 2.05513787e+00
  2.05730343e+00]
 [1.35350000e+03 2.10454059e+00 1.18175211e-03 2.10291290e+00
  2.10653710e+00]
 [1.42200000e+03 2.15405898e+00 1.10074864e-03 2.15271568e+00
  2.15552211e+00]
 [1.49300000e+03 2.21238232e+00 1.63574156e-03 2.21100640e+00
  2.21556973e+00]
 [1.56650000e+03 2.27548261e+00 1.89159371e-03 2.27201366e+00
  2.27779818e+00]
 [1.64250000e+03 2.34711957e+00 1.55148231e-03 2.34532881e+00
  2.34959888e+00]
 [1.72100000e+03 2.42391868e+00 1.81251599e-03 2.42111826e+00
  2.42647910e+00]
 [1.80250000e+03 2.50895143e+00 3.08940030e-03 2.50574207e+00
  2.51361799e+00]
 [1.88700000e+03 2.60435519e+00 1.20881450e-03 2.60267568e+00
  2.60626125e+00]
 [1.97450000e+03 2.70852365e+00 2.83370928e-03 2.70603919e+00
  2.71399784e+00]
 [2.06500000e+03 2.82222419e+00 4.39635253e-03 2.81766629e+00
  2.82978249e+00]
 [2.15850000e+03 2.94916205e+00 5.39905286e-03 2.94294024e+00
  2.95643139e+00]
 [2.25550000e+03 3.08912063e+00 5.42547819e-03 3.08235669e+00
  3.09771514e+00]
 [2.35550000e+03 3.24213367e+00 3.46729957e-03 3.23717332e+00
  3.24717331e+00]
 [2.45900000e+03 3.41007915e+00 1.77422891e-03 3.40747952e+00
  3.41220379e+00]
 [2.56600000e+03 3.59429417e+00 8.33687125e-03 3.58146548e+00
  3.60660267e+00]
 [2.67650000e+03 3.79503484e+00 1.57242331e-02 3.77308631e+00
  3.81969476e+00]
 [2.79100000e+03 4.01196752e+00 2.03052652e-02 3.98584962e+00
  4.04404974e+00]
 [2.90900000e+03 4.24773617e+00 2.34253935e-02 4.21878958e+00
  4.28271818e+00]
 [3.03100000e+03 4.50864334e+00 2.49686037e-02 4.47686720e+00
  4.53860235e+00]
 [3.15700000e+03 4.79775171e+00 2.72550606e-02 4.75415134e+00
  4.83522701e+00]
 [3.28700000e+03 5.11606092e+00 3.80268880e-02 5.07092428e+00
  5.18580055e+00]
 [3.42150000e+03 5.46777630e+00 5.14333952e-02 5.40552473e+00
  5.56243658e+00]
 [3.56100000e+03 5.84987926e+00 7.50298563e-02 5.73133278e+00
  5.96216345e+00]
 [3.70500000e+03 6.25969467e+00 1.05447044e-01 6.08055305e+00
  6.39550734e+00]
 [3.85300000e+03 6.70222244e+00 1.35494377e-01 6.46567583e+00
  6.86701775e+00]
 [4.00600000e+03 7.18599758e+00 1.47922790e-01 6.92894745e+00
  7.35595703e+00]
 [4.16450000e+03 7.70324278e+00 1.46273539e-01 7.47157049e+00
  7.86293888e+00]
 [4.32800000e+03 8.23518791e+00 1.48070681e-01 8.03652859e+00
  8.38542175e+00]
 [4.49700000e+03 8.77891655e+00 1.51683389e-01 8.57188892e+00
  8.92836380e+00]
 [4.67150000e+03 9.32618523e+00 1.58257995e-01 9.08313847e+00
  9.51231861e+00]
 [4.85150000e+03 9.87971592e+00 1.81668546e-01 9.61044502e+00
  1.01149473e+01]
 [5.03750000e+03 1.04472263e+01 2.24712880e-01 1.01548615e+01
  1.07378292e+01]
 [5.22950000e+03 1.10279295e+01 2.86683344e-01 1.06601534e+01
  1.13809900e+01]
 [5.45050000e+03 1.16950790e+01 3.68146297e-01 1.11636400e+01
  1.21214714e+01]
 [5.65550000e+03 1.23126081e+01 4.48656605e-01 1.16298027e+01
  1.28084717e+01]
 [5.84400000e+03 1.28786007e+01 5.25967522e-01 1.20559759e+01
  1.34402599e+01]
 [6.06200000e+03 1.35302250e+01 6.18580191e-01 1.25465775e+01
  1.41710091e+01]
 [6.28750000e+03 1.42007996e+01 7.16251828e-01 1.30552912e+01
  1.49269781e+01]
 [6.52000000e+03 1.48906906e+01 8.15817308e-01 1.35812969e+01
  1.57064943e+01]
 [6.76000000e+03 1.55992949e+01 9.14676354e-01 1.41260710e+01
  1.65112324e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918051 events, validating on 1918052

training QR for quantile 0.1
Model: "functional_121"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_61 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_360 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_361 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_362 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_363 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_364 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_365 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 17s - loss: 0.0248 - val_loss: 0.0239
Epoch 2/100
7493/7493 - 16s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 15s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 18s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 17s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 12s - loss: 0.0238 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 15s - loss: 0.0238 - val_loss: 0.0238
Epoch 8/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 19s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 20s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 30/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 33/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0236
Epoch 34/100

Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 35/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0236
Epoch 36/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0236
Epoch 00036: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_50_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918051

training QR for quantile 0.1
Model: "functional_123"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_62 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_366 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_367 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_368 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_369 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_370 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_371 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 15s - loss: 0.0249 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 16s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 16s - loss: 0.0239 - val_loss: 0.0239
Epoch 4/100
7493/7493 - 16s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 13s - loss: 0.0238 - val_loss: 0.0240
Epoch 6/100
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0238
Epoch 7/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 17s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100
7493/7493 - 14s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 15s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 14s - loss: 0.0236 - val_loss: 0.0237
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 18/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 18s - loss: 0.0236 - val_loss: 0.0237
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 21/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 24/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 12s - loss: 0.0236 - val_loss: 0.0237
Epoch 27/100
7493/7493 - 14s - loss: 0.0236 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0237
Epoch 30/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 15s - loss: 0.0236 - val_loss: 0.0237
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 17s - loss: 0.0236 - val_loss: 0.0237
Epoch 33/100
7493/7493 - 16s - loss: 0.0236 - val_loss: 0.0237
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_50_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918051 events, validating on 1918052

training QR for quantile 0.1
Model: "functional_125"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_63 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_372 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_373 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_374 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_375 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_376 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_377 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 18s - loss: 0.0251 - val_loss: 0.0243
Epoch 2/100
7493/7493 - 15s - loss: 0.0241 - val_loss: 0.0245
Epoch 3/100
7493/7493 - 18s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 17s - loss: 0.0239 - val_loss: 0.0237
Epoch 5/100
7493/7493 - 16s - loss: 0.0239 - val_loss: 0.0238
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 17s - loss: 0.0238 - val_loss: 0.0240
Epoch 7/100
7493/7493 - 16s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 20s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 19s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 19s - loss: 0.0237 - val_loss: 0.0237
Epoch 11/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 14/100
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 12s - loss: 0.0237 - val_loss: 0.0237
Epoch 16/100
7493/7493 - 11s - loss: 0.0237 - val_loss: 0.0237
Epoch 17/100
7493/7493 - 21s - loss: 0.0237 - val_loss: 0.0237
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 19/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 20/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 22/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 23/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 41s - loss: 0.0237 - val_loss: 0.0237
Epoch 25/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 26/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0237
Epoch 28/100
7493/7493 - 47s - loss: 0.0237 - val_loss: 0.0237
Epoch 29/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0237
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 31/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0237
Epoch 32/100
7493/7493 - 43s - loss: 0.0237 - val_loss: 0.0237
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0237
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_50_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918052

training QR for quantile 0.1
Model: "functional_127"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_64 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_378 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_379 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_380 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_381 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_382 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_383 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0250 - val_loss: 0.0253
Epoch 2/100
7493/7493 - 33s - loss: 0.0240 - val_loss: 0.0237
Epoch 3/100
7493/7493 - 35s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 34s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0238 - val_loss: 0.0237
Epoch 6/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 7/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 28s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100
7493/7493 - 34s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 36s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 30s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 45s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 44s - loss: 0.0237 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0237 - val_loss: 0.0236
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_50_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cf347b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918052 events, validating on 1918051

training QR for quantile 0.1
Model: "functional_129"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_65 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_384 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_385 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_386 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_387 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_388 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_389 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0250 - val_loss: 0.0240
Epoch 2/100
7493/7493 - 36s - loss: 0.0240 - val_loss: 0.0238
Epoch 3/100
7493/7493 - 44s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100
7493/7493 - 36s - loss: 0.0238 - val_loss: 0.0237
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0238 - val_loss: 0.0243
Epoch 6/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100
7493/7493 - 37s - loss: 0.0237 - val_loss: 0.0237
Epoch 8/100
7493/7493 - 32s - loss: 0.0237 - val_loss: 0.0237
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0237 - val_loss: 0.0237
Epoch 10/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 11/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0237
Epoch 13/100
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0236
Epoch 14/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 16/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0236
Epoch 17/100
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0236 - val_loss: 0.0236
Epoch 19/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 20/100
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 43s - loss: 0.0236 - val_loss: 0.0236
Epoch 22/100
7493/7493 - 32s - loss: 0.0236 - val_loss: 0.0236
Epoch 23/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0236
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 25/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0236
Epoch 26/100
7493/7493 - 36s - loss: 0.0236 - val_loss: 0.0236
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 28/100
7493/7493 - 40s - loss: 0.0236 - val_loss: 0.0236
Epoch 29/100
7493/7493 - 38s - loss: 0.0236 - val_loss: 0.0236
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0236 - val_loss: 0.0236
Epoch 31/100
7493/7493 - 46s - loss: 0.0236 - val_loss: 0.0236
Epoch 32/100
7493/7493 - 45s - loss: 0.0236 - val_loss: 0.0236
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0236 - val_loss: 0.0236
Epoch 34/100
7493/7493 - 34s - loss: 0.0236 - val_loss: 0.0236
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35na_sigx_50_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2f23ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.1: 
[[1.22750000e+03 1.58428581e+00 3.62815697e-04 1.58362162e+00
  1.58462620e+00]
 [1.28750000e+03 1.61240323e+00 2.96341870e-04 1.61191702e+00
  1.61281610e+00]
 [1.35350000e+03 1.64318540e+00 4.24782056e-04 1.64243889e+00
  1.64358091e+00]
 [1.42200000e+03 1.67512183e+00 4.71451804e-04 1.67452002e+00
  1.67591882e+00]
 [1.49300000e+03 1.70756643e+00 4.48713334e-04 1.70706189e+00
  1.70814610e+00]
 [1.56650000e+03 1.74173188e+00 5.74851668e-04 1.74119973e+00
  1.74276614e+00]
 [1.64250000e+03 1.77676601e+00 5.16850462e-04 1.77631724e+00
  1.77775359e+00]
 [1.72100000e+03 1.81298301e+00 5.98407919e-04 1.81222963e+00
  1.81389582e+00]
 [1.80250000e+03 1.85160167e+00 7.58638978e-04 1.85045242e+00
  1.85267746e+00]
 [1.88700000e+03 1.89268925e+00 6.23246420e-04 1.89171195e+00
  1.89335024e+00]
 [1.97450000e+03 1.93644540e+00 6.21943454e-04 1.93566287e+00
  1.93733037e+00]
 [2.06500000e+03 1.98345957e+00 1.08503647e-03 1.98168576e+00
  1.98488998e+00]
 [2.15850000e+03 2.03406601e+00 1.57742333e-03 2.03107190e+00
  2.03548932e+00]
 [2.25550000e+03 2.08833408e+00 1.86520574e-03 2.08493042e+00
  2.09035826e+00]
 [2.35550000e+03 2.14662547e+00 1.82407796e-03 2.14366221e+00
  2.14922357e+00]
 [2.45900000e+03 2.20976663e+00 1.32293849e-03 2.20804310e+00
  2.21210670e+00]
 [2.56600000e+03 2.27850475e+00 9.65149301e-04 2.27709913e+00
  2.27991414e+00]
 [2.67650000e+03 2.35369186e+00 2.01820046e-03 2.35095119e+00
  2.35708976e+00]
 [2.79100000e+03 2.43579192e+00 3.62271128e-03 2.42891359e+00
  2.43969321e+00]
 [2.90900000e+03 2.52542324e+00 5.36916630e-03 2.51508617e+00
  2.52991176e+00]
 [3.03100000e+03 2.62497807e+00 6.26552366e-03 2.61475658e+00
  2.63229513e+00]
 [3.15700000e+03 2.73702097e+00 4.63017187e-03 2.73032403e+00
  2.74326396e+00]
 [3.28700000e+03 2.86068697e+00 3.66403252e-03 2.85637927e+00
  2.86515975e+00]
 [3.42150000e+03 2.99374790e+00 7.11731617e-03 2.98132658e+00
  3.00356793e+00]
 [3.56100000e+03 3.13883801e+00 8.12334516e-03 3.12500572e+00
  3.15018654e+00]
 [3.70500000e+03 3.29899974e+00 3.87826170e-03 3.29462218e+00
  3.30478358e+00]
 [3.85300000e+03 3.46828995e+00 6.39237197e-03 3.45810914e+00
  3.47716784e+00]
 [4.00600000e+03 3.64778423e+00 1.02482097e-02 3.63445783e+00
  3.66446328e+00]
 [4.16450000e+03 3.83699074e+00 1.32723749e-02 3.82147646e+00
  3.85944986e+00]
 [4.32800000e+03 4.03471279e+00 1.52475834e-02 4.01550388e+00
  4.06136084e+00]
 [4.49700000e+03 4.24177504e+00 1.72489025e-02 4.21667814e+00
  4.27071190e+00]
 [4.67150000e+03 4.45646372e+00 2.13638776e-02 4.42481041e+00
  4.48745060e+00]
 [4.85150000e+03 4.67810116e+00 2.68939252e-02 4.63979626e+00
  4.71156502e+00]
 [5.03750000e+03 4.90714664e+00 3.31889844e-02 4.86215496e+00
  4.94745398e+00]
 [5.22950000e+03 5.14342403e+00 3.99299599e-02 5.09182453e+00
  5.19625139e+00]
 [5.45050000e+03 5.41506062e+00 4.76256527e-02 5.35628319e+00
  5.48143101e+00]
 [5.65550000e+03 5.66665144e+00 5.45385737e-02 5.60163975e+00
  5.74461651e+00]
 [5.84400000e+03 5.89762344e+00 6.06095822e-02 5.82725334e+00
  5.98529530e+00]
 [6.06200000e+03 6.16424074e+00 6.71886721e-02 6.08815765e+00
  6.26181602e+00]
 [6.28750000e+03 6.43937635e+00 7.33800304e-02 6.35799789e+00
  6.54545259e+00]
 [6.52000000e+03 6.72222586e+00 7.89703233e-02 6.63615084e+00
  6.83486271e+00]
 [6.76000000e+03 7.01311750e+00 8.37171446e-02 6.92319155e+00
  7.12966585e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918051 events, validating on 1918052

training QR for quantile 0.3
Model: "functional_131"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_66 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_390 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_391 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_392 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_393 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_394 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_395 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0491 - val_loss: 0.0469
Epoch 2/100
7493/7493 - 35s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 32s - loss: 0.0470 - val_loss: 0.0469
Epoch 4/100
7493/7493 - 33s - loss: 0.0469 - val_loss: 0.0468
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0468
Epoch 6/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0468
Epoch 7/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100
7493/7493 - 49s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_50_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918051

training QR for quantile 0.3
Model: "functional_133"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_67 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_396 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_397 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_398 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_399 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_400 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_401 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0477
Epoch 2/100
7493/7493 - 35s - loss: 0.0472 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 36s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 31s - loss: 0.0469 - val_loss: 0.0469
Epoch 5/100
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0467
Epoch 6/100
7493/7493 - 33s - loss: 0.0468 - val_loss: 0.0466
Epoch 7/100
7493/7493 - 37s - loss: 0.0468 - val_loss: 0.0467
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0468 - val_loss: 0.0469
Epoch 9/100
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0467
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0467
Epoch 12/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 45s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 33/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 34/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 35/100

Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 36/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 37/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 38/100

Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 39/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 40/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 41/100

Epoch 00041: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 42/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 43/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 00043: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_50_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918051 events, validating on 1918052

training QR for quantile 0.3
Model: "functional_135"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_68 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_402 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_403 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_404 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_405 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_406 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_407 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0486 - val_loss: 0.0471
Epoch 2/100
7493/7493 - 35s - loss: 0.0472 - val_loss: 0.0467
Epoch 3/100
7493/7493 - 39s - loss: 0.0470 - val_loss: 0.0468
Epoch 4/100
7493/7493 - 32s - loss: 0.0469 - val_loss: 0.0477
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0469 - val_loss: 0.0469
Epoch 6/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0467
Epoch 7/100
7493/7493 - 46s - loss: 0.0467 - val_loss: 0.0467
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0467
Epoch 9/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 30s - loss: 0.0466 - val_loss: 0.0467
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_50_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918052

training QR for quantile 0.3
Model: "functional_137"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_69 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_408 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_409 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_410 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_411 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_412 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_413 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0484 - val_loss: 0.0485
Epoch 2/100
7493/7493 - 33s - loss: 0.0472 - val_loss: 0.0469
Epoch 3/100
7493/7493 - 34s - loss: 0.0470 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 40s - loss: 0.0470 - val_loss: 0.0467
Epoch 5/100
7493/7493 - 37s - loss: 0.0469 - val_loss: 0.0468
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 46s - loss: 0.0469 - val_loss: 0.0468
Epoch 7/100
7493/7493 - 37s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 32s - loss: 0.0467 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 44s - loss: 0.0467 - val_loss: 0.0466
Epoch 10/100
7493/7493 - 35s - loss: 0.0467 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 36s - loss: 0.0467 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 18/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 19/100
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 21/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0466
Epoch 22/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0466 - val_loss: 0.0466
Epoch 24/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 25/100
7493/7493 - 26s - loss: 0.0466 - val_loss: 0.0466
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 27/100
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 28/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0466
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0466 - val_loss: 0.0466
Epoch 30/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 31/100
7493/7493 - 47s - loss: 0.0466 - val_loss: 0.0466
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_50_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda3c0f2400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918052 events, validating on 1918051

training QR for quantile 0.3
Model: "functional_139"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_70 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_414 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_415 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_416 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_417 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_418 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_419 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0488 - val_loss: 0.0473
Epoch 2/100
7493/7493 - 36s - loss: 0.0472 - val_loss: 0.0470
Epoch 3/100
7493/7493 - 32s - loss: 0.0469 - val_loss: 0.0467
Epoch 4/100
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0466
Epoch 5/100
7493/7493 - 35s - loss: 0.0468 - val_loss: 0.0470
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0468 - val_loss: 0.0466
Epoch 7/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 8/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0466
Epoch 9/100
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0467
Epoch 10/100
7493/7493 - 32s - loss: 0.0466 - val_loss: 0.0466
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0466 - val_loss: 0.0466
Epoch 12/100
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0466
Epoch 13/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0466
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0466
Epoch 15/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0466
Epoch 16/100
7493/7493 - 39s - loss: 0.0466 - val_loss: 0.0466
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100
7493/7493 - 34s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100
7493/7493 - 38s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100
7493/7493 - 46s - loss: 0.0466 - val_loss: 0.0465
Epoch 25/100
7493/7493 - 35s - loss: 0.0466 - val_loss: 0.0465
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 37s - loss: 0.0466 - val_loss: 0.0465
Epoch 27/100
7493/7493 - 42s - loss: 0.0466 - val_loss: 0.0465
Epoch 28/100
7493/7493 - 41s - loss: 0.0466 - val_loss: 0.0465
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 48s - loss: 0.0466 - val_loss: 0.0465
Epoch 30/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 31/100
7493/7493 - 36s - loss: 0.0466 - val_loss: 0.0465
Epoch 00031: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35na_sigx_50_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2cd7be18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.3: 
[[1.22750000e+03 1.67801270e+00 8.55108235e-05 1.67784894e+00
  1.67808437e+00]
 [1.28750000e+03 1.70590045e+00 2.71205377e-04 1.70544362e+00
  1.70628941e+00]
 [1.35350000e+03 1.73682828e+00 6.07918577e-04 1.73612654e+00
  1.73767197e+00]
 [1.42200000e+03 1.76886413e+00 4.55569329e-04 1.76813328e+00
  1.76940298e+00]
 [1.49300000e+03 1.80220194e+00 9.39372417e-04 1.80101252e+00
  1.80326200e+00]
 [1.56650000e+03 1.83848550e+00 3.95143983e-04 1.83801651e+00
  1.83906412e+00]
 [1.64250000e+03 1.87594993e+00 5.98563799e-04 1.87525702e+00
  1.87677193e+00]
 [1.72100000e+03 1.91618776e+00 4.38487943e-04 1.91570115e+00
  1.91684842e+00]
 [1.80250000e+03 1.96013360e+00 1.05518838e-04 1.96000910e+00
  1.96027970e+00]
 [1.88700000e+03 2.00707426e+00 4.32437165e-04 2.00640798e+00
  2.00748801e+00]
 [1.97450000e+03 2.05808444e+00 2.19209017e-04 2.05770326e+00
  2.05838823e+00]
 [2.06500000e+03 2.11355715e+00 6.89511533e-04 2.11277533e+00
  2.11458659e+00]
 [2.15850000e+03 2.17360568e+00 7.71775973e-04 2.17273521e+00
  2.17476463e+00]
 [2.25550000e+03 2.23906560e+00 1.18653660e-03 2.23746991e+00
  2.24098325e+00]
 [2.35550000e+03 2.31007295e+00 1.88439245e-03 2.30655241e+00
  2.31224465e+00]
 [2.45900000e+03 2.38769269e+00 1.79179100e-03 2.38459373e+00
  2.38943100e+00]
 [2.56600000e+03 2.47208982e+00 1.73591313e-03 2.47002912e+00
  2.47441673e+00]
 [2.67650000e+03 2.56432357e+00 2.20204643e-03 2.56192470e+00
  2.56736135e+00]
 [2.79100000e+03 2.66591024e+00 3.63911804e-03 2.66194415e+00
  2.67212343e+00]
 [2.90900000e+03 2.77697463e+00 5.40632010e-03 2.77059841e+00
  2.78566194e+00]
 [3.03100000e+03 2.89914184e+00 6.52966122e-03 2.88924026e+00
  2.90835786e+00]
 [3.15700000e+03 3.03325009e+00 7.41909864e-03 3.02043200e+00
  3.04086757e+00]
 [3.28700000e+03 3.18439078e+00 7.51984130e-03 3.17048812e+00
  3.19090271e+00]
 [3.42150000e+03 3.35094895e+00 5.59384689e-03 3.34280443e+00
  3.35809255e+00]
 [3.56100000e+03 3.53150396e+00 6.51389849e-03 3.51893425e+00
  3.53769445e+00]
 [3.70500000e+03 3.72557225e+00 1.58480545e-02 3.69973159e+00
  3.74544024e+00]
 [3.85300000e+03 3.93053379e+00 2.66375799e-02 3.89093065e+00
  3.96503162e+00]
 [4.00600000e+03 4.14939213e+00 3.34485719e-02 4.10041666e+00
  4.19317484e+00]
 [4.16450000e+03 4.38584919e+00 3.20303796e-02 4.34661579e+00
  4.43032742e+00]
 [4.32800000e+03 4.64329729e+00 2.80111545e-02 4.59220028e+00
  4.67554903e+00]
 [4.49700000e+03 4.91367731e+00 4.25911955e-02 4.83276081e+00
  4.95250225e+00]
 [4.67150000e+03 5.19365854e+00 6.64181359e-02 5.08202410e+00
  5.27365589e+00]
 [4.85150000e+03 5.48310213e+00 9.36448274e-02 5.33976698e+00
  5.60616779e+00]
 [5.03750000e+03 5.78271513e+00 1.22927515e-01 5.60653734e+00
  5.95100975e+00]
 [5.22950000e+03 6.09249659e+00 1.53924379e-01 5.88221407e+00
  6.30848408e+00]
 [5.45050000e+03 6.44977036e+00 1.90559595e-01 6.19975471e+00
  6.72261381e+00]
 [5.65550000e+03 6.78190832e+00 2.25565509e-01 6.49441481e+00
  7.10995960e+00]
 [5.84400000e+03 7.08740587e+00 2.58008830e-01 6.76538754e+00
  7.46676111e+00]
 [6.06200000e+03 7.44066105e+00 2.95670997e-01 7.07874966e+00
  7.87974310e+00]
 [6.28750000e+03 7.80587749e+00 3.34663048e-01 7.40282869e+00
  8.30711651e+00]
 [6.52000000e+03 8.18201761e+00 3.74699727e-01 7.73686790e+00
  8.74765205e+00]
 [6.76000000e+03 8.56953506e+00 4.15588645e-01 8.08154964e+00
  9.20208645e+00]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918051 events, validating on 1918052

training QR for quantile 0.5
Model: "functional_141"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_71 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_420 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_421 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_422 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_423 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_424 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_425 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 37s - loss: 0.0561 - val_loss: 0.0539
Epoch 2/100
7493/7493 - 27s - loss: 0.0544 - val_loss: 0.0555
Epoch 3/100
7493/7493 - 34s - loss: 0.0542 - val_loss: 0.0542
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 44s - loss: 0.0541 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0540
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0540
Epoch 8/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 28s - loss: 0.0538 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0539
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0538 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 22s - loss: 0.0538 - val_loss: 0.0539
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0539
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 45s - loss: 0.0538 - val_loss: 0.0539
Epoch 27/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0539
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 38s - loss: 0.0538 - val_loss: 0.0539
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_50_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918051

training QR for quantile 0.5
Model: "functional_143"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_72 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_426 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_427 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_428 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_429 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_430 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_431 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 45s - loss: 0.0567 - val_loss: 0.0540
Epoch 2/100
7493/7493 - 41s - loss: 0.0545 - val_loss: 0.0544
Epoch 3/100
7493/7493 - 46s - loss: 0.0543 - val_loss: 0.0550
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 31s - loss: 0.0540 - val_loss: 0.0539
Epoch 6/100
7493/7493 - 47s - loss: 0.0540 - val_loss: 0.0539
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 41s - loss: 0.0540 - val_loss: 0.0539
Epoch 8/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0540
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 46s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_50_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918051 events, validating on 1918052

training QR for quantile 0.5
Model: "functional_145"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_73 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_432 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_433 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_434 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_435 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_436 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_437 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0565 - val_loss: 0.0545
Epoch 2/100
7493/7493 - 36s - loss: 0.0545 - val_loss: 0.0544
Epoch 3/100
7493/7493 - 36s - loss: 0.0543 - val_loss: 0.0542
Epoch 4/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0542
Epoch 5/100
7493/7493 - 45s - loss: 0.0542 - val_loss: 0.0542
Epoch 6/100
7493/7493 - 37s - loss: 0.0542 - val_loss: 0.0540
Epoch 7/100
7493/7493 - 34s - loss: 0.0541 - val_loss: 0.0541
Epoch 8/100
7493/7493 - 36s - loss: 0.0541 - val_loss: 0.0540
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0541 - val_loss: 0.0540
Epoch 10/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 13/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 14/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 16/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 17/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0539
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 19/100
7493/7493 - 45s - loss: 0.0539 - val_loss: 0.0539
Epoch 20/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 22/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 23/100
7493/7493 - 36s - loss: 0.0539 - val_loss: 0.0539
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 25/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 26/100
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 47s - loss: 0.0539 - val_loss: 0.0539
Epoch 28/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0539
Epoch 29/100
7493/7493 - 38s - loss: 0.0539 - val_loss: 0.0539
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 31/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 32/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0539
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 34/100
7493/7493 - 37s - loss: 0.0539 - val_loss: 0.0539
Epoch 35/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 00035: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_50_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918052

training QR for quantile 0.5
Model: "functional_147"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_74 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_438 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_439 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_440 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_441 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_442 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_443 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0565 - val_loss: 0.0542
Epoch 2/100
7493/7493 - 43s - loss: 0.0546 - val_loss: 0.0539
Epoch 3/100
7493/7493 - 45s - loss: 0.0543 - val_loss: 0.0553
Epoch 4/100
7493/7493 - 35s - loss: 0.0542 - val_loss: 0.0543
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 42s - loss: 0.0542 - val_loss: 0.0540
Epoch 6/100
7493/7493 - 44s - loss: 0.0540 - val_loss: 0.0540
Epoch 7/100
7493/7493 - 36s - loss: 0.0540 - val_loss: 0.0539
Epoch 8/100

Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0539
Epoch 9/100
7493/7493 - 30s - loss: 0.0539 - val_loss: 0.0539
Epoch 10/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0539
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0539
Epoch 12/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 48s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 44s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 41s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 34s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 32s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 42s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 35s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 31s - loss: 0.0539 - val_loss: 0.0538
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_50_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda3027d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918052 events, validating on 1918051

training QR for quantile 0.5
Model: "functional_149"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_75 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_444 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_445 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_446 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_447 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_448 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_449 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0565 - val_loss: 0.0542
Epoch 2/100
7493/7493 - 37s - loss: 0.0546 - val_loss: 0.0553
Epoch 3/100
7493/7493 - 40s - loss: 0.0543 - val_loss: 0.0539
Epoch 4/100
7493/7493 - 46s - loss: 0.0542 - val_loss: 0.0540
Epoch 5/100
7493/7493 - 38s - loss: 0.0541 - val_loss: 0.0545
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 34s - loss: 0.0541 - val_loss: 0.0539
Epoch 7/100
7493/7493 - 39s - loss: 0.0539 - val_loss: 0.0539
Epoch 8/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 9/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 11/100

Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 13/100
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0538
Epoch 15/100
7493/7493 - 48s - loss: 0.0538 - val_loss: 0.0538
Epoch 16/100
7493/7493 - 32s - loss: 0.0538 - val_loss: 0.0538
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0538 - val_loss: 0.0538
Epoch 18/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0538
Epoch 19/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0538
Epoch 21/100
7493/7493 - 30s - loss: 0.0538 - val_loss: 0.0538
Epoch 22/100
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0538
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 24/100
7493/7493 - 42s - loss: 0.0538 - val_loss: 0.0538
Epoch 25/100
7493/7493 - 33s - loss: 0.0538 - val_loss: 0.0538
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 34s - loss: 0.0538 - val_loss: 0.0538
Epoch 27/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0538
Epoch 28/100
7493/7493 - 36s - loss: 0.0538 - val_loss: 0.0538
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 46s - loss: 0.0538 - val_loss: 0.0538
Epoch 30/100
7493/7493 - 40s - loss: 0.0538 - val_loss: 0.0538
Epoch 31/100
7493/7493 - 39s - loss: 0.0538 - val_loss: 0.0538
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0538
Epoch 33/100
7493/7493 - 43s - loss: 0.0538 - val_loss: 0.0538
Epoch 34/100
7493/7493 - 47s - loss: 0.0538 - val_loss: 0.0538
Epoch 00034: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35na_sigx_50_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda30362b70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.5: 
[[1.22750000e+03 1.73781440e+00 1.15907515e-04 1.73764873e+00
  1.73800719e+00]
 [1.28750000e+03 1.76589925e+00 1.20975067e-04 1.76574361e+00
  1.76611626e+00]
 [1.35350000e+03 1.79744771e+00 4.07700102e-04 1.79701710e+00
  1.79809368e+00]
 [1.42200000e+03 1.83159635e+00 2.27926383e-04 1.83132446e+00
  1.83197606e+00]
 [1.49300000e+03 1.86670556e+00 2.87780434e-04 1.86625302e+00
  1.86708891e+00]
 [1.56650000e+03 1.90569718e+00 1.71713036e-04 1.90537274e+00
  1.90587866e+00]
 [1.64250000e+03 1.94629638e+00 2.28587779e-04 1.94610476e+00
  1.94668090e+00]
 [1.72100000e+03 1.99086549e+00 5.86372483e-04 1.99017918e+00
  1.99195457e+00]
 [1.80250000e+03 2.03960528e+00 6.02485525e-04 2.03884459e+00
  2.04035020e+00]
 [1.88700000e+03 2.09255576e+00 6.42000418e-04 2.09175611e+00
  2.09368300e+00]
 [1.97450000e+03 2.15022435e+00 4.41586599e-04 2.14957118e+00
  2.15087247e+00]
 [2.06500000e+03 2.21303468e+00 9.02783419e-04 2.21184969e+00
  2.21435118e+00]
 [2.15850000e+03 2.28118534e+00 1.50961915e-03 2.27862239e+00
  2.28269410e+00]
 [2.25550000e+03 2.35575533e+00 1.69227954e-03 2.35259032e+00
  2.35713649e+00]
 [2.35550000e+03 2.43706808e+00 1.41924452e-03 2.43460250e+00
  2.43897462e+00]
 [2.45900000e+03 2.52617078e+00 1.25928198e-03 2.52426028e+00
  2.52763367e+00]
 [2.56600000e+03 2.62412939e+00 1.30294094e-03 2.62261534e+00
  2.62591457e+00]
 [2.67650000e+03 2.73105693e+00 2.32378909e-03 2.72697163e+00
  2.73323488e+00]
 [2.79100000e+03 2.84794374e+00 3.87635552e-03 2.84079409e+00
  2.85114384e+00]
 [2.90900000e+03 2.97646351e+00 5.72781397e-03 2.96609712e+00
  2.98162293e+00]
 [3.03100000e+03 3.12028894e+00 7.00913055e-03 3.10760260e+00
  3.12761879e+00]
 [3.15700000e+03 3.28024750e+00 7.89713718e-03 3.26649761e+00
  3.28839946e+00]
 [3.28700000e+03 3.45313244e+00 9.78431895e-03 3.43935895e+00
  3.46169996e+00]
 [3.42150000e+03 3.64080992e+00 1.08921021e-02 3.62520480e+00
  3.65170360e+00]
 [3.56100000e+03 3.84671402e+00 8.91707837e-03 3.83160210e+00
  3.85586762e+00]
 [3.70500000e+03 4.07158813e+00 1.54306085e-02 4.05618334e+00
  4.09982872e+00]
 [3.85300000e+03 4.31297045e+00 2.61426721e-02 4.28980255e+00
  4.36366940e+00]
 [4.00600000e+03 4.57441034e+00 3.75769411e-02 4.52402020e+00
  4.64017916e+00]
 [4.16450000e+03 4.85343437e+00 5.06790778e-02 4.77535391e+00
  4.93014336e+00]
 [4.32800000e+03 5.14797144e+00 6.06024210e-02 5.05355120e+00
  5.23200321e+00]
 [4.49700000e+03 5.46282797e+00 5.92184343e-02 5.38448000e+00
  5.54488325e+00]
 [4.67150000e+03 5.79577942e+00 5.45298336e-02 5.74051523e+00
  5.86828995e+00]
 [4.85150000e+03 6.13947983e+00 5.93741685e-02 6.06077576e+00
  6.20353937e+00]
 [5.03750000e+03 6.49445353e+00 7.24295069e-02 6.39206076e+00
  6.56111956e+00]
 [5.22950000e+03 6.86066923e+00 9.09439943e-02 6.73420477e+00
  6.96586895e+00]
 [5.45050000e+03 7.28191662e+00 1.15519263e-01 7.12807655e+00
  7.44324780e+00]
 [5.65550000e+03 7.67237778e+00 1.39976489e-01 7.49336624e+00
  7.88588715e+00]
 [5.84400000e+03 8.03115234e+00 1.63295940e-01 7.82912827e+00
  8.29271698e+00]
 [6.06200000e+03 8.44575253e+00 1.90912753e-01 8.21720982e+00
  8.76297569e+00]
 [6.28750000e+03 8.87421303e+00 2.20027356e-01 8.61818218e+00
  9.24913025e+00]
 [6.52000000e+03 9.31548328e+00 2.50549726e-01 9.03080750e+00
  9.75008011e+00]
 [6.76000000e+03 9.77044334e+00 2.82504595e-01 9.45585251e+00
  1.02668676e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918051 events, validating on 1918052

training QR for quantile 0.7
Model: "functional_151"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_76 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_450 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_451 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_452 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_453 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_454 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_455 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 46s - loss: 0.0506 - val_loss: 0.0485
Epoch 2/100
7493/7493 - 37s - loss: 0.0483 - val_loss: 0.0479
Epoch 3/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0480
Epoch 4/100
7493/7493 - 46s - loss: 0.0480 - val_loss: 0.0481
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0480 - val_loss: 0.0480
Epoch 6/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 7/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0479
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 48s - loss: 0.0477 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 32s - loss: 0.0477 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 44s - loss: 0.0477 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 41s - loss: 0.0477 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 46s - loss: 0.0477 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 40s - loss: 0.0477 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 34s - loss: 0.0477 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 37s - loss: 0.0477 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 38s - loss: 0.0477 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 36s - loss: 0.0477 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 47s - loss: 0.0477 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 35s - loss: 0.0477 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 45s - loss: 0.0477 - val_loss: 0.0478
Epoch 00029: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_50_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918051

training QR for quantile 0.7
Model: "functional_153"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_77 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_456 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_457 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_458 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_459 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_460 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_461 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 33s - loss: 0.0504 - val_loss: 0.0485
Epoch 2/100
7493/7493 - 36s - loss: 0.0484 - val_loss: 0.0489
Epoch 3/100
7493/7493 - 45s - loss: 0.0482 - val_loss: 0.0478
Epoch 4/100
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 37s - loss: 0.0481 - val_loss: 0.0484
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 36s - loss: 0.0481 - val_loss: 0.0480
Epoch 7/100
7493/7493 - 41s - loss: 0.0479 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 43s - loss: 0.0479 - val_loss: 0.0480
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100
7493/7493 - 43s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 47s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 42s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 00032: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_50_loss_rk5_05_20210924_B.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918051 events, validating on 1918052

training QR for quantile 0.7
Model: "functional_155"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_78 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_462 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_463 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_464 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_465 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_466 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_467 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0502 - val_loss: 0.0488
Epoch 2/100
7493/7493 - 43s - loss: 0.0484 - val_loss: 0.0480
Epoch 3/100
7493/7493 - 36s - loss: 0.0483 - val_loss: 0.0483
Epoch 4/100
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0479
Epoch 5/100
7493/7493 - 45s - loss: 0.0481 - val_loss: 0.0479
Epoch 6/100
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0480
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 32s - loss: 0.0480 - val_loss: 0.0482
Epoch 8/100
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 48s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 34s - loss: 0.0479 - val_loss: 0.0479
Epoch 11/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 39s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 31/100

Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 32/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 33/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_50_loss_rk5_05_20210924_C.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918052

training QR for quantile 0.7
Model: "functional_157"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_79 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_468 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_469 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_470 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_471 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_472 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_473 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0503 - val_loss: 0.0485
Epoch 2/100
7493/7493 - 35s - loss: 0.0484 - val_loss: 0.0484
Epoch 3/100
7493/7493 - 34s - loss: 0.0482 - val_loss: 0.0479
Epoch 4/100
7493/7493 - 36s - loss: 0.0482 - val_loss: 0.0480
Epoch 5/100
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0483
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0481 - val_loss: 0.0482
Epoch 7/100
7493/7493 - 31s - loss: 0.0479 - val_loss: 0.0478
Epoch 8/100
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0478
Epoch 9/100
7493/7493 - 35s - loss: 0.0479 - val_loss: 0.0479
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 33s - loss: 0.0479 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 12/100
7493/7493 - 48s - loss: 0.0478 - val_loss: 0.0478
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 15/100
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0478
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 17/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 18/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 20/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 21/100
7493/7493 - 34s - loss: 0.0478 - val_loss: 0.0478
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 23/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 24/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0478
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 26/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0478
Epoch 27/100
7493/7493 - 45s - loss: 0.0478 - val_loss: 0.0478
Epoch 28/100

Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0478
Epoch 29/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0478
Epoch 30/100
7493/7493 - 41s - loss: 0.0478 - val_loss: 0.0478
Epoch 00030: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_50_loss_rk5_05_20210924_D.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:5 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2eb1ff28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
training on 1918052 events, validating on 1918051

training QR for quantile 0.7
Model: "functional_159"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_80 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_474 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_475 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_476 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_477 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_478 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_479 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 35s - loss: 0.0514 - val_loss: 0.0491
Epoch 2/100
7493/7493 - 26s - loss: 0.0485 - val_loss: 0.0487
Epoch 3/100
7493/7493 - 32s - loss: 0.0482 - val_loss: 0.0481
Epoch 4/100
7493/7493 - 38s - loss: 0.0481 - val_loss: 0.0478
Epoch 5/100
7493/7493 - 33s - loss: 0.0481 - val_loss: 0.0482
Epoch 6/100
7493/7493 - 36s - loss: 0.0480 - val_loss: 0.0480
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 37s - loss: 0.0480 - val_loss: 0.0479
Epoch 8/100
7493/7493 - 40s - loss: 0.0478 - val_loss: 0.0477
Epoch 9/100
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0478
Epoch 11/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 12/100
7493/7493 - 31s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 38s - loss: 0.0478 - val_loss: 0.0478
Epoch 14/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 15/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 46s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100
7493/7493 - 36s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100
7493/7493 - 33s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100
7493/7493 - 32s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 44s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100
7493/7493 - 35s - loss: 0.0478 - val_loss: 0.0477
Epoch 24/100
7493/7493 - 37s - loss: 0.0478 - val_loss: 0.0477
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35na_sigx_50_loss_rk5_05_20210924_E.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
WARNING:tensorflow:6 out of the last 171 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fda2fd281e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
cuts for quantile 0.7: 
[[1.22750000e+03 1.79574749e+00 2.36578173e-04 1.79528868e+00
  1.79594040e+00]
 [1.28750000e+03 1.82498960e+00 3.48188681e-04 1.82448161e+00
  1.82555163e+00]
 [1.35350000e+03 1.85831466e+00 5.38578095e-04 1.85750532e+00
  1.85904145e+00]
 [1.42200000e+03 1.89474633e+00 4.87816219e-04 1.89406228e+00
  1.89552391e+00]
 [1.49300000e+03 1.93316517e+00 2.49038693e-04 1.93291306e+00
  1.93361378e+00]
 [1.56650000e+03 1.97603073e+00 5.42525081e-04 1.97534609e+00
  1.97673988e+00]
 [1.64250000e+03 2.02114868e+00 2.40831704e-04 2.02086067e+00
  2.02156687e+00]
 [1.72100000e+03 2.07050586e+00 4.05217188e-04 2.07015419e+00
  2.07123017e+00]
 [1.80250000e+03 2.12580991e+00 3.33135379e-04 2.12526631e+00
  2.12614608e+00]
 [1.88700000e+03 2.18594289e+00 8.03337284e-04 2.18474960e+00
  2.18726635e+00]
 [1.97450000e+03 2.25142689e+00 1.06675501e-03 2.25005150e+00
  2.25298762e+00]
 [2.06500000e+03 2.32281847e+00 1.49589133e-03 2.31991625e+00
  2.32415628e+00]
 [2.15850000e+03 2.40072813e+00 1.75918090e-03 2.39768672e+00
  2.40278745e+00]
 [2.25550000e+03 2.48696303e+00 2.01573199e-03 2.48410773e+00
  2.48946571e+00]
 [2.35550000e+03 2.58072562e+00 2.65990799e-03 2.57670212e+00
  2.58427525e+00]
 [2.45900000e+03 2.68309016e+00 9.97852973e-04 2.68186712e+00
  2.68458700e+00]
 [2.56600000e+03 2.79449329e+00 3.22716435e-03 2.79017162e+00
  2.79996848e+00]
 [2.67650000e+03 2.91666808e+00 5.62725527e-03 2.90702152e+00
  2.92334723e+00]
 [2.79100000e+03 3.05312443e+00 5.58514559e-03 3.04253006e+00
  3.05752182e+00]
 [2.90900000e+03 3.20445108e+00 3.39931010e-03 3.19787741e+00
  3.20725012e+00]
 [3.03100000e+03 3.36940126e+00 4.67822869e-03 3.36286211e+00
  3.37709451e+00]
 [3.15700000e+03 3.54755492e+00 7.33226614e-03 3.53858995e+00
  3.55974030e+00]
 [3.28700000e+03 3.74135256e+00 1.12372975e-02 3.72781777e+00
  3.75567293e+00]
 [3.42150000e+03 3.95354595e+00 1.43851213e-02 3.93713570e+00
  3.97836065e+00]
 [3.56100000e+03 4.19454384e+00 1.04317888e-02 4.18213844e+00
  4.21094990e+00]
 [3.70500000e+03 4.45985527e+00 1.95381796e-02 4.43130636e+00
  4.49208689e+00]
 [3.85300000e+03 4.74169979e+00 3.27365634e-02 4.70594931e+00
  4.79437542e+00]
 [4.00600000e+03 5.04559050e+00 4.47656517e-02 4.97636890e+00
  5.10896254e+00]
 [4.16450000e+03 5.37577496e+00 5.32900772e-02 5.28384352e+00
  5.43597698e+00]
 [4.32800000e+03 5.72933931e+00 4.94568691e-02 5.65830231e+00
  5.77414513e+00]
 [4.49700000e+03 6.09768448e+00 5.25225831e-02 6.01771450e+00
  6.15464926e+00]
 [4.67150000e+03 6.47910995e+00 6.47341886e-02 6.36646938e+00
  6.55956697e+00]
 [4.85150000e+03 6.87342405e+00 8.29189136e-02 6.72705364e+00
  6.97833586e+00]
 [5.03750000e+03 7.28158703e+00 1.04622307e-01 7.10058069e+00
  7.41187334e+00]
 [5.22950000e+03 7.70349035e+00 1.28565325e-01 7.48706532e+00
  7.85997200e+00]
 [5.45050000e+03 8.18958836e+00 1.57259778e-01 7.93254185e+00
  8.37620163e+00]
 [5.65550000e+03 8.64081078e+00 1.84478819e-01 8.34635830e+00
  8.85529232e+00]
 [5.84400000e+03 9.05578194e+00 2.09913929e-01 8.72697449e+00
  9.29591179e+00]
 [6.06200000e+03 9.53563671e+00 2.39780653e-01 9.16689396e+00
  9.80548000e+00]
 [6.28750000e+03 1.00318018e+01 2.71044389e-01 9.62154961e+00
  1.03324614e+01]
 [6.52000000e+03 1.05430756e+01 3.03640983e-01 1.00897427e+01
  1.08755903e+01]
 [6.76000000e+03 1.10704317e+01 3.37693117e-01 1.05722275e+01
  1.14359236e+01]]
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
training on 1918051 events, validating on 1918052

training QR for quantile 0.9
Model: "functional_161"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_81 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_480 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_481 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_482 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_483 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_484 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_485 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 38s - loss: 0.0276 - val_loss: 0.0257
Epoch 2/100
7493/7493 - 32s - loss: 0.0257 - val_loss: 0.0257
Epoch 3/100
7493/7493 - 43s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100
7493/7493 - 34s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 34s - loss: 0.0254 - val_loss: 0.0254
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 35s - loss: 0.0254 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 41s - loss: 0.0253 - val_loss: 0.0255
Epoch 9/100
7493/7493 - 40s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 41s - loss: 0.0252 - val_loss: 0.0253
Epoch 12/100
7493/7493 - 45s - loss: 0.0252 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 45s - loss: 0.0252 - val_loss: 0.0253
Epoch 14/100

Epoch 00014: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 15/100
7493/7493 - 46s - loss: 0.0252 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 17/100

Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 41s - loss: 0.0252 - val_loss: 0.0253
Epoch 18/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 20/100

Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 21/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 23/100

Epoch 00023: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 24/100
7493/7493 - 33s - loss: 0.0252 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 26/100

Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 27/100
7493/7493 - 31s - loss: 0.0252 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 42s - loss: 0.0252 - val_loss: 0.0253
Epoch 29/100

Epoch 00029: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 36s - loss: 0.0252 - val_loss: 0.0253
Epoch 30/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 35s - loss: 0.0252 - val_loss: 0.0253
Epoch 32/100

Epoch 00032: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 37s - loss: 0.0252 - val_loss: 0.0253
Epoch 33/100
7493/7493 - 34s - loss: 0.0252 - val_loss: 0.0253
Epoch 00033: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35na_sigx_50_loss_rk5_05_20210924_A.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113/envelope/GtoWW35na/xsec_30/envelope/GtoWW35na/xsec_40/envelope/GtoWW35na/xsec_50
training on 1918052 events, validating on 1918051

training QR for quantile 0.9
Model: "functional_163"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_82 (InputLayer)        [(None, 1)]               0         
_________________________________________________________________
Std_Normalize (StdNormalizat (None, 1)                 0         
_________________________________________________________________
dense_486 (Dense)            (None, 60)                120       
_________________________________________________________________
dense_487 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_488 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_489 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_490 (Dense)            (None, 60)                3660      
_________________________________________________________________
dense_491 (Dense)            (None, 1)                 61        
=================================================================
Total params: 14,821
Trainable params: 14,821
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
7493/7493 - 36s - loss: 0.0274 - val_loss: 0.0259
Epoch 2/100
7493/7493 - 31s - loss: 0.0257 - val_loss: 0.0254
Epoch 3/100
7493/7493 - 30s - loss: 0.0256 - val_loss: 0.0257
Epoch 4/100
7493/7493 - 35s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
7493/7493 - 33s - loss: 0.0255 - val_loss: 0.0255
Epoch 6/100
7493/7493 - 38s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 10/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 11/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 13/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 14/100
7493/7493 - 42s - loss: 0.0253 - val_loss: 0.0253
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 16/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 17/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 19/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 20/100
7493/7493 - 46s - loss: 0.0253 - val_loss: 0.0253
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 22/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 23/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 25/100
7493/7493 - 36s - loss: 0.0253 - val_loss: 0.0253
Epoch 26/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 27/100

Epoch 00027: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
7493/7493 - 45s - loss: 0.0253 - val_loss: 0.0253
Epoch 28/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 29/100
7493/7493 - 37s - loss: 0.0253 - val_loss: 0.0253
Epoch 30/100

Epoch 00030: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 31/100
7493/7493 - 32s - loss: 0.0253 - val_loss: 0.0253
Epoch 32/100
7493/7493 - 34s - loss: 0.0253 - val_loss: 0.0253
Epoch 33/100

Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 34/100
7493/7493 - 39s - loss: 0.0253 - val_loss: 0.0253
Epoch 35/100
7493/7493 - 35s - loss: 0.0253 - val_loss: 0.0253
Epoch 36/100

Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 37/100
7493/7493 - 33s - loss: 0.0253 - val_loss: 0.0253
Epoch 00037: early stopping
