setGPU: Setting GPU to: 1

**********************************************************************
			 TRAINING RUN 
Parameters(run_n_vae=113, run_n_qr=3, qcd_sample_id='qcdSigReco', qcd_ext_sample_id='qcdSigExtReco', qcd_train_sample_id='qcdSigAllTrainReco', qcd_test_sample_id='qcdSigAllTestReco', sig_sample_id=None, strategy_id='rk5_05', epochs=100, read_n=None, poly_qr=True)
**********************************************************************
reading samples from /eos/user/k/kiwoznia/data/VAE_results/events/run_113
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_ALL_Train_signalregion_parts
1917939 events read in 1 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_ALL_Train_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_ALL_Test_signalregion_parts
7671759 events read in 1 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/qcd_sqrtshatTeV_13TeV_PU40_NEW_ALL_Test_signalregion_parts
[DataReader] read_jet_features_from_dir(): reading all events from /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
531825 events read in 2 files in dir /eos/user/k/kiwoznia/data/VAE_results/events/run_113/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_NEW_parts
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.1
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_1[0][0]                    
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 3)            0           lambda[0][0]                     
                                                                 lambda_1[0][0]                   
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 1)            4           concatenate[0][0]                
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 12s - loss: 0.0836 - val_loss: 0.0236
Epoch 2/100
5994/5994 - 10s - loss: 0.0239 - val_loss: 0.0236
Epoch 3/100
5994/5994 - 13s - loss: 0.0239 - val_loss: 0.0237
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 11s - loss: 0.0239 - val_loss: 0.0237
Epoch 5/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0238
Epoch 6/100
5994/5994 - 10s - loss: 0.0237 - val_loss: 0.0237
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 8/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 9/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 11/100
5994/5994 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 12/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 10s - loss: 0.0237 - val_loss: 0.0236
Epoch 14/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 15/100
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 17/100
5994/5994 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 18/100
5994/5994 - 10s - loss: 0.0237 - val_loss: 0.0236
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 12s - loss: 0.0237 - val_loss: 0.0236
Epoch 20/100
5994/5994 - 11s - loss: 0.0237 - val_loss: 0.0236
Epoch 21/100
5994/5994 - 9s - loss: 0.0237 - val_loss: 0.0236
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
5994/5994 - 6s - loss: 0.0237 - val_loss: 0.0236
Epoch 23/100
5994/5994 - 6s - loss: 0.0237 - val_loss: 0.0236
Epoch 24/100
5994/5994 - 4s - loss: 0.0237 - val_loss: 0.0236
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
5994/5994 - 4s - loss: 0.0237 - val_loss: 0.0236
Epoch 26/100
5994/5994 - 4s - loss: 0.0237 - val_loss: 0.0236
Epoch 00026: early stopping
saving model QRmodel_run_113_qnt_10_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.3
Model: "functional_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_2[0][0]                    
__________________________________________________________________________________________________
lambda_2 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_3 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 3)            0           lambda_2[0][0]                   
                                                                 lambda_3[0][0]                   
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1)            4           concatenate_1[0][0]              
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 4s - loss: 0.3434 - val_loss: 0.0469
Epoch 2/100
5994/5994 - 4s - loss: 0.0469 - val_loss: 0.0474
Epoch 3/100
5994/5994 - 4s - loss: 0.0469 - val_loss: 0.0465
Epoch 4/100
5994/5994 - 4s - loss: 0.0469 - val_loss: 0.0466
Epoch 5/100
5994/5994 - 4s - loss: 0.0469 - val_loss: 0.0466
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 12s - loss: 0.0469 - val_loss: 0.0465
Epoch 7/100
5994/5994 - 8s - loss: 0.0467 - val_loss: 0.0466
Epoch 8/100
5994/5994 - 11s - loss: 0.0467 - val_loss: 0.0465
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 10s - loss: 0.0467 - val_loss: 0.0465
Epoch 10/100
5994/5994 - 9s - loss: 0.0466 - val_loss: 0.0465
Epoch 11/100
5994/5994 - 12s - loss: 0.0466 - val_loss: 0.0465
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 11s - loss: 0.0466 - val_loss: 0.0465
Epoch 13/100
5994/5994 - 12s - loss: 0.0466 - val_loss: 0.0465
Epoch 14/100
5994/5994 - 10s - loss: 0.0466 - val_loss: 0.0465
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 8s - loss: 0.0466 - val_loss: 0.0465
Epoch 16/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 17/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 19/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 20/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 22/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 23/100
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 24/100

Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
5994/5994 - 4s - loss: 0.0466 - val_loss: 0.0465
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_30_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.5
Model: "functional_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_3[0][0]                    
__________________________________________________________________________________________________
lambda_4 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_5 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 3)            0           lambda_4[0][0]                   
                                                                 lambda_5[0][0]                   
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1)            4           concatenate_2[0][0]              
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 7s - loss: 0.2046 - val_loss: 0.0538
Epoch 2/100
5994/5994 - 10s - loss: 0.0542 - val_loss: 0.0538
Epoch 3/100
5994/5994 - 11s - loss: 0.0541 - val_loss: 0.0539
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 11s - loss: 0.0542 - val_loss: 0.0550
Epoch 5/100
5994/5994 - 13s - loss: 0.0539 - val_loss: 0.0538
Epoch 6/100
5994/5994 - 10s - loss: 0.0539 - val_loss: 0.0538
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 11s - loss: 0.0539 - val_loss: 0.0538
Epoch 8/100
5994/5994 - 13s - loss: 0.0539 - val_loss: 0.0538
Epoch 9/100
5994/5994 - 11s - loss: 0.0539 - val_loss: 0.0538
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 11/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 12/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 14/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 15/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 17/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 18/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 20/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 21/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 23/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 24/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 25/100

Epoch 00025: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 26/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 27/100
5994/5994 - 4s - loss: 0.0539 - val_loss: 0.0538
Epoch 00027: early stopping
saving model QRmodel_run_113_qnt_50_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.7
Model: "functional_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_4 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_4[0][0]                    
__________________________________________________________________________________________________
lambda_6 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_7 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 3)            0           lambda_6[0][0]                   
                                                                 lambda_7[0][0]                   
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            4           concatenate_3[0][0]              
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 4s - loss: 0.2655 - val_loss: 0.0478
Epoch 2/100
5994/5994 - 4s - loss: 0.0481 - val_loss: 0.0480
Epoch 3/100
5994/5994 - 4s - loss: 0.0480 - val_loss: 0.0479
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 4s - loss: 0.0480 - val_loss: 0.0478
Epoch 5/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 6/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0478
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0478
Epoch 8/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0478
Epoch 9/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 10/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 11/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 13/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 14/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 16/100
5994/5994 - 4s - loss: 0.0478 - val_loss: 0.0477
Epoch 17/100
5994/5994 - 5s - loss: 0.0478 - val_loss: 0.0477
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 10s - loss: 0.0478 - val_loss: 0.0477
Epoch 19/100
5994/5994 - 12s - loss: 0.0478 - val_loss: 0.0477
Epoch 20/100
5994/5994 - 9s - loss: 0.0478 - val_loss: 0.0477
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 11s - loss: 0.0478 - val_loss: 0.0477
Epoch 22/100
5994/5994 - 11s - loss: 0.0478 - val_loss: 0.0477
Epoch 23/100
5994/5994 - 11s - loss: 0.0478 - val_loss: 0.0477
Epoch 00023: early stopping
saving model QRmodel_run_113_qnt_70_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.9
Model: "functional_9"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_5 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_5[0][0]                    
__________________________________________________________________________________________________
lambda_8 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_9 (Lambda)               (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 3)            0           lambda_8[0][0]                   
                                                                 lambda_9[0][0]                   
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 1)            4           concatenate_4[0][0]              
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 4s - loss: 0.2840 - val_loss: 0.0252
Epoch 2/100
5994/5994 - 4s - loss: 0.0255 - val_loss: 0.0252
Epoch 3/100
5994/5994 - 4s - loss: 0.0255 - val_loss: 0.0255
Epoch 4/100

Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 4s - loss: 0.0255 - val_loss: 0.0254
Epoch 5/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 6/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0253
Epoch 7/100

Epoch 00007: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0253
Epoch 8/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 9/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 10/100

Epoch 00010: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 11/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 12/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 13/100

Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 14/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 15/100
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 16/100

Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 4s - loss: 0.0253 - val_loss: 0.0252
Epoch 17/100
5994/5994 - 5s - loss: 0.0253 - val_loss: 0.0252
Epoch 18/100
5994/5994 - 11s - loss: 0.0253 - val_loss: 0.0252
Epoch 19/100

Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 12s - loss: 0.0253 - val_loss: 0.0252
Epoch 20/100
5994/5994 - 11s - loss: 0.0253 - val_loss: 0.0252
Epoch 21/100
5994/5994 - 14s - loss: 0.0253 - val_loss: 0.0252
Epoch 22/100

Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.
5994/5994 - 11s - loss: 0.0253 - val_loss: 0.0252
Epoch 23/100
5994/5994 - 10s - loss: 0.0253 - val_loss: 0.0252
Epoch 24/100
5994/5994 - 13s - loss: 0.0253 - val_loss: 0.0252
Epoch 00024: early stopping
saving model QRmodel_run_113_qnt_90_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
training on 1534351 events, validating on 383588

training <class 'dadrah.selection.discriminator.QRDiscriminatorPoly_KerasAPI'> QR for quantile 0.99
Model: "functional_11"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_6 (InputLayer)            [(None, 1)]          0                                            
__________________________________________________________________________________________________
Std_Normalize (StdNormalization (None, 1)            0           input_6[0][0]                    
__________________________________________________________________________________________________
lambda_10 (Lambda)              (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
lambda_11 (Lambda)              (None, 1)            0           Std_Normalize[0][0]              
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 3)            0           lambda_10[0][0]                  
                                                                 lambda_11[0][0]                  
                                                                 Std_Normalize[0][0]              
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            4           concatenate_5[0][0]              
==================================================================================================
Total params: 4
Trainable params: 4
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
5994/5994 - 4s - loss: 0.1937 - val_loss: 0.0141
Epoch 2/100
5994/5994 - 4s - loss: 0.0109 - val_loss: 0.0074
Epoch 3/100
5994/5994 - 4s - loss: 0.0052 - val_loss: 0.0043
Epoch 4/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 5/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 6/100

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0044
Epoch 7/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 8/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 9/100

Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 10/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 11/100
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 12/100

Epoch 00012: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.
5994/5994 - 4s - loss: 0.0043 - val_loss: 0.0043
Epoch 13/100
5994/5994 - 10s - loss: 0.0043 - val_loss: 0.0043
Epoch 14/100
5994/5994 - 10s - loss: 0.0043 - val_loss: 0.0043
Epoch 15/100

Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.
5994/5994 - 12s - loss: 0.0043 - val_loss: 0.0043
Epoch 16/100
5994/5994 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 17/100
5994/5994 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 18/100

Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.
5994/5994 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 19/100
5994/5994 - 11s - loss: 0.0043 - val_loss: 0.0043
Epoch 20/100
5994/5994 - 12s - loss: 0.0043 - val_loss: 0.0043
Epoch 21/100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.
5994/5994 - 9s - loss: 0.0043 - val_loss: 0.0043
Epoch 22/100
5994/5994 - 12s - loss: 0.0043 - val_loss: 0.0043
Epoch 00022: early stopping
saving model QRmodel_run_113_qnt_99_GtoWW35naReco_sigx_0_loss_rk5_05_20220315.h5 to /eos/home-k/kiwoznia/data/QR_models/run_113
predicting qcdSigAllTestReco
predicting GtoWW35naReco
writing selections to  /eos/user/k/kiwoznia/data/QR_results/events/vae_run_113/qr_run_3/sig_GtoWW35naReco/xsec_0/loss_rk5_05
written data sample to /eos/user/k/kiwoznia/data/QR_results/events/vae_run_113/qr_run_3/sig_GtoWW35naReco/xsec_0/loss_rk5_05/qcd_sqrtshatTeV_13TeV_PU40_NEW_ALL_Test_reco.h5
written data sample to /eos/user/k/kiwoznia/data/QR_results/events/vae_run_113/qr_run_3/sig_GtoWW35naReco/xsec_0/loss_rk5_05/RSGraviton_WW_NARROW_13TeV_PU40_3.5TeV_reco.h5
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f656016d748>
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f66603abfd0>
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f6590202908>
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f665b687eb8>
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f6660319160>
loaded model  <tensorflow.python.keras.engine.functional.Functional object at 0x7f6560161a20>
/afs/cern.ch/user/k/kiwoznia/.local/lib/python3.6/site-packages/matplotlib/colors.py:1207: RuntimeWarning: invalid value encountered in less_equal
  mask |= resdat <= 0
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.
'texgyreheros-regular.otf' can not be subsetted into a Type 3 font. The entire font will be embedded in the output.
