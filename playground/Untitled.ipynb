{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import setGPU\n",
    "from importlib import reload\n",
    "import dadrah.playground.test_gradient_tape as tegrta\n",
    "import dadrah.selection.loss_strategy as ls\n",
    "import dadrah.selection.discriminator as disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([3000, 1], minval=1, maxval=100, dtype=tf.float32)\n",
    "y = tf.random.uniform([3000,1], minval=1, maxval=10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.1\n",
    "strategy = ls.combine_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 22.723190307617188\n",
      "Training loss (for one batch) at step 2: 18.726512908935547\n",
      "Training loss (for one batch) at step 4: 17.877185821533203\n",
      "Training loss (for one batch) at step 6: 14.043351173400879\n",
      "Training loss (for one batch) at step 8: 13.787345886230469\n",
      "Training loss (for one batch) at step 10: 9.477657318115234\n",
      "Training loss (for one batch) at step 12: 8.797708511352539\n",
      "Training loss (for one batch) at step 14: 7.82716178894043\n",
      "Training loss (for one batch) at step 16: 8.134836196899414\n",
      "Training loss (for one batch) at step 18: 8.824692726135254\n",
      "Training loss (for one batch) at step 20: 8.91861343383789\n",
      "Training loss (for one batch) at step 22: 7.822970390319824\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 7.62371301651001\n",
      "Training loss (for one batch) at step 2: 7.972781181335449\n",
      "Training loss (for one batch) at step 4: 7.719485282897949\n",
      "Training loss (for one batch) at step 6: 7.957101821899414\n",
      "Training loss (for one batch) at step 8: 8.944622039794922\n",
      "Training loss (for one batch) at step 10: 7.104853630065918\n",
      "Training loss (for one batch) at step 12: 8.079032897949219\n",
      "Training loss (for one batch) at step 14: 7.681585788726807\n",
      "Training loss (for one batch) at step 16: 6.893377304077148\n",
      "Training loss (for one batch) at step 18: 7.332361698150635\n",
      "Training loss (for one batch) at step 20: 7.6382856369018555\n",
      "Training loss (for one batch) at step 22: 7.4368696212768555\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 7.610207557678223\n",
      "Training loss (for one batch) at step 2: 7.962589263916016\n",
      "Training loss (for one batch) at step 4: 7.375912666320801\n",
      "Training loss (for one batch) at step 6: 8.056855201721191\n",
      "Training loss (for one batch) at step 8: 8.44125747680664\n",
      "Training loss (for one batch) at step 10: 7.049088001251221\n",
      "Training loss (for one batch) at step 12: 7.703713893890381\n",
      "Training loss (for one batch) at step 14: 7.692936420440674\n",
      "Training loss (for one batch) at step 16: 6.889401435852051\n",
      "Training loss (for one batch) at step 18: 7.30406379699707\n",
      "Training loss (for one batch) at step 20: 7.553789138793945\n",
      "Training loss (for one batch) at step 22: 7.385063171386719\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 7.4821271896362305\n",
      "Training loss (for one batch) at step 2: 7.953469276428223\n",
      "Training loss (for one batch) at step 4: 7.420279026031494\n",
      "Training loss (for one batch) at step 6: 8.052042961120605\n",
      "Training loss (for one batch) at step 8: 8.41636848449707\n",
      "Training loss (for one batch) at step 10: 7.058048248291016\n",
      "Training loss (for one batch) at step 12: 7.629293441772461\n",
      "Training loss (for one batch) at step 14: 7.7058305740356445\n",
      "Training loss (for one batch) at step 16: 6.894387245178223\n",
      "Training loss (for one batch) at step 18: 7.290522575378418\n",
      "Training loss (for one batch) at step 20: 7.567924499511719\n",
      "Training loss (for one batch) at step 22: 7.39055061340332\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 7.460436820983887\n",
      "Training loss (for one batch) at step 2: 7.953834533691406\n",
      "Training loss (for one batch) at step 4: 7.46484899520874\n",
      "Training loss (for one batch) at step 6: 8.018708229064941\n",
      "Training loss (for one batch) at step 8: 8.431903839111328\n",
      "Training loss (for one batch) at step 10: 7.053131103515625\n",
      "Training loss (for one batch) at step 12: 7.621234893798828\n",
      "Training loss (for one batch) at step 14: 7.718959331512451\n",
      "Training loss (for one batch) at step 16: 6.907697677612305\n",
      "Training loss (for one batch) at step 18: 7.28945255279541\n",
      "Training loss (for one batch) at step 20: 7.59597110748291\n",
      "Training loss (for one batch) at step 22: 7.3997802734375\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 7.4524617195129395\n",
      "Training loss (for one batch) at step 2: 7.953815937042236\n",
      "Training loss (for one batch) at step 4: 7.52037239074707\n",
      "Training loss (for one batch) at step 6: 8.002670288085938\n",
      "Training loss (for one batch) at step 8: 8.433650970458984\n",
      "Training loss (for one batch) at step 10: 7.05470609664917\n",
      "Training loss (for one batch) at step 12: 7.606448173522949\n",
      "Training loss (for one batch) at step 14: 7.72783899307251\n",
      "Training loss (for one batch) at step 16: 6.901275634765625\n",
      "Training loss (for one batch) at step 18: 7.279477119445801\n",
      "Training loss (for one batch) at step 20: 7.600174903869629\n",
      "Training loss (for one batch) at step 22: 7.403210639953613\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 7.448293209075928\n",
      "Training loss (for one batch) at step 2: 7.952789306640625\n",
      "Training loss (for one batch) at step 4: 7.520801067352295\n",
      "Training loss (for one batch) at step 6: 8.020883560180664\n",
      "Training loss (for one batch) at step 8: 8.407705307006836\n",
      "Training loss (for one batch) at step 10: 7.061428070068359\n",
      "Training loss (for one batch) at step 12: 7.601154327392578\n",
      "Training loss (for one batch) at step 14: 7.723132133483887\n",
      "Training loss (for one batch) at step 16: 6.885157108306885\n",
      "Training loss (for one batch) at step 18: 7.265491485595703\n",
      "Training loss (for one batch) at step 20: 7.599574089050293\n",
      "Training loss (for one batch) at step 22: 7.406978607177734\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 7.44868278503418\n",
      "Training loss (for one batch) at step 2: 7.952323913574219\n",
      "Training loss (for one batch) at step 4: 7.548980712890625\n",
      "Training loss (for one batch) at step 6: 8.010673522949219\n",
      "Training loss (for one batch) at step 8: 8.417348861694336\n",
      "Training loss (for one batch) at step 10: 7.059613227844238\n",
      "Training loss (for one batch) at step 12: 7.596672534942627\n",
      "Training loss (for one batch) at step 14: 7.7340087890625\n",
      "Training loss (for one batch) at step 16: 6.891622543334961\n",
      "Training loss (for one batch) at step 18: 7.267451763153076\n",
      "Training loss (for one batch) at step 20: 7.600060939788818\n",
      "Training loss (for one batch) at step 22: 7.405178546905518\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 7.448149681091309\n",
      "Training loss (for one batch) at step 2: 7.952160835266113\n",
      "Training loss (for one batch) at step 4: 7.570479393005371\n",
      "Training loss (for one batch) at step 6: 8.004764556884766\n",
      "Training loss (for one batch) at step 8: 8.427373886108398\n",
      "Training loss (for one batch) at step 10: 7.056582450866699\n",
      "Training loss (for one batch) at step 12: 7.5939202308654785\n",
      "Training loss (for one batch) at step 14: 7.741516590118408\n",
      "Training loss (for one batch) at step 16: 6.890048027038574\n",
      "Training loss (for one batch) at step 18: 7.260882377624512\n",
      "Training loss (for one batch) at step 20: 7.593024253845215\n",
      "Training loss (for one batch) at step 22: 7.405592918395996\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 7.448277473449707\n",
      "Training loss (for one batch) at step 2: 7.9527435302734375\n",
      "Training loss (for one batch) at step 4: 7.572715759277344\n",
      "Training loss (for one batch) at step 6: 8.011704444885254\n",
      "Training loss (for one batch) at step 8: 8.408214569091797\n",
      "Training loss (for one batch) at step 10: 7.069927215576172\n",
      "Training loss (for one batch) at step 12: 7.575007915496826\n",
      "Training loss (for one batch) at step 14: 7.759005546569824\n",
      "Training loss (for one batch) at step 16: 6.901210784912109\n",
      "Training loss (for one batch) at step 18: 7.256543159484863\n",
      "Training loss (for one batch) at step 20: 7.581090927124023\n",
      "Training loss (for one batch) at step 22: 7.413547515869141\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 7.448091506958008\n",
      "Training loss (for one batch) at step 2: 7.95063591003418\n",
      "Training loss (for one batch) at step 4: 7.56707239151001\n",
      "Training loss (for one batch) at step 6: 8.020336151123047\n",
      "Training loss (for one batch) at step 8: 8.392416000366211\n",
      "Training loss (for one batch) at step 10: 7.07766056060791\n",
      "Training loss (for one batch) at step 12: 7.561711311340332\n",
      "Training loss (for one batch) at step 14: 7.769865989685059\n",
      "Training loss (for one batch) at step 16: 6.905088424682617\n",
      "Training loss (for one batch) at step 18: 7.254219055175781\n",
      "Training loss (for one batch) at step 20: 7.57518196105957\n",
      "Training loss (for one batch) at step 22: 7.425330638885498\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 7.449182510375977\n",
      "Training loss (for one batch) at step 2: 7.957594871520996\n",
      "Training loss (for one batch) at step 4: 7.588379859924316\n",
      "Training loss (for one batch) at step 6: 8.023102760314941\n",
      "Training loss (for one batch) at step 8: 8.375465393066406\n",
      "Training loss (for one batch) at step 10: 7.091634750366211\n",
      "Training loss (for one batch) at step 12: 7.535261154174805\n",
      "Training loss (for one batch) at step 14: 7.788714408874512\n",
      "Training loss (for one batch) at step 16: 6.917623043060303\n",
      "Training loss (for one batch) at step 18: 7.2514967918396\n",
      "Training loss (for one batch) at step 20: 7.569599151611328\n",
      "Training loss (for one batch) at step 22: 7.43590784072876\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 7.449154376983643\n",
      "Training loss (for one batch) at step 2: 7.9581990242004395\n",
      "Training loss (for one batch) at step 4: 7.593032360076904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 6: 8.033365249633789\n",
      "Training loss (for one batch) at step 8: 8.355247497558594\n",
      "Training loss (for one batch) at step 10: 7.101864814758301\n",
      "Training loss (for one batch) at step 12: 7.530684471130371\n",
      "Training loss (for one batch) at step 14: 7.79904317855835\n",
      "Training loss (for one batch) at step 16: 6.924688816070557\n",
      "Training loss (for one batch) at step 18: 7.243954658508301\n",
      "Training loss (for one batch) at step 20: 7.560063362121582\n",
      "Training loss (for one batch) at step 22: 7.454474449157715\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 7.449627876281738\n",
      "Training loss (for one batch) at step 2: 7.9605536460876465\n",
      "Training loss (for one batch) at step 4: 7.591211318969727\n",
      "Training loss (for one batch) at step 6: 8.043828964233398\n",
      "Training loss (for one batch) at step 8: 8.349967956542969\n",
      "Training loss (for one batch) at step 10: 7.096415042877197\n",
      "Training loss (for one batch) at step 12: 7.536911964416504\n",
      "Training loss (for one batch) at step 14: 7.783079147338867\n",
      "Training loss (for one batch) at step 16: 6.903011798858643\n",
      "Training loss (for one batch) at step 18: 7.244721412658691\n",
      "Training loss (for one batch) at step 20: 7.57710599899292\n",
      "Training loss (for one batch) at step 22: 7.430218696594238\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 7.448417663574219\n",
      "Training loss (for one batch) at step 2: 7.956272125244141\n",
      "Training loss (for one batch) at step 4: 7.592545509338379\n",
      "Training loss (for one batch) at step 6: 8.031261444091797\n",
      "Training loss (for one batch) at step 8: 8.360004425048828\n",
      "Training loss (for one batch) at step 10: 7.10053825378418\n",
      "Training loss (for one batch) at step 12: 7.531215190887451\n",
      "Training loss (for one batch) at step 14: 7.800199508666992\n",
      "Training loss (for one batch) at step 16: 6.925473213195801\n",
      "Training loss (for one batch) at step 18: 7.242524147033691\n",
      "Training loss (for one batch) at step 20: 7.561555862426758\n",
      "Training loss (for one batch) at step 22: 7.455698013305664\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 7.4495849609375\n",
      "Training loss (for one batch) at step 2: 7.96151065826416\n",
      "Training loss (for one batch) at step 4: 7.586518287658691\n",
      "Training loss (for one batch) at step 6: 8.05671501159668\n",
      "Training loss (for one batch) at step 8: 8.331375122070312\n",
      "Training loss (for one batch) at step 10: 7.099407196044922\n",
      "Training loss (for one batch) at step 12: 7.54011344909668\n",
      "Training loss (for one batch) at step 14: 7.781774520874023\n",
      "Training loss (for one batch) at step 16: 6.902274131774902\n",
      "Training loss (for one batch) at step 18: 7.243676662445068\n",
      "Training loss (for one batch) at step 20: 7.580935478210449\n",
      "Training loss (for one batch) at step 22: 7.431439399719238\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 7.4480204582214355\n",
      "Training loss (for one batch) at step 2: 7.956271171569824\n",
      "Training loss (for one batch) at step 4: 7.591512680053711\n",
      "Training loss (for one batch) at step 6: 8.03592300415039\n",
      "Training loss (for one batch) at step 8: 8.353944778442383\n",
      "Training loss (for one batch) at step 10: 7.103903770446777\n",
      "Training loss (for one batch) at step 12: 7.529892921447754\n",
      "Training loss (for one batch) at step 14: 7.800705909729004\n",
      "Training loss (for one batch) at step 16: 6.91397762298584\n",
      "Training loss (for one batch) at step 18: 7.236376762390137\n",
      "Training loss (for one batch) at step 20: 7.559867858886719\n",
      "Training loss (for one batch) at step 22: 7.450629711151123\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 7.449512958526611\n",
      "Training loss (for one batch) at step 2: 7.960780143737793\n",
      "Training loss (for one batch) at step 4: 7.583898544311523\n",
      "Training loss (for one batch) at step 6: 8.05634880065918\n",
      "Training loss (for one batch) at step 8: 8.33369255065918\n",
      "Training loss (for one batch) at step 10: 7.099941253662109\n",
      "Training loss (for one batch) at step 12: 7.539291858673096\n",
      "Training loss (for one batch) at step 14: 7.782773017883301\n",
      "Training loss (for one batch) at step 16: 6.903046607971191\n",
      "Training loss (for one batch) at step 18: 7.244661331176758\n",
      "Training loss (for one batch) at step 20: 7.578213691711426\n",
      "Training loss (for one batch) at step 22: 7.430768966674805\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 7.448489189147949\n",
      "Training loss (for one batch) at step 2: 7.95739221572876\n",
      "Training loss (for one batch) at step 4: 7.594188690185547\n",
      "Training loss (for one batch) at step 6: 8.04269027709961\n",
      "Training loss (for one batch) at step 8: 8.348119735717773\n",
      "Training loss (for one batch) at step 10: 7.103000164031982\n",
      "Training loss (for one batch) at step 12: 7.5320234298706055\n",
      "Training loss (for one batch) at step 14: 7.791576385498047\n",
      "Training loss (for one batch) at step 16: 6.905913352966309\n",
      "Training loss (for one batch) at step 18: 7.243413925170898\n",
      "Training loss (for one batch) at step 20: 7.573342323303223\n",
      "Training loss (for one batch) at step 22: 7.435446739196777\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 7.449213981628418\n",
      "Training loss (for one batch) at step 2: 7.962628364562988\n",
      "Training loss (for one batch) at step 4: 7.600297451019287\n",
      "Training loss (for one batch) at step 6: 8.047544479370117\n",
      "Training loss (for one batch) at step 8: 8.338099479675293\n",
      "Training loss (for one batch) at step 10: 7.110600471496582\n",
      "Training loss (for one batch) at step 12: 7.5281853675842285\n",
      "Training loss (for one batch) at step 14: 7.801233291625977\n",
      "Training loss (for one batch) at step 16: 6.908975601196289\n",
      "Training loss (for one batch) at step 18: 7.242109298706055\n",
      "Training loss (for one batch) at step 20: 7.566744804382324\n",
      "Training loss (for one batch) at step 22: 7.444056034088135\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 7.449649810791016\n",
      "Training loss (for one batch) at step 2: 7.96330451965332\n",
      "Training loss (for one batch) at step 4: 7.5945234298706055\n",
      "Training loss (for one batch) at step 6: 8.053345680236816\n",
      "Training loss (for one batch) at step 8: 8.333799362182617\n",
      "Training loss (for one batch) at step 10: 7.104546546936035\n",
      "Training loss (for one batch) at step 12: 7.533797264099121\n",
      "Training loss (for one batch) at step 14: 7.778435230255127\n",
      "Training loss (for one batch) at step 16: 6.887451648712158\n",
      "Training loss (for one batch) at step 18: 7.234971046447754\n",
      "Training loss (for one batch) at step 20: 7.564589500427246\n",
      "Training loss (for one batch) at step 22: 7.436207294464111\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 7.448184490203857\n",
      "Training loss (for one batch) at step 2: 7.952455997467041\n",
      "Training loss (for one batch) at step 4: 7.568410873413086\n",
      "Training loss (for one batch) at step 6: 8.044438362121582\n",
      "Training loss (for one batch) at step 8: 8.361114501953125\n",
      "Training loss (for one batch) at step 10: 7.09480094909668\n",
      "Training loss (for one batch) at step 12: 7.537076950073242\n",
      "Training loss (for one batch) at step 14: 7.7870893478393555\n",
      "Training loss (for one batch) at step 16: 6.9067182540893555\n",
      "Training loss (for one batch) at step 18: 7.240110397338867\n",
      "Training loss (for one batch) at step 20: 7.5695319175720215\n",
      "Training loss (for one batch) at step 22: 7.444197654724121\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 7.4493184089660645\n",
      "Training loss (for one batch) at step 2: 7.96251106262207\n",
      "Training loss (for one batch) at step 4: 7.595588207244873\n",
      "Training loss (for one batch) at step 6: 8.0558443069458\n",
      "Training loss (for one batch) at step 8: 8.33022689819336\n",
      "Training loss (for one batch) at step 10: 7.106510162353516\n",
      "Training loss (for one batch) at step 12: 7.533144950866699\n",
      "Training loss (for one batch) at step 14: 7.782011985778809\n",
      "Training loss (for one batch) at step 16: 6.890220642089844\n",
      "Training loss (for one batch) at step 18: 7.234238624572754\n",
      "Training loss (for one batch) at step 20: 7.5684709548950195\n",
      "Training loss (for one batch) at step 22: 7.4346489906311035\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 7.448459625244141\n",
      "Training loss (for one batch) at step 2: 7.9565043449401855\n",
      "Training loss (for one batch) at step 4: 7.595026969909668\n",
      "Training loss (for one batch) at step 6: 8.037388801574707\n",
      "Training loss (for one batch) at step 8: 8.35803508758545\n",
      "Training loss (for one batch) at step 10: 7.102689743041992\n",
      "Training loss (for one batch) at step 12: 7.530948638916016\n",
      "Training loss (for one batch) at step 14: 7.801711082458496\n",
      "Training loss (for one batch) at step 16: 6.922460079193115\n",
      "Training loss (for one batch) at step 18: 7.240030288696289\n",
      "Training loss (for one batch) at step 20: 7.5639801025390625\n",
      "Training loss (for one batch) at step 22: 7.45353364944458\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 7.449803352355957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 2: 7.967255592346191\n",
      "Training loss (for one batch) at step 4: 7.609829425811768\n",
      "Training loss (for one batch) at step 6: 8.04998779296875\n",
      "Training loss (for one batch) at step 8: 8.330480575561523\n",
      "Training loss (for one batch) at step 10: 7.11219596862793\n",
      "Training loss (for one batch) at step 12: 7.528994560241699\n",
      "Training loss (for one batch) at step 14: 7.797492027282715\n",
      "Training loss (for one batch) at step 16: 6.901211738586426\n",
      "Training loss (for one batch) at step 18: 7.234837532043457\n",
      "Training loss (for one batch) at step 20: 7.567105770111084\n",
      "Training loss (for one batch) at step 22: 7.443316459655762\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 7.448978424072266\n",
      "Training loss (for one batch) at step 2: 7.959220886230469\n",
      "Training loss (for one batch) at step 4: 7.595559597015381\n",
      "Training loss (for one batch) at step 6: 8.05402946472168\n",
      "Training loss (for one batch) at step 8: 8.336478233337402\n",
      "Training loss (for one batch) at step 10: 7.10585880279541\n",
      "Training loss (for one batch) at step 12: 7.532480239868164\n",
      "Training loss (for one batch) at step 14: 7.789193153381348\n",
      "Training loss (for one batch) at step 16: 6.898897171020508\n",
      "Training loss (for one batch) at step 18: 7.23538875579834\n",
      "Training loss (for one batch) at step 20: 7.5739030838012695\n",
      "Training loss (for one batch) at step 22: 7.435795783996582\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 7.448645114898682\n",
      "Training loss (for one batch) at step 2: 7.959441661834717\n",
      "Training loss (for one batch) at step 4: 7.604833126068115\n",
      "Training loss (for one batch) at step 6: 8.04564094543457\n",
      "Training loss (for one batch) at step 8: 8.345714569091797\n",
      "Training loss (for one batch) at step 10: 7.1052703857421875\n",
      "Training loss (for one batch) at step 12: 7.53139591217041\n",
      "Training loss (for one batch) at step 14: 7.796605110168457\n",
      "Training loss (for one batch) at step 16: 6.909908294677734\n",
      "Training loss (for one batch) at step 18: 7.2374267578125\n",
      "Training loss (for one batch) at step 20: 7.564194679260254\n",
      "Training loss (for one batch) at step 22: 7.4498395919799805\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 7.449559688568115\n",
      "Training loss (for one batch) at step 2: 7.962406158447266\n",
      "Training loss (for one batch) at step 4: 7.590187072753906\n",
      "Training loss (for one batch) at step 6: 8.054513931274414\n",
      "Training loss (for one batch) at step 8: 8.33154296875\n",
      "Training loss (for one batch) at step 10: 7.109816074371338\n",
      "Training loss (for one batch) at step 12: 7.530862808227539\n",
      "Training loss (for one batch) at step 14: 7.7933268547058105\n",
      "Training loss (for one batch) at step 16: 6.902812957763672\n",
      "Training loss (for one batch) at step 18: 7.237735748291016\n",
      "Training loss (for one batch) at step 20: 7.568398952484131\n",
      "Training loss (for one batch) at step 22: 7.446440696716309\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 7.449350357055664\n",
      "Training loss (for one batch) at step 2: 7.964086055755615\n",
      "Training loss (for one batch) at step 4: 7.603013038635254\n",
      "Training loss (for one batch) at step 6: 8.054720878601074\n",
      "Training loss (for one batch) at step 8: 8.330705642700195\n",
      "Training loss (for one batch) at step 10: 7.106621742248535\n",
      "Training loss (for one batch) at step 12: 7.533297538757324\n",
      "Training loss (for one batch) at step 14: 7.783358097076416\n",
      "Training loss (for one batch) at step 16: 6.890413284301758\n",
      "Training loss (for one batch) at step 18: 7.232616424560547\n",
      "Training loss (for one batch) at step 20: 7.571063041687012\n",
      "Training loss (for one batch) at step 22: 7.435027122497559\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 7.448271751403809\n",
      "Training loss (for one batch) at step 2: 7.956602573394775\n",
      "Training loss (for one batch) at step 4: 7.592954635620117\n",
      "Training loss (for one batch) at step 6: 8.040080070495605\n",
      "Training loss (for one batch) at step 8: 8.355730056762695\n",
      "Training loss (for one batch) at step 10: 7.104667663574219\n",
      "Training loss (for one batch) at step 12: 7.530262470245361\n",
      "Training loss (for one batch) at step 14: 7.800633430480957\n",
      "Training loss (for one batch) at step 16: 6.908687591552734\n",
      "Training loss (for one batch) at step 18: 7.234075546264648\n",
      "Training loss (for one batch) at step 20: 7.565375328063965\n",
      "Training loss (for one batch) at step 22: 7.44985818862915\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 7.449559688568115\n",
      "Training loss (for one batch) at step 2: 7.966181755065918\n",
      "Training loss (for one batch) at step 4: 7.609780311584473\n",
      "Training loss (for one batch) at step 6: 8.051904678344727\n",
      "Training loss (for one batch) at step 8: 8.328177452087402\n",
      "Training loss (for one batch) at step 10: 7.110671043395996\n",
      "Training loss (for one batch) at step 12: 7.532181739807129\n",
      "Training loss (for one batch) at step 14: 7.786306381225586\n",
      "Training loss (for one batch) at step 16: 6.892026901245117\n",
      "Training loss (for one batch) at step 18: 7.232354164123535\n",
      "Training loss (for one batch) at step 20: 7.571679592132568\n",
      "Training loss (for one batch) at step 22: 7.435370445251465\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 7.448286533355713\n",
      "Training loss (for one batch) at step 2: 7.9570465087890625\n",
      "Training loss (for one batch) at step 4: 7.5963616371154785\n",
      "Training loss (for one batch) at step 6: 8.04627513885498\n",
      "Training loss (for one batch) at step 8: 8.344886779785156\n",
      "Training loss (for one batch) at step 10: 7.10930061340332\n",
      "Training loss (for one batch) at step 12: 7.528705596923828\n",
      "Training loss (for one batch) at step 14: 7.803971767425537\n",
      "Training loss (for one batch) at step 16: 6.910714149475098\n",
      "Training loss (for one batch) at step 18: 7.235012054443359\n",
      "Training loss (for one batch) at step 20: 7.564037322998047\n",
      "Training loss (for one batch) at step 22: 7.4504194259643555\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 7.449886322021484\n",
      "Training loss (for one batch) at step 2: 7.967831611633301\n",
      "Training loss (for one batch) at step 4: 7.6129608154296875\n",
      "Training loss (for one batch) at step 6: 8.048555374145508\n",
      "Training loss (for one batch) at step 8: 8.332938194274902\n",
      "Training loss (for one batch) at step 10: 7.112067222595215\n",
      "Training loss (for one batch) at step 12: 7.529071807861328\n",
      "Training loss (for one batch) at step 14: 7.797849655151367\n",
      "Training loss (for one batch) at step 16: 6.900313377380371\n",
      "Training loss (for one batch) at step 18: 7.233831405639648\n",
      "Training loss (for one batch) at step 20: 7.573877334594727\n",
      "Training loss (for one batch) at step 22: 7.434172630310059\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 7.4483489990234375\n",
      "Training loss (for one batch) at step 2: 7.957477569580078\n",
      "Training loss (for one batch) at step 4: 7.5989089012146\n",
      "Training loss (for one batch) at step 6: 8.042961120605469\n",
      "Training loss (for one batch) at step 8: 8.350578308105469\n",
      "Training loss (for one batch) at step 10: 7.106606483459473\n",
      "Training loss (for one batch) at step 12: 7.529750823974609\n",
      "Training loss (for one batch) at step 14: 7.799617767333984\n",
      "Training loss (for one batch) at step 16: 6.905158042907715\n",
      "Training loss (for one batch) at step 18: 7.237689971923828\n",
      "Training loss (for one batch) at step 20: 7.565737724304199\n",
      "Training loss (for one batch) at step 22: 7.449297904968262\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 7.449957370758057\n",
      "Training loss (for one batch) at step 2: 7.968580722808838\n",
      "Training loss (for one batch) at step 4: 7.615321636199951\n",
      "Training loss (for one batch) at step 6: 8.050444602966309\n",
      "Training loss (for one batch) at step 8: 8.327812194824219\n",
      "Training loss (for one batch) at step 10: 7.111086845397949\n",
      "Training loss (for one batch) at step 12: 7.532177925109863\n",
      "Training loss (for one batch) at step 14: 7.783967018127441\n",
      "Training loss (for one batch) at step 16: 6.889019966125488\n",
      "Training loss (for one batch) at step 18: 7.233160018920898\n",
      "Training loss (for one batch) at step 20: 7.566108226776123\n",
      "Training loss (for one batch) at step 22: 7.438477039337158\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 7.448234558105469\n",
      "Training loss (for one batch) at step 2: 7.954156875610352\n",
      "Training loss (for one batch) at step 4: 7.580289840698242\n",
      "Training loss (for one batch) at step 6: 8.03681755065918\n",
      "Training loss (for one batch) at step 8: 8.362478256225586\n",
      "Training loss (for one batch) at step 10: 7.101225852966309\n",
      "Training loss (for one batch) at step 12: 7.531759738922119\n",
      "Training loss (for one batch) at step 14: 7.799842834472656\n",
      "Training loss (for one batch) at step 16: 6.917139053344727\n",
      "Training loss (for one batch) at step 18: 7.235862731933594\n",
      "Training loss (for one batch) at step 20: 7.559443950653076\n",
      "Training loss (for one batch) at step 22: 7.46157169342041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 7.449802398681641\n",
      "Training loss (for one batch) at step 2: 7.963845252990723\n",
      "Training loss (for one batch) at step 4: 7.591770172119141\n",
      "Training loss (for one batch) at step 6: 8.059288024902344\n",
      "Training loss (for one batch) at step 8: 8.328255653381348\n",
      "Training loss (for one batch) at step 10: 7.10693359375\n",
      "Training loss (for one batch) at step 12: 7.533851623535156\n",
      "Training loss (for one batch) at step 14: 7.781269073486328\n",
      "Training loss (for one batch) at step 16: 6.886734485626221\n",
      "Training loss (for one batch) at step 18: 7.230682373046875\n",
      "Training loss (for one batch) at step 20: 7.568514823913574\n",
      "Training loss (for one batch) at step 22: 7.438812255859375\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 7.448484420776367\n",
      "Training loss (for one batch) at step 2: 7.958773136138916\n",
      "Training loss (for one batch) at step 4: 7.600405693054199\n",
      "Training loss (for one batch) at step 6: 8.052024841308594\n",
      "Training loss (for one batch) at step 8: 8.334935188293457\n",
      "Training loss (for one batch) at step 10: 7.112550735473633\n",
      "Training loss (for one batch) at step 12: 7.528244495391846\n",
      "Training loss (for one batch) at step 14: 7.805042743682861\n",
      "Training loss (for one batch) at step 16: 6.910090446472168\n",
      "Training loss (for one batch) at step 18: 7.232802867889404\n",
      "Training loss (for one batch) at step 20: 7.563575744628906\n",
      "Training loss (for one batch) at step 22: 7.4494218826293945\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 7.4489264488220215\n",
      "Training loss (for one batch) at step 2: 7.959916114807129\n",
      "Training loss (for one batch) at step 4: 7.596682548522949\n",
      "Training loss (for one batch) at step 6: 8.054861068725586\n",
      "Training loss (for one batch) at step 8: 8.334575653076172\n",
      "Training loss (for one batch) at step 10: 7.103823184967041\n",
      "Training loss (for one batch) at step 12: 7.535003185272217\n",
      "Training loss (for one batch) at step 14: 7.777085304260254\n",
      "Training loss (for one batch) at step 16: 6.882160186767578\n",
      "Training loss (for one batch) at step 18: 7.231575012207031\n",
      "Training loss (for one batch) at step 20: 7.562853813171387\n",
      "Training loss (for one batch) at step 22: 7.441314697265625\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 7.447983741760254\n",
      "Training loss (for one batch) at step 2: 7.954660415649414\n",
      "Training loss (for one batch) at step 4: 7.583036422729492\n",
      "Training loss (for one batch) at step 6: 8.035931587219238\n",
      "Training loss (for one batch) at step 8: 8.363783836364746\n",
      "Training loss (for one batch) at step 10: 7.100620269775391\n",
      "Training loss (for one batch) at step 12: 7.532111167907715\n",
      "Training loss (for one batch) at step 14: 7.800045490264893\n",
      "Training loss (for one batch) at step 16: 6.917856216430664\n",
      "Training loss (for one batch) at step 18: 7.235889434814453\n",
      "Training loss (for one batch) at step 20: 7.561174392700195\n",
      "Training loss (for one batch) at step 22: 7.460123538970947\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 7.449399948120117\n",
      "Training loss (for one batch) at step 2: 7.962762832641602\n",
      "Training loss (for one batch) at step 4: 7.590118408203125\n",
      "Training loss (for one batch) at step 6: 8.060760498046875\n",
      "Training loss (for one batch) at step 8: 8.326192855834961\n",
      "Training loss (for one batch) at step 10: 7.107686519622803\n",
      "Training loss (for one batch) at step 12: 7.533763885498047\n",
      "Training loss (for one batch) at step 14: 7.782445430755615\n",
      "Training loss (for one batch) at step 16: 6.887497901916504\n",
      "Training loss (for one batch) at step 18: 7.230064392089844\n",
      "Training loss (for one batch) at step 20: 7.569948673248291\n",
      "Training loss (for one batch) at step 22: 7.438309669494629\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 7.447909355163574\n",
      "Training loss (for one batch) at step 2: 7.95816707611084\n",
      "Training loss (for one batch) at step 4: 7.59865140914917\n",
      "Training loss (for one batch) at step 6: 8.053868293762207\n",
      "Training loss (for one batch) at step 8: 8.331727981567383\n",
      "Training loss (for one batch) at step 10: 7.111047744750977\n",
      "Training loss (for one batch) at step 12: 7.53131103515625\n",
      "Training loss (for one batch) at step 14: 7.793776035308838\n",
      "Training loss (for one batch) at step 16: 6.900411128997803\n",
      "Training loss (for one batch) at step 18: 7.234294891357422\n",
      "Training loss (for one batch) at step 20: 7.567671775817871\n",
      "Training loss (for one batch) at step 22: 7.449462413787842\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 7.449032783508301\n",
      "Training loss (for one batch) at step 2: 7.966926097869873\n",
      "Training loss (for one batch) at step 4: 7.615164756774902\n",
      "Training loss (for one batch) at step 6: 8.049662590026855\n",
      "Training loss (for one batch) at step 8: 8.330154418945312\n",
      "Training loss (for one batch) at step 10: 7.112277507781982\n",
      "Training loss (for one batch) at step 12: 7.529595375061035\n",
      "Training loss (for one batch) at step 14: 7.7986907958984375\n",
      "Training loss (for one batch) at step 16: 6.900260925292969\n",
      "Training loss (for one batch) at step 18: 7.231482982635498\n",
      "Training loss (for one batch) at step 20: 7.570304870605469\n",
      "Training loss (for one batch) at step 22: 7.445217609405518\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 7.448153972625732\n",
      "Training loss (for one batch) at step 2: 7.959779262542725\n",
      "Training loss (for one batch) at step 4: 7.606736183166504\n",
      "Training loss (for one batch) at step 6: 8.051349639892578\n",
      "Training loss (for one batch) at step 8: 8.333246231079102\n",
      "Training loss (for one batch) at step 10: 7.113524436950684\n",
      "Training loss (for one batch) at step 12: 7.5284953117370605\n",
      "Training loss (for one batch) at step 14: 7.79868745803833\n",
      "Training loss (for one batch) at step 16: 6.894998550415039\n",
      "Training loss (for one batch) at step 18: 7.229772090911865\n",
      "Training loss (for one batch) at step 20: 7.5673723220825195\n",
      "Training loss (for one batch) at step 22: 7.443636894226074\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 7.447972774505615\n",
      "Training loss (for one batch) at step 2: 7.957082271575928\n",
      "Training loss (for one batch) at step 4: 7.594643592834473\n",
      "Training loss (for one batch) at step 6: 8.052446365356445\n",
      "Training loss (for one batch) at step 8: 8.338146209716797\n",
      "Training loss (for one batch) at step 10: 7.108248710632324\n",
      "Training loss (for one batch) at step 12: 7.531087875366211\n",
      "Training loss (for one batch) at step 14: 7.795350074768066\n",
      "Training loss (for one batch) at step 16: 6.896956920623779\n",
      "Training loss (for one batch) at step 18: 7.230144500732422\n",
      "Training loss (for one batch) at step 20: 7.576296806335449\n",
      "Training loss (for one batch) at step 22: 7.435319900512695\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 7.447598457336426\n",
      "Training loss (for one batch) at step 2: 7.956833839416504\n",
      "Training loss (for one batch) at step 4: 7.601982116699219\n",
      "Training loss (for one batch) at step 6: 8.044723510742188\n",
      "Training loss (for one batch) at step 8: 8.347654342651367\n",
      "Training loss (for one batch) at step 10: 7.107083797454834\n",
      "Training loss (for one batch) at step 12: 7.530161380767822\n",
      "Training loss (for one batch) at step 14: 7.8012495040893555\n",
      "Training loss (for one batch) at step 16: 6.906525611877441\n",
      "Training loss (for one batch) at step 18: 7.231600761413574\n",
      "Training loss (for one batch) at step 20: 7.570417881011963\n",
      "Training loss (for one batch) at step 22: 7.440542221069336\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 7.448062896728516\n",
      "Training loss (for one batch) at step 2: 7.956995010375977\n",
      "Training loss (for one batch) at step 4: 7.601057529449463\n",
      "Training loss (for one batch) at step 6: 8.039113998413086\n",
      "Training loss (for one batch) at step 8: 8.341428756713867\n",
      "Training loss (for one batch) at step 10: 7.115817070007324\n",
      "Training loss (for one batch) at step 12: 7.52767276763916\n",
      "Training loss (for one batch) at step 14: 7.801286220550537\n",
      "Training loss (for one batch) at step 16: 6.891960620880127\n",
      "Training loss (for one batch) at step 18: 7.231196403503418\n",
      "Training loss (for one batch) at step 20: 7.553079128265381\n",
      "Training loss (for one batch) at step 22: 7.454530715942383\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 7.448794841766357\n",
      "Training loss (for one batch) at step 2: 7.954137802124023\n",
      "Training loss (for one batch) at step 4: 7.568531036376953\n",
      "Training loss (for one batch) at step 6: 8.048382759094238\n",
      "Training loss (for one batch) at step 8: 8.358107566833496\n",
      "Training loss (for one batch) at step 10: 7.0963263511657715\n",
      "Training loss (for one batch) at step 12: 7.537104606628418\n",
      "Training loss (for one batch) at step 14: 7.786144256591797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 16: 6.903165340423584\n",
      "Training loss (for one batch) at step 18: 7.242545127868652\n",
      "Training loss (for one batch) at step 20: 7.576935768127441\n",
      "Training loss (for one batch) at step 22: 7.434330463409424\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 7.448892116546631\n",
      "Training loss (for one batch) at step 2: 7.962510585784912\n",
      "Training loss (for one batch) at step 4: 7.60082483291626\n",
      "Training loss (for one batch) at step 6: 8.049226760864258\n",
      "Training loss (for one batch) at step 8: 8.335723876953125\n",
      "Training loss (for one batch) at step 10: 7.110115051269531\n",
      "Training loss (for one batch) at step 12: 7.5300798416137695\n",
      "Training loss (for one batch) at step 14: 7.797059535980225\n",
      "Training loss (for one batch) at step 16: 6.906932830810547\n",
      "Training loss (for one batch) at step 18: 7.241290092468262\n",
      "Training loss (for one batch) at step 20: 7.570888996124268\n",
      "Training loss (for one batch) at step 22: 7.441948890686035\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 7.448738098144531\n",
      "Training loss (for one batch) at step 2: 7.961668968200684\n",
      "Training loss (for one batch) at step 4: 7.591371059417725\n",
      "Training loss (for one batch) at step 6: 8.050827026367188\n",
      "Training loss (for one batch) at step 8: 8.338065147399902\n",
      "Training loss (for one batch) at step 10: 7.111745834350586\n",
      "Training loss (for one batch) at step 12: 7.528558731079102\n",
      "Training loss (for one batch) at step 14: 7.802302360534668\n",
      "Training loss (for one batch) at step 16: 6.905971527099609\n",
      "Training loss (for one batch) at step 18: 7.237465858459473\n",
      "Training loss (for one batch) at step 20: 7.5651021003723145\n",
      "Training loss (for one batch) at step 22: 7.450503349304199\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 7.449258327484131\n",
      "Training loss (for one batch) at step 2: 7.969964027404785\n",
      "Training loss (for one batch) at step 4: 7.612793922424316\n",
      "Training loss (for one batch) at step 6: 8.053764343261719\n",
      "Training loss (for one batch) at step 8: 8.32621955871582\n",
      "Training loss (for one batch) at step 10: 7.110523223876953\n",
      "Training loss (for one batch) at step 12: 7.533192157745361\n",
      "Training loss (for one batch) at step 14: 7.78082799911499\n",
      "Training loss (for one batch) at step 16: 6.884718418121338\n",
      "Training loss (for one batch) at step 18: 7.232925891876221\n",
      "Training loss (for one batch) at step 20: 7.56184720993042\n",
      "Training loss (for one batch) at step 22: 7.442046642303467\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 7.447833061218262\n",
      "Training loss (for one batch) at step 2: 7.955631256103516\n",
      "Training loss (for one batch) at step 4: 7.584962368011475\n",
      "Training loss (for one batch) at step 6: 8.039487838745117\n",
      "Training loss (for one batch) at step 8: 8.360203742980957\n",
      "Training loss (for one batch) at step 10: 7.101789474487305\n",
      "Training loss (for one batch) at step 12: 7.532194137573242\n",
      "Training loss (for one batch) at step 14: 7.799510478973389\n",
      "Training loss (for one batch) at step 16: 6.916919231414795\n",
      "Training loss (for one batch) at step 18: 7.235384941101074\n",
      "Training loss (for one batch) at step 20: 7.560833930969238\n",
      "Training loss (for one batch) at step 22: 7.461398601531982\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 7.449151992797852\n",
      "Training loss (for one batch) at step 2: 7.963262557983398\n",
      "Training loss (for one batch) at step 4: 7.590995788574219\n",
      "Training loss (for one batch) at step 6: 8.059340476989746\n",
      "Training loss (for one batch) at step 8: 8.327568054199219\n",
      "Training loss (for one batch) at step 10: 7.10670280456543\n",
      "Training loss (for one batch) at step 12: 7.534102439880371\n",
      "Training loss (for one batch) at step 14: 7.781030654907227\n",
      "Training loss (for one batch) at step 16: 6.886348724365234\n",
      "Training loss (for one batch) at step 18: 7.230417251586914\n",
      "Training loss (for one batch) at step 20: 7.568620681762695\n",
      "Training loss (for one batch) at step 22: 7.4383721351623535\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 7.448075294494629\n",
      "Training loss (for one batch) at step 2: 7.958282947540283\n",
      "Training loss (for one batch) at step 4: 7.597202301025391\n",
      "Training loss (for one batch) at step 6: 8.053481101989746\n",
      "Training loss (for one batch) at step 8: 8.332162857055664\n",
      "Training loss (for one batch) at step 10: 7.114112377166748\n",
      "Training loss (for one batch) at step 12: 7.527799606323242\n",
      "Training loss (for one batch) at step 14: 7.798125267028809\n",
      "Training loss (for one batch) at step 16: 6.894693851470947\n",
      "Training loss (for one batch) at step 18: 7.231276988983154\n",
      "Training loss (for one batch) at step 20: 7.5645904541015625\n",
      "Training loss (for one batch) at step 22: 7.444365501403809\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 7.448330879211426\n",
      "Training loss (for one batch) at step 2: 7.957646369934082\n",
      "Training loss (for one batch) at step 4: 7.593611717224121\n",
      "Training loss (for one batch) at step 6: 8.050416946411133\n",
      "Training loss (for one batch) at step 8: 8.340980529785156\n",
      "Training loss (for one batch) at step 10: 7.106993675231934\n",
      "Training loss (for one batch) at step 12: 7.531415939331055\n",
      "Training loss (for one batch) at step 14: 7.797975540161133\n",
      "Training loss (for one batch) at step 16: 6.910669326782227\n",
      "Training loss (for one batch) at step 18: 7.2356462478637695\n",
      "Training loss (for one batch) at step 20: 7.565485954284668\n",
      "Training loss (for one batch) at step 22: 7.4517364501953125\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 7.448957920074463\n",
      "Training loss (for one batch) at step 2: 7.964132785797119\n",
      "Training loss (for one batch) at step 4: 7.598601341247559\n",
      "Training loss (for one batch) at step 6: 8.058891296386719\n",
      "Training loss (for one batch) at step 8: 8.32440185546875\n",
      "Training loss (for one batch) at step 10: 7.108799934387207\n",
      "Training loss (for one batch) at step 12: 7.534531593322754\n",
      "Training loss (for one batch) at step 14: 7.774576187133789\n",
      "Training loss (for one batch) at step 16: 6.881941795349121\n",
      "Training loss (for one batch) at step 18: 7.232824325561523\n",
      "Training loss (for one batch) at step 20: 7.570107460021973\n",
      "Training loss (for one batch) at step 22: 7.432814598083496\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 7.447732925415039\n",
      "Training loss (for one batch) at step 2: 7.954392910003662\n",
      "Training loss (for one batch) at step 4: 7.583251953125\n",
      "Training loss (for one batch) at step 6: 8.036768913269043\n",
      "Training loss (for one batch) at step 8: 8.366668701171875\n",
      "Training loss (for one batch) at step 10: 7.096875190734863\n",
      "Training loss (for one batch) at step 12: 7.534257888793945\n",
      "Training loss (for one batch) at step 14: 7.7894487380981445\n",
      "Training loss (for one batch) at step 16: 6.903792381286621\n",
      "Training loss (for one batch) at step 18: 7.240930557250977\n",
      "Training loss (for one batch) at step 20: 7.5759782791137695\n",
      "Training loss (for one batch) at step 22: 7.437926292419434\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 7.449065685272217\n",
      "Training loss (for one batch) at step 2: 7.9657464027404785\n",
      "Training loss (for one batch) at step 4: 7.6146321296691895\n",
      "Training loss (for one batch) at step 6: 8.046504974365234\n",
      "Training loss (for one batch) at step 8: 8.32956314086914\n",
      "Training loss (for one batch) at step 10: 7.113129615783691\n",
      "Training loss (for one batch) at step 12: 7.530135631561279\n",
      "Training loss (for one batch) at step 14: 7.795015335083008\n",
      "Training loss (for one batch) at step 16: 6.902985095977783\n",
      "Training loss (for one batch) at step 18: 7.237193584442139\n",
      "Training loss (for one batch) at step 20: 7.5669450759887695\n",
      "Training loss (for one batch) at step 22: 7.447417259216309\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 7.449409484863281\n",
      "Training loss (for one batch) at step 2: 7.965700149536133\n",
      "Training loss (for one batch) at step 4: 7.605809688568115\n",
      "Training loss (for one batch) at step 6: 8.050822257995605\n",
      "Training loss (for one batch) at step 8: 8.329959869384766\n",
      "Training loss (for one batch) at step 10: 7.113254547119141\n",
      "Training loss (for one batch) at step 12: 7.528741836547852\n",
      "Training loss (for one batch) at step 14: 7.7994232177734375\n",
      "Training loss (for one batch) at step 16: 6.902494430541992\n",
      "Training loss (for one batch) at step 18: 7.2346625328063965\n",
      "Training loss (for one batch) at step 20: 7.5692853927612305\n",
      "Training loss (for one batch) at step 22: 7.4425554275512695\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 7.448661804199219\n",
      "Training loss (for one batch) at step 2: 7.95903205871582\n",
      "Training loss (for one batch) at step 4: 7.5954270362854\n",
      "Training loss (for one batch) at step 6: 8.053313255310059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 8: 8.331623077392578\n",
      "Training loss (for one batch) at step 10: 7.108924865722656\n",
      "Training loss (for one batch) at step 12: 7.531540870666504\n",
      "Training loss (for one batch) at step 14: 7.793542861938477\n",
      "Training loss (for one batch) at step 16: 6.902478218078613\n",
      "Training loss (for one batch) at step 18: 7.236998558044434\n",
      "Training loss (for one batch) at step 20: 7.571258544921875\n",
      "Training loss (for one batch) at step 22: 7.4446024894714355\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 7.448911666870117\n",
      "Training loss (for one batch) at step 2: 7.962349891662598\n",
      "Training loss (for one batch) at step 4: 7.599430084228516\n",
      "Training loss (for one batch) at step 6: 8.057812690734863\n",
      "Training loss (for one batch) at step 8: 8.326594352722168\n",
      "Training loss (for one batch) at step 10: 7.108104228973389\n",
      "Training loss (for one batch) at step 12: 7.533138751983643\n",
      "Training loss (for one batch) at step 14: 7.785914421081543\n",
      "Training loss (for one batch) at step 16: 6.892598628997803\n",
      "Training loss (for one batch) at step 18: 7.2311506271362305\n",
      "Training loss (for one batch) at step 20: 7.575927257537842\n",
      "Training loss (for one batch) at step 22: 7.433476448059082\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 7.447572708129883\n",
      "Training loss (for one batch) at step 2: 7.954951763153076\n",
      "Training loss (for one batch) at step 4: 7.594091415405273\n",
      "Training loss (for one batch) at step 6: 8.037885665893555\n",
      "Training loss (for one batch) at step 8: 8.344552993774414\n",
      "Training loss (for one batch) at step 10: 7.115560531616211\n",
      "Training loss (for one batch) at step 12: 7.528332710266113\n",
      "Training loss (for one batch) at step 14: 7.814023017883301\n",
      "Training loss (for one batch) at step 16: 6.91111946105957\n",
      "Training loss (for one batch) at step 18: 7.2311859130859375\n",
      "Training loss (for one batch) at step 20: 7.561553955078125\n",
      "Training loss (for one batch) at step 22: 7.44993782043457\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 7.448960304260254\n",
      "Training loss (for one batch) at step 2: 7.958802223205566\n",
      "Training loss (for one batch) at step 4: 7.5937910079956055\n",
      "Training loss (for one batch) at step 6: 8.048269271850586\n",
      "Training loss (for one batch) at step 8: 8.343765258789062\n",
      "Training loss (for one batch) at step 10: 7.106389999389648\n",
      "Training loss (for one batch) at step 12: 7.531053066253662\n",
      "Training loss (for one batch) at step 14: 7.797399997711182\n",
      "Training loss (for one batch) at step 16: 6.910804271697998\n",
      "Training loss (for one batch) at step 18: 7.238129615783691\n",
      "Training loss (for one batch) at step 20: 7.563835620880127\n",
      "Training loss (for one batch) at step 22: 7.449407577514648\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 7.449497222900391\n",
      "Training loss (for one batch) at step 2: 7.962558269500732\n",
      "Training loss (for one batch) at step 4: 7.587874412536621\n",
      "Training loss (for one batch) at step 6: 8.05290412902832\n",
      "Training loss (for one batch) at step 8: 8.333824157714844\n",
      "Training loss (for one batch) at step 10: 7.10888671875\n",
      "Training loss (for one batch) at step 12: 7.5311479568481445\n",
      "Training loss (for one batch) at step 14: 7.7921342849731445\n",
      "Training loss (for one batch) at step 16: 6.902348518371582\n",
      "Training loss (for one batch) at step 18: 7.23801326751709\n",
      "Training loss (for one batch) at step 20: 7.574867248535156\n",
      "Training loss (for one batch) at step 22: 7.43393087387085\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 7.448500633239746\n",
      "Training loss (for one batch) at step 2: 7.959050178527832\n",
      "Training loss (for one batch) at step 4: 7.598728179931641\n",
      "Training loss (for one batch) at step 6: 8.044733047485352\n",
      "Training loss (for one batch) at step 8: 8.347527503967285\n",
      "Training loss (for one batch) at step 10: 7.105477333068848\n",
      "Training loss (for one batch) at step 12: 7.53105354309082\n",
      "Training loss (for one batch) at step 14: 7.796661376953125\n",
      "Training loss (for one batch) at step 16: 6.909787654876709\n",
      "Training loss (for one batch) at step 18: 7.238276481628418\n",
      "Training loss (for one batch) at step 20: 7.569652557373047\n",
      "Training loss (for one batch) at step 22: 7.439911842346191\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 7.44861364364624\n",
      "Training loss (for one batch) at step 2: 7.958733081817627\n",
      "Training loss (for one batch) at step 4: 7.5969557762146\n",
      "Training loss (for one batch) at step 6: 8.041196823120117\n",
      "Training loss (for one batch) at step 8: 8.351164817810059\n",
      "Training loss (for one batch) at step 10: 7.106537818908691\n",
      "Training loss (for one batch) at step 12: 7.529725074768066\n",
      "Training loss (for one batch) at step 14: 7.804559707641602\n",
      "Training loss (for one batch) at step 16: 6.924778938293457\n",
      "Training loss (for one batch) at step 18: 7.238069534301758\n",
      "Training loss (for one batch) at step 20: 7.559967041015625\n",
      "Training loss (for one batch) at step 22: 7.4558210372924805\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 7.448953628540039\n",
      "Training loss (for one batch) at step 2: 7.959906578063965\n",
      "Training loss (for one batch) at step 4: 7.588693618774414\n",
      "Training loss (for one batch) at step 6: 8.050691604614258\n",
      "Training loss (for one batch) at step 8: 8.339668273925781\n",
      "Training loss (for one batch) at step 10: 7.108426094055176\n",
      "Training loss (for one batch) at step 12: 7.530534744262695\n",
      "Training loss (for one batch) at step 14: 7.799952983856201\n",
      "Training loss (for one batch) at step 16: 6.914094924926758\n",
      "Training loss (for one batch) at step 18: 7.2345123291015625\n",
      "Training loss (for one batch) at step 20: 7.56341552734375\n",
      "Training loss (for one batch) at step 22: 7.4499897956848145\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 7.448606967926025\n",
      "Training loss (for one batch) at step 2: 7.960623264312744\n",
      "Training loss (for one batch) at step 4: 7.585813045501709\n",
      "Training loss (for one batch) at step 6: 8.058490753173828\n",
      "Training loss (for one batch) at step 8: 8.331174850463867\n",
      "Training loss (for one batch) at step 10: 7.102953910827637\n",
      "Training loss (for one batch) at step 12: 7.536652565002441\n",
      "Training loss (for one batch) at step 14: 7.781829833984375\n",
      "Training loss (for one batch) at step 16: 6.893697738647461\n",
      "Training loss (for one batch) at step 18: 7.233881950378418\n",
      "Training loss (for one batch) at step 20: 7.574563980102539\n",
      "Training loss (for one batch) at step 22: 7.434221267700195\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 7.448020935058594\n",
      "Training loss (for one batch) at step 2: 7.957655906677246\n",
      "Training loss (for one batch) at step 4: 7.600141525268555\n",
      "Training loss (for one batch) at step 6: 8.04452133178711\n",
      "Training loss (for one batch) at step 8: 8.343517303466797\n",
      "Training loss (for one batch) at step 10: 7.107587814331055\n",
      "Training loss (for one batch) at step 12: 7.530289173126221\n",
      "Training loss (for one batch) at step 14: 7.797867774963379\n",
      "Training loss (for one batch) at step 16: 6.903149604797363\n",
      "Training loss (for one batch) at step 18: 7.235531806945801\n",
      "Training loss (for one batch) at step 20: 7.56659460067749\n",
      "Training loss (for one batch) at step 22: 7.449710845947266\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 7.4493818283081055\n",
      "Training loss (for one batch) at step 2: 7.967430114746094\n",
      "Training loss (for one batch) at step 4: 7.614063262939453\n",
      "Training loss (for one batch) at step 6: 8.050765037536621\n",
      "Training loss (for one batch) at step 8: 8.326618194580078\n",
      "Training loss (for one batch) at step 10: 7.108792304992676\n",
      "Training loss (for one batch) at step 12: 7.5337677001953125\n",
      "Training loss (for one batch) at step 14: 7.779909133911133\n",
      "Training loss (for one batch) at step 16: 6.884945869445801\n",
      "Training loss (for one batch) at step 18: 7.230749130249023\n",
      "Training loss (for one batch) at step 20: 7.56561279296875\n",
      "Training loss (for one batch) at step 22: 7.439894199371338\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 7.447734832763672\n",
      "Training loss (for one batch) at step 2: 7.9537787437438965\n",
      "Training loss (for one batch) at step 4: 7.5797014236450195\n",
      "Training loss (for one batch) at step 6: 8.038228988647461\n",
      "Training loss (for one batch) at step 8: 8.359764099121094\n",
      "Training loss (for one batch) at step 10: 7.100364685058594\n",
      "Training loss (for one batch) at step 12: 7.532686710357666\n",
      "Training loss (for one batch) at step 14: 7.797417163848877\n",
      "Training loss (for one batch) at step 16: 6.9150309562683105\n",
      "Training loss (for one batch) at step 18: 7.241055488586426\n",
      "Training loss (for one batch) at step 20: 7.572847366333008\n",
      "Training loss (for one batch) at step 22: 7.444094657897949\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 7.449193000793457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 2: 7.9642109870910645\n",
      "Training loss (for one batch) at step 4: 7.60451602935791\n",
      "Training loss (for one batch) at step 6: 8.050935745239258\n",
      "Training loss (for one batch) at step 8: 8.33091926574707\n",
      "Training loss (for one batch) at step 10: 7.110411643981934\n",
      "Training loss (for one batch) at step 12: 7.530399322509766\n",
      "Training loss (for one batch) at step 14: 7.798259735107422\n",
      "Training loss (for one batch) at step 16: 6.910511493682861\n",
      "Training loss (for one batch) at step 18: 7.236508846282959\n",
      "Training loss (for one batch) at step 20: 7.564128398895264\n",
      "Training loss (for one batch) at step 22: 7.450791835784912\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 7.449369430541992\n",
      "Training loss (for one batch) at step 2: 7.963285446166992\n",
      "Training loss (for one batch) at step 4: 7.5928449630737305\n",
      "Training loss (for one batch) at step 6: 8.05876350402832\n",
      "Training loss (for one batch) at step 8: 8.32533073425293\n",
      "Training loss (for one batch) at step 10: 7.1109113693237305\n",
      "Training loss (for one batch) at step 12: 7.531150817871094\n",
      "Training loss (for one batch) at step 14: 7.790538311004639\n",
      "Training loss (for one batch) at step 16: 6.899219036102295\n",
      "Training loss (for one batch) at step 18: 7.235965251922607\n",
      "Training loss (for one batch) at step 20: 7.571044921875\n",
      "Training loss (for one batch) at step 22: 7.436290740966797\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 7.448728561401367\n",
      "Training loss (for one batch) at step 2: 7.9604268074035645\n",
      "Training loss (for one batch) at step 4: 7.591671466827393\n",
      "Training loss (for one batch) at step 6: 8.047323226928711\n",
      "Training loss (for one batch) at step 8: 8.344926834106445\n",
      "Training loss (for one batch) at step 10: 7.104862213134766\n",
      "Training loss (for one batch) at step 12: 7.531929969787598\n",
      "Training loss (for one batch) at step 14: 7.7935380935668945\n",
      "Training loss (for one batch) at step 16: 6.905771255493164\n",
      "Training loss (for one batch) at step 18: 7.2359209060668945\n",
      "Training loss (for one batch) at step 20: 7.568325042724609\n",
      "Training loss (for one batch) at step 22: 7.442102432250977\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 7.448603630065918\n",
      "Training loss (for one batch) at step 2: 7.959159851074219\n",
      "Training loss (for one batch) at step 4: 7.593149662017822\n",
      "Training loss (for one batch) at step 6: 8.051301002502441\n",
      "Training loss (for one batch) at step 8: 8.334916114807129\n",
      "Training loss (for one batch) at step 10: 7.106416702270508\n",
      "Training loss (for one batch) at step 12: 7.532608985900879\n",
      "Training loss (for one batch) at step 14: 7.788449287414551\n",
      "Training loss (for one batch) at step 16: 6.89794397354126\n",
      "Training loss (for one batch) at step 18: 7.235079765319824\n",
      "Training loss (for one batch) at step 20: 7.5737504959106445\n",
      "Training loss (for one batch) at step 22: 7.435131072998047\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 7.448206424713135\n",
      "Training loss (for one batch) at step 2: 7.958758354187012\n",
      "Training loss (for one batch) at step 4: 7.598738670349121\n",
      "Training loss (for one batch) at step 6: 8.046296119689941\n",
      "Training loss (for one batch) at step 8: 8.34476089477539\n",
      "Training loss (for one batch) at step 10: 7.1053242683410645\n",
      "Training loss (for one batch) at step 12: 7.53151798248291\n",
      "Training loss (for one batch) at step 14: 7.795927047729492\n",
      "Training loss (for one batch) at step 16: 6.909584999084473\n",
      "Training loss (for one batch) at step 18: 7.237669944763184\n",
      "Training loss (for one batch) at step 20: 7.564355373382568\n",
      "Training loss (for one batch) at step 22: 7.451578140258789\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 7.449747085571289\n",
      "Training loss (for one batch) at step 2: 7.970588684082031\n",
      "Training loss (for one batch) at step 4: 7.613459587097168\n",
      "Training loss (for one batch) at step 6: 8.049739837646484\n",
      "Training loss (for one batch) at step 8: 8.33134651184082\n",
      "Training loss (for one batch) at step 10: 7.108570575714111\n",
      "Training loss (for one batch) at step 12: 7.53156852722168\n",
      "Training loss (for one batch) at step 14: 7.790501594543457\n",
      "Training loss (for one batch) at step 16: 6.9008989334106445\n",
      "Training loss (for one batch) at step 18: 7.2372145652771\n",
      "Training loss (for one batch) at step 20: 7.5739216804504395\n",
      "Training loss (for one batch) at step 22: 7.4342732429504395\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 7.448293209075928\n",
      "Training loss (for one batch) at step 2: 7.95867919921875\n",
      "Training loss (for one batch) at step 4: 7.594976425170898\n",
      "Training loss (for one batch) at step 6: 8.045876502990723\n",
      "Training loss (for one batch) at step 8: 8.345702171325684\n",
      "Training loss (for one batch) at step 10: 7.105403900146484\n",
      "Training loss (for one batch) at step 12: 7.531164646148682\n",
      "Training loss (for one batch) at step 14: 7.795562744140625\n",
      "Training loss (for one batch) at step 16: 6.909091949462891\n",
      "Training loss (for one batch) at step 18: 7.23870325088501\n",
      "Training loss (for one batch) at step 20: 7.569609642028809\n",
      "Training loss (for one batch) at step 22: 7.4385175704956055\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 7.448606491088867\n",
      "Training loss (for one batch) at step 2: 7.957766056060791\n",
      "Training loss (for one batch) at step 4: 7.59611177444458\n",
      "Training loss (for one batch) at step 6: 8.038045883178711\n",
      "Training loss (for one batch) at step 8: 8.345647811889648\n",
      "Training loss (for one batch) at step 10: 7.1144185066223145\n",
      "Training loss (for one batch) at step 12: 7.5266571044921875\n",
      "Training loss (for one batch) at step 14: 7.809067249298096\n",
      "Training loss (for one batch) at step 16: 6.906752586364746\n",
      "Training loss (for one batch) at step 18: 7.236305236816406\n",
      "Training loss (for one batch) at step 20: 7.562038421630859\n",
      "Training loss (for one batch) at step 22: 7.448757648468018\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 7.449707984924316\n",
      "Training loss (for one batch) at step 2: 7.965023517608643\n",
      "Training loss (for one batch) at step 4: 7.5971360206604\n",
      "Training loss (for one batch) at step 6: 8.051262855529785\n",
      "Training loss (for one batch) at step 8: 8.330377578735352\n",
      "Training loss (for one batch) at step 10: 7.112624168395996\n",
      "Training loss (for one batch) at step 12: 7.528733253479004\n",
      "Training loss (for one batch) at step 14: 7.8014702796936035\n",
      "Training loss (for one batch) at step 16: 6.915647506713867\n",
      "Training loss (for one batch) at step 18: 7.237077236175537\n",
      "Training loss (for one batch) at step 20: 7.5611114501953125\n",
      "Training loss (for one batch) at step 22: 7.449138164520264\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 7.449264049530029\n",
      "Training loss (for one batch) at step 2: 7.9603729248046875\n",
      "Training loss (for one batch) at step 4: 7.593369483947754\n",
      "Training loss (for one batch) at step 6: 8.049983978271484\n",
      "Training loss (for one batch) at step 8: 8.335512161254883\n",
      "Training loss (for one batch) at step 10: 7.106813430786133\n",
      "Training loss (for one batch) at step 12: 7.531973838806152\n",
      "Training loss (for one batch) at step 14: 7.78934907913208\n",
      "Training loss (for one batch) at step 16: 6.900058746337891\n",
      "Training loss (for one batch) at step 18: 7.237307071685791\n",
      "Training loss (for one batch) at step 20: 7.573920249938965\n",
      "Training loss (for one batch) at step 22: 7.433103561401367\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 7.448184013366699\n",
      "Training loss (for one batch) at step 2: 7.957793235778809\n",
      "Training loss (for one batch) at step 4: 7.603902816772461\n",
      "Training loss (for one batch) at step 6: 8.032796859741211\n",
      "Training loss (for one batch) at step 8: 8.347257614135742\n",
      "Training loss (for one batch) at step 10: 7.116011619567871\n",
      "Training loss (for one batch) at step 12: 7.526670455932617\n",
      "Training loss (for one batch) at step 14: 7.81694221496582\n",
      "Training loss (for one batch) at step 16: 6.919861316680908\n",
      "Training loss (for one batch) at step 18: 7.235158443450928\n",
      "Training loss (for one batch) at step 20: 7.555019378662109\n",
      "Training loss (for one batch) at step 22: 7.455804824829102\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 7.449138641357422\n",
      "Training loss (for one batch) at step 2: 7.957696914672852\n",
      "Training loss (for one batch) at step 4: 7.582208156585693\n",
      "Training loss (for one batch) at step 6: 8.039146423339844\n",
      "Training loss (for one batch) at step 8: 8.35672378540039\n",
      "Training loss (for one batch) at step 10: 7.102675914764404\n",
      "Training loss (for one batch) at step 12: 7.53109884262085\n",
      "Training loss (for one batch) at step 14: 7.799182891845703\n",
      "Training loss (for one batch) at step 16: 6.919584274291992\n",
      "Training loss (for one batch) at step 18: 7.240442276000977\n",
      "Training loss (for one batch) at step 20: 7.561954975128174\n",
      "Training loss (for one batch) at step 22: 7.452550888061523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 7.449799537658691\n",
      "Training loss (for one batch) at step 2: 7.966331481933594\n",
      "Training loss (for one batch) at step 4: 7.599984645843506\n",
      "Training loss (for one batch) at step 6: 8.049132347106934\n",
      "Training loss (for one batch) at step 8: 8.332558631896973\n",
      "Training loss (for one batch) at step 10: 7.110859394073486\n",
      "Training loss (for one batch) at step 12: 7.529368877410889\n",
      "Training loss (for one batch) at step 14: 7.798802375793457\n",
      "Training loss (for one batch) at step 16: 6.912568092346191\n",
      "Training loss (for one batch) at step 18: 7.241021156311035\n",
      "Training loss (for one batch) at step 20: 7.5629682540893555\n",
      "Training loss (for one batch) at step 22: 7.449703216552734\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 7.4501824378967285\n",
      "Training loss (for one batch) at step 2: 7.969762325286865\n",
      "Training loss (for one batch) at step 4: 7.612244606018066\n",
      "Training loss (for one batch) at step 6: 8.042268753051758\n",
      "Training loss (for one batch) at step 8: 8.334501266479492\n",
      "Training loss (for one batch) at step 10: 7.113411903381348\n",
      "Training loss (for one batch) at step 12: 7.52718448638916\n",
      "Training loss (for one batch) at step 14: 7.804156303405762\n",
      "Training loss (for one batch) at step 16: 6.912702560424805\n",
      "Training loss (for one batch) at step 18: 7.239307880401611\n",
      "Training loss (for one batch) at step 20: 7.563431739807129\n",
      "Training loss (for one batch) at step 22: 7.4458770751953125\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 7.449618339538574\n",
      "Training loss (for one batch) at step 2: 7.963292598724365\n",
      "Training loss (for one batch) at step 4: 7.589916706085205\n",
      "Training loss (for one batch) at step 6: 8.045482635498047\n",
      "Training loss (for one batch) at step 8: 8.34525203704834\n",
      "Training loss (for one batch) at step 10: 7.104996681213379\n",
      "Training loss (for one batch) at step 12: 7.53132438659668\n",
      "Training loss (for one batch) at step 14: 7.793577194213867\n",
      "Training loss (for one batch) at step 16: 6.908108711242676\n",
      "Training loss (for one batch) at step 18: 7.244922637939453\n",
      "Training loss (for one batch) at step 20: 7.574939727783203\n",
      "Training loss (for one batch) at step 22: 7.434085845947266\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 7.448783874511719\n",
      "Training loss (for one batch) at step 2: 7.96173095703125\n",
      "Training loss (for one batch) at step 4: 7.59354829788208\n",
      "Training loss (for one batch) at step 6: 8.047536849975586\n",
      "Training loss (for one batch) at step 8: 8.33862018585205\n",
      "Training loss (for one batch) at step 10: 7.105088233947754\n",
      "Training loss (for one batch) at step 12: 7.532444953918457\n",
      "Training loss (for one batch) at step 14: 7.786602973937988\n",
      "Training loss (for one batch) at step 16: 6.898520469665527\n",
      "Training loss (for one batch) at step 18: 7.238466739654541\n",
      "Training loss (for one batch) at step 20: 7.570562362670898\n",
      "Training loss (for one batch) at step 22: 7.432927131652832\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 7.44852352142334\n",
      "Training loss (for one batch) at step 2: 7.957943439483643\n",
      "Training loss (for one batch) at step 4: 7.598477363586426\n",
      "Training loss (for one batch) at step 6: 8.032281875610352\n",
      "Training loss (for one batch) at step 8: 8.347143173217773\n",
      "Training loss (for one batch) at step 10: 7.116136074066162\n",
      "Training loss (for one batch) at step 12: 7.525673866271973\n",
      "Training loss (for one batch) at step 14: 7.816632270812988\n",
      "Training loss (for one batch) at step 16: 6.921205520629883\n",
      "Training loss (for one batch) at step 18: 7.237730979919434\n",
      "Training loss (for one batch) at step 20: 7.554224491119385\n",
      "Training loss (for one batch) at step 22: 7.453656196594238\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 7.449558258056641\n",
      "Training loss (for one batch) at step 2: 7.957269191741943\n",
      "Training loss (for one batch) at step 4: 7.5751051902771\n",
      "Training loss (for one batch) at step 6: 8.040544509887695\n",
      "Training loss (for one batch) at step 8: 8.355093002319336\n",
      "Training loss (for one batch) at step 10: 7.103466987609863\n",
      "Training loss (for one batch) at step 12: 7.53048038482666\n",
      "Training loss (for one batch) at step 14: 7.798835754394531\n",
      "Training loss (for one batch) at step 16: 6.919561862945557\n",
      "Training loss (for one batch) at step 18: 7.241928577423096\n",
      "Training loss (for one batch) at step 20: 7.56007719039917\n",
      "Training loss (for one batch) at step 22: 7.451812267303467\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 7.44944953918457\n",
      "Training loss (for one batch) at step 2: 7.958758354187012\n",
      "Training loss (for one batch) at step 4: 7.5872802734375\n",
      "Training loss (for one batch) at step 6: 8.035463333129883\n",
      "Training loss (for one batch) at step 8: 8.353909492492676\n",
      "Training loss (for one batch) at step 10: 7.1067070960998535\n",
      "Training loss (for one batch) at step 12: 7.528286933898926\n",
      "Training loss (for one batch) at step 14: 7.805508136749268\n",
      "Training loss (for one batch) at step 16: 6.921534538269043\n",
      "Training loss (for one batch) at step 18: 7.239758491516113\n",
      "Training loss (for one batch) at step 20: 7.562696933746338\n",
      "Training loss (for one batch) at step 22: 7.449801921844482\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 7.449863910675049\n",
      "Training loss (for one batch) at step 2: 7.967584609985352\n",
      "Training loss (for one batch) at step 4: 7.6066694259643555\n",
      "Training loss (for one batch) at step 6: 8.04686164855957\n",
      "Training loss (for one batch) at step 8: 8.327823638916016\n",
      "Training loss (for one batch) at step 10: 7.112478256225586\n",
      "Training loss (for one batch) at step 12: 7.530099868774414\n",
      "Training loss (for one batch) at step 14: 7.793703079223633\n",
      "Training loss (for one batch) at step 16: 6.903936386108398\n",
      "Training loss (for one batch) at step 18: 7.241950035095215\n",
      "Training loss (for one batch) at step 20: 7.575303554534912\n",
      "Training loss (for one batch) at step 22: 7.432188034057617\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 7.4485015869140625\n",
      "Training loss (for one batch) at step 2: 7.957574844360352\n",
      "Training loss (for one batch) at step 4: 7.598725318908691\n",
      "Training loss (for one batch) at step 6: 8.032257080078125\n",
      "Training loss (for one batch) at step 8: 8.34865951538086\n",
      "Training loss (for one batch) at step 10: 7.113539218902588\n",
      "Training loss (for one batch) at step 12: 7.525994300842285\n",
      "Training loss (for one batch) at step 14: 7.811408996582031\n",
      "Training loss (for one batch) at step 16: 6.913196563720703\n",
      "Training loss (for one batch) at step 18: 7.237945556640625\n",
      "Training loss (for one batch) at step 20: 7.561214447021484\n",
      "Training loss (for one batch) at step 22: 7.444214820861816\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 7.449706077575684\n",
      "Training loss (for one batch) at step 2: 7.960142135620117\n",
      "Training loss (for one batch) at step 4: 7.592622756958008\n",
      "Training loss (for one batch) at step 6: 8.040552139282227\n",
      "Training loss (for one batch) at step 8: 8.347570419311523\n",
      "Training loss (for one batch) at step 10: 7.105597496032715\n",
      "Training loss (for one batch) at step 12: 7.530078411102295\n",
      "Training loss (for one batch) at step 14: 7.7977752685546875\n",
      "Training loss (for one batch) at step 16: 6.916186332702637\n",
      "Training loss (for one batch) at step 18: 7.241242408752441\n",
      "Training loss (for one batch) at step 20: 7.563260078430176\n",
      "Training loss (for one batch) at step 22: 7.44503927230835\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 7.449605941772461\n",
      "Training loss (for one batch) at step 2: 7.962609767913818\n",
      "Training loss (for one batch) at step 4: 7.587357521057129\n",
      "Training loss (for one batch) at step 6: 8.045697212219238\n",
      "Training loss (for one batch) at step 8: 8.355321884155273\n",
      "Training loss (for one batch) at step 10: 7.103682041168213\n",
      "Training loss (for one batch) at step 12: 7.531982421875\n",
      "Training loss (for one batch) at step 14: 7.790318489074707\n",
      "Training loss (for one batch) at step 16: 6.905016899108887\n",
      "Training loss (for one batch) at step 18: 7.243105888366699\n",
      "Training loss (for one batch) at step 20: 7.571446895599365\n",
      "Training loss (for one batch) at step 22: 7.435351371765137\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 7.448604583740234\n",
      "Training loss (for one batch) at step 2: 7.956598281860352\n",
      "Training loss (for one batch) at step 4: 7.585606098175049\n",
      "Training loss (for one batch) at step 6: 8.034207344055176\n",
      "Training loss (for one batch) at step 8: 8.360467910766602\n",
      "Training loss (for one batch) at step 10: 7.0999627113342285\n",
      "Training loss (for one batch) at step 12: 7.532167434692383\n",
      "Training loss (for one batch) at step 14: 7.795914173126221\n",
      "Training loss (for one batch) at step 16: 6.915949821472168\n",
      "Training loss (for one batch) at step 18: 7.239652156829834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 20: 7.562649726867676\n",
      "Training loss (for one batch) at step 22: 7.450356960296631\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 7.449376106262207\n",
      "Training loss (for one batch) at step 2: 7.963282108306885\n",
      "Training loss (for one batch) at step 4: 7.590299606323242\n",
      "Training loss (for one batch) at step 6: 8.047698974609375\n",
      "Training loss (for one batch) at step 8: 8.340556144714355\n",
      "Training loss (for one batch) at step 10: 7.1055073738098145\n",
      "Training loss (for one batch) at step 12: 7.531401634216309\n",
      "Training loss (for one batch) at step 14: 7.794197082519531\n",
      "Training loss (for one batch) at step 16: 6.90949821472168\n",
      "Training loss (for one batch) at step 18: 7.239975452423096\n",
      "Training loss (for one batch) at step 20: 7.564706325531006\n",
      "Training loss (for one batch) at step 22: 7.447644233703613\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 7.449519157409668\n",
      "Training loss (for one batch) at step 2: 7.965912818908691\n",
      "Training loss (for one batch) at step 4: 7.600881576538086\n",
      "Training loss (for one batch) at step 6: 8.049543380737305\n",
      "Training loss (for one batch) at step 8: 8.329967498779297\n",
      "Training loss (for one batch) at step 10: 7.1084136962890625\n",
      "Training loss (for one batch) at step 12: 7.531367301940918\n",
      "Training loss (for one batch) at step 14: 7.79014778137207\n",
      "Training loss (for one batch) at step 16: 6.902552604675293\n",
      "Training loss (for one batch) at step 18: 7.239983558654785\n",
      "Training loss (for one batch) at step 20: 7.568116664886475\n",
      "Training loss (for one batch) at step 22: 7.4406962394714355\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 7.448816776275635\n",
      "Training loss (for one batch) at step 2: 7.958831787109375\n",
      "Training loss (for one batch) at step 4: 7.599183559417725\n",
      "Training loss (for one batch) at step 6: 8.033564567565918\n",
      "Training loss (for one batch) at step 8: 8.343402862548828\n",
      "Training loss (for one batch) at step 10: 7.116241455078125\n",
      "Training loss (for one batch) at step 12: 7.526103496551514\n",
      "Training loss (for one batch) at step 14: 7.816678524017334\n",
      "Training loss (for one batch) at step 16: 6.923487663269043\n",
      "Training loss (for one batch) at step 18: 7.237514495849609\n",
      "Training loss (for one batch) at step 20: 7.551886558532715\n",
      "Training loss (for one batch) at step 22: 7.463005065917969\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 7.450210094451904\n",
      "Training loss (for one batch) at step 2: 7.96382999420166\n",
      "Training loss (for one batch) at step 4: 7.581021308898926\n",
      "Training loss (for one batch) at step 6: 8.053672790527344\n",
      "Training loss (for one batch) at step 8: 8.335548400878906\n",
      "Training loss (for one batch) at step 10: 7.10041618347168\n",
      "Training loss (for one batch) at step 12: 7.537195682525635\n",
      "Training loss (for one batch) at step 14: 7.783804893493652\n",
      "Training loss (for one batch) at step 16: 6.906007766723633\n",
      "Training loss (for one batch) at step 18: 7.245211601257324\n",
      "Training loss (for one batch) at step 20: 7.570669174194336\n",
      "Training loss (for one batch) at step 22: 7.437705039978027\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 7.448722839355469\n",
      "Training loss (for one batch) at step 2: 7.958196640014648\n",
      "Training loss (for one batch) at step 4: 7.59308385848999\n",
      "Training loss (for one batch) at step 6: 8.034296989440918\n",
      "Training loss (for one batch) at step 8: 8.35244369506836\n",
      "Training loss (for one batch) at step 10: 7.106720924377441\n",
      "Training loss (for one batch) at step 12: 7.528229713439941\n",
      "Training loss (for one batch) at step 14: 7.80607795715332\n",
      "Training loss (for one batch) at step 16: 6.924571990966797\n",
      "Training loss (for one batch) at step 18: 7.239752769470215\n",
      "Training loss (for one batch) at step 20: 7.557597637176514\n",
      "Training loss (for one batch) at step 22: 7.455815315246582\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "feature_normalization_3 (Fea (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                60        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 1,951\n",
      "Trainable params: 1,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "reload(disc)\n",
    "discriminator = disc.QRDiscriminator(quantile=quantile, loss_strategy=strategy, epochs=10, n_nodes=30)\n",
    "discriminator.fit(x, y)\n",
    "#regressor = tegrta.QuantileRegressionV2() \n",
    "#model = regressor.make_model(x_mean_var=(np.mean(x), np.var(x)))\n",
    "print(discriminator.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.random.uniform([300, 1], minval=1, maxval=100, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 1), dtype=float32, numpy=\n",
       "array([[18.487127 ],\n",
       "       [16.861074 ],\n",
       "       [92.28928  ],\n",
       "       [96.92668  ],\n",
       "       [50.121624 ],\n",
       "       [33.68681  ],\n",
       "       [28.721767 ],\n",
       "       [40.233143 ],\n",
       "       [54.551426 ],\n",
       "       [97.98378  ],\n",
       "       [97.22862  ],\n",
       "       [63.698807 ],\n",
       "       [98.870125 ],\n",
       "       [20.362291 ],\n",
       "       [12.642586 ],\n",
       "       [77.735146 ],\n",
       "       [21.457336 ],\n",
       "       [ 5.1104445],\n",
       "       [89.220665 ],\n",
       "       [56.86901  ],\n",
       "       [74.38422  ],\n",
       "       [80.01396  ],\n",
       "       [ 2.852221 ],\n",
       "       [89.45097  ],\n",
       "       [84.226715 ],\n",
       "       [91.27842  ],\n",
       "       [41.463398 ],\n",
       "       [20.920607 ],\n",
       "       [33.423325 ],\n",
       "       [22.43203  ],\n",
       "       [12.715768 ],\n",
       "       [26.771852 ],\n",
       "       [76.642624 ],\n",
       "       [31.216616 ],\n",
       "       [42.329857 ],\n",
       "       [33.90782  ],\n",
       "       [13.357864 ],\n",
       "       [36.023785 ],\n",
       "       [23.12603  ],\n",
       "       [33.20478  ],\n",
       "       [44.853573 ],\n",
       "       [94.57649  ],\n",
       "       [ 3.6008983],\n",
       "       [43.379917 ],\n",
       "       [53.584095 ],\n",
       "       [58.926605 ],\n",
       "       [17.653631 ],\n",
       "       [93.686966 ],\n",
       "       [99.18541  ],\n",
       "       [95.67611  ],\n",
       "       [64.252686 ],\n",
       "       [12.094962 ],\n",
       "       [96.96888  ],\n",
       "       [63.55506  ],\n",
       "       [ 1.0199921],\n",
       "       [94.48725  ],\n",
       "       [24.235214 ],\n",
       "       [80.49349  ],\n",
       "       [ 3.4620395],\n",
       "       [68.429115 ],\n",
       "       [81.645164 ],\n",
       "       [ 8.502093 ],\n",
       "       [80.86202  ],\n",
       "       [95.33887  ],\n",
       "       [26.893244 ],\n",
       "       [32.125595 ],\n",
       "       [49.12962  ],\n",
       "       [99.73667  ],\n",
       "       [85.38717  ],\n",
       "       [ 4.426712 ],\n",
       "       [43.238934 ],\n",
       "       [34.987938 ],\n",
       "       [39.576435 ],\n",
       "       [92.08535  ],\n",
       "       [21.988651 ],\n",
       "       [88.28303  ],\n",
       "       [72.84593  ],\n",
       "       [79.59348  ],\n",
       "       [25.334238 ],\n",
       "       [49.795685 ],\n",
       "       [53.697746 ],\n",
       "       [94.47029  ],\n",
       "       [93.96419  ],\n",
       "       [21.768324 ],\n",
       "       [33.37468  ],\n",
       "       [34.670155 ],\n",
       "       [40.962193 ],\n",
       "       [35.516266 ],\n",
       "       [15.656744 ],\n",
       "       [29.077293 ],\n",
       "       [79.50695  ],\n",
       "       [97.49438  ],\n",
       "       [90.858826 ],\n",
       "       [49.01066  ],\n",
       "       [ 6.497914 ],\n",
       "       [78.25231  ],\n",
       "       [ 6.286887 ],\n",
       "       [32.248863 ],\n",
       "       [51.440773 ],\n",
       "       [26.633228 ],\n",
       "       [30.093388 ],\n",
       "       [11.505726 ],\n",
       "       [ 3.1732395],\n",
       "       [87.22976  ],\n",
       "       [93.18909  ],\n",
       "       [89.54887  ],\n",
       "       [60.27192  ],\n",
       "       [43.58956  ],\n",
       "       [18.20045  ],\n",
       "       [77.03397  ],\n",
       "       [55.521065 ],\n",
       "       [46.06575  ],\n",
       "       [14.857237 ],\n",
       "       [33.51872  ],\n",
       "       [78.78865  ],\n",
       "       [49.790836 ],\n",
       "       [60.986313 ],\n",
       "       [45.79803  ],\n",
       "       [25.436028 ],\n",
       "       [71.93756  ],\n",
       "       [64.5587   ],\n",
       "       [51.013966 ],\n",
       "       [58.612583 ],\n",
       "       [88.48044  ],\n",
       "       [59.223053 ],\n",
       "       [64.432556 ],\n",
       "       [35.238277 ],\n",
       "       [47.021988 ],\n",
       "       [42.44783  ],\n",
       "       [74.13163  ],\n",
       "       [87.046524 ],\n",
       "       [41.61261  ],\n",
       "       [11.953601 ],\n",
       "       [68.80776  ],\n",
       "       [31.202135 ],\n",
       "       [84.144936 ],\n",
       "       [92.420425 ],\n",
       "       [20.565693 ],\n",
       "       [28.74379  ],\n",
       "       [29.906212 ],\n",
       "       [ 9.166908 ],\n",
       "       [63.819702 ],\n",
       "       [ 6.278024 ],\n",
       "       [80.89001  ],\n",
       "       [75.95813  ],\n",
       "       [59.781685 ],\n",
       "       [88.57203  ],\n",
       "       [12.068455 ],\n",
       "       [59.11679  ],\n",
       "       [37.03401  ],\n",
       "       [51.12043  ],\n",
       "       [44.155632 ],\n",
       "       [13.037506 ],\n",
       "       [11.385206 ],\n",
       "       [10.133103 ],\n",
       "       [94.349976 ],\n",
       "       [14.109539 ],\n",
       "       [93.530815 ],\n",
       "       [88.62907  ],\n",
       "       [11.432602 ],\n",
       "       [88.57868  ],\n",
       "       [62.658756 ],\n",
       "       [97.67924  ],\n",
       "       [43.66303  ],\n",
       "       [52.367184 ],\n",
       "       [66.37489  ],\n",
       "       [14.922264 ],\n",
       "       [44.063225 ],\n",
       "       [33.67169  ],\n",
       "       [27.898138 ],\n",
       "       [51.40184  ],\n",
       "       [22.861977 ],\n",
       "       [94.44249  ],\n",
       "       [49.760506 ],\n",
       "       [19.819046 ],\n",
       "       [60.875023 ],\n",
       "       [43.817738 ],\n",
       "       [57.617924 ],\n",
       "       [51.8006   ],\n",
       "       [ 8.495095 ],\n",
       "       [91.10454  ],\n",
       "       [50.985783 ],\n",
       "       [23.197582 ],\n",
       "       [12.188797 ],\n",
       "       [28.46737  ],\n",
       "       [33.14111  ],\n",
       "       [14.013237 ],\n",
       "       [98.92153  ],\n",
       "       [17.000525 ],\n",
       "       [29.538221 ],\n",
       "       [85.144196 ],\n",
       "       [95.52532  ],\n",
       "       [31.60219  ],\n",
       "       [14.095825 ],\n",
       "       [ 5.0961647],\n",
       "       [84.1195   ],\n",
       "       [62.41656  ],\n",
       "       [97.98461  ],\n",
       "       [67.21121  ],\n",
       "       [14.654471 ],\n",
       "       [52.39602  ],\n",
       "       [46.818596 ],\n",
       "       [59.692997 ],\n",
       "       [93.00924  ],\n",
       "       [35.82534  ],\n",
       "       [39.46327  ],\n",
       "       [59.090603 ],\n",
       "       [47.33787  ],\n",
       "       [ 3.2880466],\n",
       "       [60.467506 ],\n",
       "       [81.3724   ],\n",
       "       [66.60739  ],\n",
       "       [27.74966  ],\n",
       "       [59.919674 ],\n",
       "       [93.610405 ],\n",
       "       [ 3.6642618],\n",
       "       [36.72188  ],\n",
       "       [49.917645 ],\n",
       "       [13.088985 ],\n",
       "       [ 9.068812 ],\n",
       "       [25.541004 ],\n",
       "       [12.166811 ],\n",
       "       [57.872993 ],\n",
       "       [56.75837  ],\n",
       "       [73.59757  ],\n",
       "       [81.50954  ],\n",
       "       [43.36985  ],\n",
       "       [ 5.678792 ],\n",
       "       [97.796005 ],\n",
       "       [48.63462  ],\n",
       "       [34.4055   ],\n",
       "       [58.678593 ],\n",
       "       [20.09261  ],\n",
       "       [32.25998  ],\n",
       "       [84.66339  ],\n",
       "       [97.23873  ],\n",
       "       [52.443695 ],\n",
       "       [91.10512  ],\n",
       "       [20.350395 ],\n",
       "       [ 1.1366521],\n",
       "       [41.86112  ],\n",
       "       [ 6.330046 ],\n",
       "       [49.084087 ],\n",
       "       [94.54187  ],\n",
       "       [21.682985 ],\n",
       "       [91.45431  ],\n",
       "       [16.616425 ],\n",
       "       [23.53981  ],\n",
       "       [45.390255 ],\n",
       "       [34.99823  ],\n",
       "       [ 8.173463 ],\n",
       "       [92.762726 ],\n",
       "       [67.97419  ],\n",
       "       [13.928843 ],\n",
       "       [73.01847  ],\n",
       "       [15.926767 ],\n",
       "       [81.22945  ],\n",
       "       [85.66821  ],\n",
       "       [93.91023  ],\n",
       "       [57.266693 ],\n",
       "       [ 8.990095 ],\n",
       "       [63.307747 ],\n",
       "       [66.49297  ],\n",
       "       [45.29235  ],\n",
       "       [27.710182 ],\n",
       "       [49.429974 ],\n",
       "       [61.869823 ],\n",
       "       [ 9.315952 ],\n",
       "       [ 1.1907866],\n",
       "       [20.084915 ],\n",
       "       [15.200773 ],\n",
       "       [ 9.319068 ],\n",
       "       [12.254734 ],\n",
       "       [70.54854  ],\n",
       "       [30.563932 ],\n",
       "       [53.22379  ],\n",
       "       [28.771606 ],\n",
       "       [65.611336 ],\n",
       "       [70.019844 ],\n",
       "       [35.784943 ],\n",
       "       [ 4.842864 ],\n",
       "       [35.34167  ],\n",
       "       [22.151396 ],\n",
       "       [ 4.0990844],\n",
       "       [15.433244 ],\n",
       "       [62.211544 ],\n",
       "       [12.073648 ],\n",
       "       [ 9.44223  ],\n",
       "       [28.066677 ],\n",
       "       [42.874496 ],\n",
       "       [90.060135 ],\n",
       "       [56.47266  ],\n",
       "       [85.74033  ],\n",
       "       [11.3685665],\n",
       "       [65.34457  ],\n",
       "       [ 5.24679  ],\n",
       "       [49.908096 ],\n",
       "       [84.74257  ],\n",
       "       [13.12878  ],\n",
       "       [50.492493 ]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = discriminator.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49534723],\n",
       "       [0.49543893],\n",
       "       [0.5177956 ],\n",
       "       [0.5150038 ],\n",
       "       [0.50528795],\n",
       "       [0.5119459 ],\n",
       "       [0.5148262 ],\n",
       "       [0.50412494],\n",
       "       [0.49396366],\n",
       "       [0.49425113],\n",
       "       [0.49432507],\n",
       "       [0.5104157 ],\n",
       "       [0.51002014],\n",
       "       [0.49757567],\n",
       "       [0.4959273 ],\n",
       "       [0.51492584],\n",
       "       [0.5120544 ],\n",
       "       [0.5072611 ],\n",
       "       [0.49691176],\n",
       "       [0.5027422 ],\n",
       "       [0.5035956 ],\n",
       "       [0.49532726],\n",
       "       [0.49896654],\n",
       "       [0.51544756],\n",
       "       [0.49678046],\n",
       "       [0.51170695],\n",
       "       [0.50514245],\n",
       "       [0.49750474],\n",
       "       [0.49870756],\n",
       "       [0.50688076],\n",
       "       [0.49390173],\n",
       "       [0.4954791 ],\n",
       "       [0.49583963],\n",
       "       [0.4950637 ],\n",
       "       [0.4950526 ],\n",
       "       [0.49715355],\n",
       "       [0.4964343 ],\n",
       "       [0.49631926],\n",
       "       [0.50843173],\n",
       "       [0.5025207 ],\n",
       "       [0.4958775 ],\n",
       "       [0.49552667],\n",
       "       [0.51316285],\n",
       "       [0.49467617],\n",
       "       [0.5092292 ],\n",
       "       [0.50852525],\n",
       "       [0.49821994],\n",
       "       [0.49788722],\n",
       "       [0.49733716],\n",
       "       [0.5110395 ],\n",
       "       [0.4990447 ],\n",
       "       [0.49646908],\n",
       "       [0.4978291 ],\n",
       "       [0.5085666 ],\n",
       "       [0.49498254],\n",
       "       [0.51734793],\n",
       "       [0.49496728],\n",
       "       [0.5127925 ],\n",
       "       [0.49762136],\n",
       "       [0.4956115 ],\n",
       "       [0.50662726],\n",
       "       [0.5009431 ],\n",
       "       [0.5145014 ],\n",
       "       [0.51199996],\n",
       "       [0.51677   ],\n",
       "       [0.49835983],\n",
       "       [0.5044768 ],\n",
       "       [0.49503282],\n",
       "       [0.5080805 ],\n",
       "       [0.5029614 ],\n",
       "       [0.51567066],\n",
       "       [0.49390972],\n",
       "       [0.49908984],\n",
       "       [0.49546182],\n",
       "       [0.4991562 ],\n",
       "       [0.5109104 ],\n",
       "       [0.49456063],\n",
       "       [0.49508625],\n",
       "       [0.5074309 ],\n",
       "       [0.50694406],\n",
       "       [0.5029577 ],\n",
       "       [0.49675635],\n",
       "       [0.498076  ],\n",
       "       [0.510368  ],\n",
       "       [0.4960112 ],\n",
       "       [0.49509913],\n",
       "       [0.49483252],\n",
       "       [0.49488726],\n",
       "       [0.5020275 ],\n",
       "       [0.49533036],\n",
       "       [0.4942272 ],\n",
       "       [0.497266  ],\n",
       "       [0.49571362],\n",
       "       [0.506397  ],\n",
       "       [0.49728933],\n",
       "       [0.50350267],\n",
       "       [0.49636978],\n",
       "       [0.51345146],\n",
       "       [0.49826437],\n",
       "       [0.5066673 ],\n",
       "       [0.49592236],\n",
       "       [0.49583918],\n",
       "       [0.50435215],\n",
       "       [0.4951033 ],\n",
       "       [0.49750745],\n",
       "       [0.49654967],\n",
       "       [0.49760178],\n",
       "       [0.4963169 ],\n",
       "       [0.49437788],\n",
       "       [0.50536495],\n",
       "       [0.51255137],\n",
       "       [0.5162513 ],\n",
       "       [0.5144544 ],\n",
       "       [0.49747327],\n",
       "       [0.49616006],\n",
       "       [0.51286405],\n",
       "       [0.5132679 ],\n",
       "       [0.5128681 ],\n",
       "       [0.49819118],\n",
       "       [0.50837296],\n",
       "       [0.50452524],\n",
       "       [0.49430692],\n",
       "       [0.49897456],\n",
       "       [0.5034587 ],\n",
       "       [0.49496526],\n",
       "       [0.49632022],\n",
       "       [0.49708796],\n",
       "       [0.5059419 ],\n",
       "       [0.5003342 ],\n",
       "       [0.49656653],\n",
       "       [0.49753284],\n",
       "       [0.49527413],\n",
       "       [0.4994087 ],\n",
       "       [0.4971297 ],\n",
       "       [0.49527264],\n",
       "       [0.49756214],\n",
       "       [0.5149555 ],\n",
       "       [0.5176688 ],\n",
       "       [0.49571818],\n",
       "       [0.51232016],\n",
       "       [0.49876773],\n",
       "       [0.49483547],\n",
       "       [0.5162764 ],\n",
       "       [0.4965119 ],\n",
       "       [0.49512488],\n",
       "       [0.5077035 ],\n",
       "       [0.4973367 ],\n",
       "       [0.5090681 ],\n",
       "       [0.49778882],\n",
       "       [0.5166997 ],\n",
       "       [0.5101023 ],\n",
       "       [0.49481037],\n",
       "       [0.49478042],\n",
       "       [0.5090556 ],\n",
       "       [0.51374483],\n",
       "       [0.5064465 ],\n",
       "       [0.51664275],\n",
       "       [0.4956952 ],\n",
       "       [0.49481001],\n",
       "       [0.49516064],\n",
       "       [0.5091123 ],\n",
       "       [0.5165809 ],\n",
       "       [0.5142105 ],\n",
       "       [0.49644083],\n",
       "       [0.4979479 ],\n",
       "       [0.5123549 ],\n",
       "       [0.5149906 ],\n",
       "       [0.49628398],\n",
       "       [0.49633735],\n",
       "       [0.510672  ],\n",
       "       [0.50966334],\n",
       "       [0.51285046],\n",
       "       [0.4979999 ],\n",
       "       [0.5048277 ],\n",
       "       [0.49737012],\n",
       "       [0.5045995 ],\n",
       "       [0.49426335],\n",
       "       [0.5112481 ],\n",
       "       [0.50240797],\n",
       "       [0.5056367 ],\n",
       "       [0.51150966],\n",
       "       [0.49816835],\n",
       "       [0.51618993],\n",
       "       [0.49429578],\n",
       "       [0.5043874 ],\n",
       "       [0.51533246],\n",
       "       [0.50134224],\n",
       "       [0.50598645],\n",
       "       [0.51257366],\n",
       "       [0.49714872],\n",
       "       [0.49613264],\n",
       "       [0.49982527],\n",
       "       [0.49792114],\n",
       "       [0.49445316],\n",
       "       [0.50757617],\n",
       "       [0.4947481 ],\n",
       "       [0.5108817 ],\n",
       "       [0.5141953 ],\n",
       "       [0.5074984 ],\n",
       "       [0.49700177],\n",
       "       [0.4960154 ],\n",
       "       [0.5117992 ],\n",
       "       [0.49616048],\n",
       "       [0.495855  ],\n",
       "       [0.4941293 ],\n",
       "       [0.5104388 ],\n",
       "       [0.50224483],\n",
       "       [0.5150083 ],\n",
       "       [0.4955039 ],\n",
       "       [0.5075337 ],\n",
       "       [0.49836144],\n",
       "       [0.49469075],\n",
       "       [0.50250846],\n",
       "       [0.4943451 ],\n",
       "       [0.49412844],\n",
       "       [0.49622425],\n",
       "       [0.49427253],\n",
       "       [0.49629876],\n",
       "       [0.5108769 ],\n",
       "       [0.51788205],\n",
       "       [0.49558005],\n",
       "       [0.5150588 ],\n",
       "       [0.51241004],\n",
       "       [0.49441668],\n",
       "       [0.49842367],\n",
       "       [0.49457046],\n",
       "       [0.5130218 ],\n",
       "       [0.5052394 ],\n",
       "       [0.4985177 ],\n",
       "       [0.49740443],\n",
       "       [0.50939685],\n",
       "       [0.49966013],\n",
       "       [0.49778914],\n",
       "       [0.5039452 ],\n",
       "       [0.49567768],\n",
       "       [0.50513047],\n",
       "       [0.5152887 ],\n",
       "       [0.5144961 ],\n",
       "       [0.5093023 ],\n",
       "       [0.5055405 ],\n",
       "       [0.49462315],\n",
       "       [0.4966493 ],\n",
       "       [0.49578762],\n",
       "       [0.4942389 ],\n",
       "       [0.4948212 ],\n",
       "       [0.49846286],\n",
       "       [0.509408  ],\n",
       "       [0.4960459 ],\n",
       "       [0.4941343 ],\n",
       "       [0.4950155 ],\n",
       "       [0.49812025],\n",
       "       [0.49520716],\n",
       "       [0.5115999 ],\n",
       "       [0.49396983],\n",
       "       [0.49740398],\n",
       "       [0.5084422 ],\n",
       "       [0.49598992],\n",
       "       [0.49415693],\n",
       "       [0.50649595],\n",
       "       [0.49415886],\n",
       "       [0.4962283 ],\n",
       "       [0.49992195],\n",
       "       [0.49891394],\n",
       "       [0.49674714],\n",
       "       [0.5134104 ],\n",
       "       [0.49574402],\n",
       "       [0.49687123],\n",
       "       [0.4952877 ],\n",
       "       [0.5150642 ],\n",
       "       [0.5149376 ],\n",
       "       [0.5158479 ],\n",
       "       [0.49410313],\n",
       "       [0.4952294 ],\n",
       "       [0.4976775 ],\n",
       "       [0.49479035],\n",
       "       [0.50197196],\n",
       "       [0.5118046 ],\n",
       "       [0.5015521 ],\n",
       "       [0.5153012 ],\n",
       "       [0.49419066],\n",
       "       [0.49469915],\n",
       "       [0.5157749 ],\n",
       "       [0.4949511 ],\n",
       "       [0.50083303],\n",
       "       [0.49737123],\n",
       "       [0.49463224],\n",
       "       [0.5046884 ],\n",
       "       [0.5132813 ],\n",
       "       [0.49593332],\n",
       "       [0.49512693],\n",
       "       [0.50261515],\n",
       "       [0.49505204],\n",
       "       [0.512954  ],\n",
       "       [0.49570507],\n",
       "       [0.49456185],\n",
       "       [0.5073745 ],\n",
       "       [0.50835073],\n",
       "       [0.5081634 ],\n",
       "       [0.49395022],\n",
       "       [0.49441794]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reload(tegrta)\n",
    "loaded_model = tf.keras.models.load_model('./my_model.h5', custom_objects={'FeatureNormalization': tegrta.FeatureNormalization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weights = loaded_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    assert np.allclose(weights[0], loaded_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(y, y_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
