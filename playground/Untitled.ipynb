{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 2. 3. 4. 5. 6. 7. 8. 9.], shape=(10,), dtype=float32)\n",
      "tf.Tensor([ 0.  2.  4.  6.  8. 10. 12. 14. 16. 18.], shape=(10,), dtype=float32)\n",
      "tf.Tensor([ 1.  3.  5.  7.  9. 11. 13. 15. 17. 19.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t = tf.range(0., 10.)\n",
    "print(t)\n",
    "t = t * 2\n",
    "print(t)\n",
    "t = t + 1\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new graph created <dtype: 'float32'> (10,)\n",
      "Tensor(\"mul:0\", shape=(10,), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(10,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([ 1.,  3.,  5.,  7.,  9., 11., 13., 15., 17., 19.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tf.function\n",
    "def my_func(t):\n",
    "    print(\"new graph created\", t.dtype, t.shape)\n",
    "    t = t * 2\n",
    "    print(t)\n",
    "    t = t + 1\n",
    "    print(t)\n",
    "    return t\n",
    "\n",
    "t = tf.range(0., 10.)\n",
    "my_func(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "print(t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([21., 23., 25., 27., 29., 31., 33., 35., 37., 39.], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(tf.range(10., 20.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new graph created <dtype: 'float32'> (11,)\n",
      "Tensor(\"mul:0\", shape=(11,), dtype=float32)\n",
      "Tensor(\"add:0\", shape=(11,), dtype=float32)\n",
      "---\n",
      "new graph created <dtype: 'int32'> (10,)\n",
      "Tensor(\"mul:0\", shape=(10,), dtype=int32)\n",
      "Tensor(\"add:0\", shape=(10,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([21, 23, 25, 27, 29, 31, 33, 35, 37, 39], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(tf.range(10., 21.))\n",
    "print(\"---\")\n",
    "my_func(tf.range(10, 20, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dadrah.playground.test_gradient_tape as tegrta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([3000, 1], minval=1, maxval=100, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "feature_normalization_1 (Fea (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                40        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 901\n",
      "Trainable params: 901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(tegrta)\n",
    "model = tegrta.make_model(x_mean_var=(np.mean(x), np.var(x)))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.random.uniform([3000,1], minval=1, maxval=10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_loss = tegrta.make_quantile_loss(np.mean(y), np.var(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 18.58209991455078\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 21.166526794433594\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 15.412264823913574\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 12.009082794189453\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 11.298194885253906\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 8.151603698730469\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.299548149108887\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 8.664018630981445\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 8.327523231506348\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 8.19874382019043\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.93535041809082\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.600473403930664\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 8.00317668914795\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 8.164216995239258\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.845913887023926\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.031431198120117\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 8.110984802246094\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.225831985473633\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.2978596687316895\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.6826863288879395\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.237705230712891\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.359308242797852\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.6733598709106445\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.430184364318848\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 8.165336608886719\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.521449089050293\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.790353298187256\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.105830192565918\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.794589996337891\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.097273826599121\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.288881301879883\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.645225524902344\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.394416809082031\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.536770820617676\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.770479202270508\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.337316513061523\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 8.147562026977539\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.481180191040039\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.857869625091553\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.212579727172852\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.700466156005859\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.013670921325684\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.2577924728393555\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.655976295471191\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.3992204666137695\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.616457939147949\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.819148063659668\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.3195037841796875\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 8.089365005493164\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.5163254737854\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.837155818939209\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.196325302124023\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.709088325500488\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.020339488983154\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.261227130889893\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.653085231781006\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.399724960327148\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.602132797241211\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.80106258392334\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.331185340881348\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 8.141927719116211\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.482736587524414\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.856442451477051\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.206437110900879\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.713412284851074\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.034045219421387\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.269920349121094\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.648265361785889\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.418216228485107\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.597263813018799\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.7807536125183105\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.347932815551758\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 8.188859939575195\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.4650115966796875\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.853288650512695\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.166845321655273\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.770917892456055\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.099996566772461\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.2941718101501465\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.6423797607421875\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.427803993225098\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.566145896911621\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.7556657791137695\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.375407695770264\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 8.203275680541992\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.465203285217285\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.844427108764648\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.152237892150879\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.791828155517578\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.120501518249512\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.298768997192383\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642502307891846\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.419398784637451\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.540414810180664\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.7463507652282715\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.385136127471924\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 8.199562072753906\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.472777843475342\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.8391008377075195\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.14919662475586\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.792291641235352\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.11846923828125\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.297391891479492\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642831802368164\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.408435344696045\n",
      "Seen so far: 544 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 18: 7.5203375816345215\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.740170001983643\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.392614364624023\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 8.204527854919434\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.47432804107666\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.837993144989014\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.14365005493164\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.800989151000977\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.121391296386719\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.294346332550049\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.644660949707031\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.383437156677246\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.498575210571289\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.738271713256836\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.384259223937988\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 8.198989868164062\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.479489326477051\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.834142684936523\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.137157440185547\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.808963775634766\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.1299896240234375\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.298525810241699\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642786026000977\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.391021728515625\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.495527267456055\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.731294631958008\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.404047966003418\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 8.20758056640625\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.483438968658447\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.830219745635986\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.130465507507324\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.816995620727539\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.138622283935547\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.30296516418457\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.640548229217529\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.39996337890625\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.493160247802734\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.727115631103516\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.409977912902832\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 8.2105131149292\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.4846038818359375\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.828545570373535\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.126099586486816\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.823243141174316\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.146292209625244\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.306488037109375\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.639081001281738\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.413699626922607\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.50764274597168\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.734098434448242\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.406062126159668\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 8.214912414550781\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.4798712730407715\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.831588268280029\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.12545394897461\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.827218055725098\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.144721984863281\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.3043532371521\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.639929294586182\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.397595405578613\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.492093086242676\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.73016357421875\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.397811412811279\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 8.201717376708984\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.487251281738281\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.82953405380249\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.130117416381836\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.81697416305542\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.138581275939941\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.302072525024414\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.640998363494873\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.39682149887085\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.489400863647461\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.724302291870117\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.418007850646973\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 8.218225479125977\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.479715347290039\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.827081203460693\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.11800765991211\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.839749813079834\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.157269477844238\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.307653427124023\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.640327453613281\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.396389007568359\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.488640308380127\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.725555419921875\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.411354064941406\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 8.209219932556152\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.48415470123291\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.825915813446045\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.123502731323242\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.827317714691162\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.1431660652160645\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.301236152648926\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642441749572754\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.386918067932129\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.484499931335449\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.729388236999512\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.391143321990967\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 8.188409805297852\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.491125106811523\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.825456619262695\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.131874084472656\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.8129119873046875\n",
      "Seen so far: 288 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 10: 7.132172584533691\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.296083450317383\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.645163536071777\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.38758659362793\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.503411293029785\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.737621307373047\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.391625881195068\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 8.197528839111328\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.48165225982666\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.831361770629883\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.13746166229248\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.808472156524658\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.128853797912598\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.295529365539551\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.644866943359375\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.387068271636963\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.50242280960083\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.738081932067871\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.388947010040283\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 8.195098876953125\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.485175132751465\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.829607963562012\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.134714126586914\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.810171127319336\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.129998207092285\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.2967119216918945\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643985748291016\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.383953094482422\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.494167804718018\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.734899520874023\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.39149284362793\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 8.196113586425781\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.485279083251953\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.829373359680176\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.134265899658203\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.81013298034668\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.129894256591797\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.296687126159668\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643894195556641\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.384209632873535\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.4947614669799805\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.735732078552246\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.388867378234863\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 8.193347930908203\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.483527660369873\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.836831092834473\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.149696350097656\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.78893518447876\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.113178730010986\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.293754577636719\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.644092559814453\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.396030426025391\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.511172294616699\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.736918926239014\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.396882057189941\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 8.207197189331055\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.475003719329834\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.837459087371826\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.141489028930664\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.80388879776001\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.125096321105957\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.295845031738281\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643802165985107\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.3836517333984375\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.495189666748047\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.737685680389404\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.382637023925781\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 8.195286750793457\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.481722831726074\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.831255912780762\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.13489055633545\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.80876350402832\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.128852844238281\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.296841144561768\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.6437668800354\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.386682510375977\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.493529319763184\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.731215476989746\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.399361610412598\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 8.205785751342773\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.477538108825684\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.8344221115112305\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.137408256530762\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.806458950042725\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.1270599365234375\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.296665191650391\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643649101257324\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.387306213378906\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.494961738586426\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.733142375946045\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.393742084503174\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 8.199712753295898\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.48078727722168\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.830605506896973\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.133353233337402\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.8092145919799805\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.128885269165039\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.297672271728516\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643209934234619\n",
      "Seen so far: 480 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 16: 7.389687538146973\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.497355937957764\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.73539924621582\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.387892246246338\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 8.203187942504883\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.474992752075195\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.841770648956299\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.151445388793945\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.786258697509766\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.110982894897461\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.294792652130127\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.643054485321045\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.396902084350586\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.504709243774414\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.739906311035156\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.380237579345703\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 8.194490432739258\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.478728771209717\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.8366827964782715\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.146455764770508\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.790061950683594\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.113635063171387\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.29556941986084\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642992973327637\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.398676872253418\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.507360935211182\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.73588752746582\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.39247989654541\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 8.20197868347168\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.476690292358398\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.838354587554932\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.147174835205078\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.788983345031738\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.112676620483398\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.29596471786499\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.6425676345825195\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.400002479553223\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.5090742111206055\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.738409042358398\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.385522842407227\n",
      "Seen so far: 736 samples\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 8.203248977661133\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 2: 7.472740173339844\n",
      "Seen so far: 96 samples\n",
      "Training loss (for one batch) at step 4: 7.841511249542236\n",
      "Seen so far: 160 samples\n",
      "Training loss (for one batch) at step 6: 8.149273872375488\n",
      "Seen so far: 224 samples\n",
      "Training loss (for one batch) at step 8: 7.787676811218262\n",
      "Seen so far: 288 samples\n",
      "Training loss (for one batch) at step 10: 7.111739158630371\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 12: 7.297238349914551\n",
      "Seen so far: 416 samples\n",
      "Training loss (for one batch) at step 14: 7.642084121704102\n",
      "Seen so far: 480 samples\n",
      "Training loss (for one batch) at step 16: 7.402919769287109\n",
      "Seen so far: 544 samples\n",
      "Training loss (for one batch) at step 18: 7.512646675109863\n",
      "Seen so far: 608 samples\n",
      "Training loss (for one batch) at step 20: 7.741361618041992\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 22: 7.377850532531738\n",
      "Seen so far: 736 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x_batch_train, training=True)\n",
    "            loss_value = quantile_loss(y_batch_train, predictions)\n",
    "        \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        if step % 2 == 0:\n",
    "            print(\"Training loss (for one batch) at step {}: {}\".format(step, np.sum(loss_value)))\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.random.uniform([300, 1], minval=1, maxval=100, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 1), dtype=float32, numpy=\n",
       "array([[23.52121  ],\n",
       "       [ 5.6895785],\n",
       "       [40.299137 ],\n",
       "       [24.915867 ],\n",
       "       [11.131139 ],\n",
       "       [42.03452  ],\n",
       "       [72.43971  ],\n",
       "       [ 8.1663   ],\n",
       "       [ 5.486365 ],\n",
       "       [99.60236  ],\n",
       "       [ 5.014969 ],\n",
       "       [89.82392  ],\n",
       "       [81.182976 ],\n",
       "       [62.763287 ],\n",
       "       [34.006424 ],\n",
       "       [ 5.039127 ],\n",
       "       [14.0648575],\n",
       "       [97.17816  ],\n",
       "       [98.78617  ],\n",
       "       [ 9.456794 ],\n",
       "       [75.49267  ],\n",
       "       [13.146813 ],\n",
       "       [ 6.9719534],\n",
       "       [10.219362 ],\n",
       "       [ 9.137392 ],\n",
       "       [44.76474  ],\n",
       "       [52.38588  ],\n",
       "       [57.040207 ],\n",
       "       [46.433693 ],\n",
       "       [28.694235 ],\n",
       "       [62.02917  ],\n",
       "       [85.37827  ],\n",
       "       [44.421913 ],\n",
       "       [57.02465  ],\n",
       "       [61.623287 ],\n",
       "       [92.22365  ],\n",
       "       [74.09047  ],\n",
       "       [12.265213 ],\n",
       "       [28.42827  ],\n",
       "       [39.809566 ],\n",
       "       [36.44872  ],\n",
       "       [62.145725 ],\n",
       "       [74.97686  ],\n",
       "       [22.868351 ],\n",
       "       [91.095474 ],\n",
       "       [ 1.6071513],\n",
       "       [99.408134 ],\n",
       "       [92.10622  ],\n",
       "       [77.4273   ],\n",
       "       [19.830683 ],\n",
       "       [85.880615 ],\n",
       "       [18.930635 ],\n",
       "       [23.995344 ],\n",
       "       [97.082954 ],\n",
       "       [75.100746 ],\n",
       "       [75.24922  ],\n",
       "       [86.99873  ],\n",
       "       [ 8.949909 ],\n",
       "       [34.290585 ],\n",
       "       [32.122433 ],\n",
       "       [56.88617  ],\n",
       "       [36.32551  ],\n",
       "       [18.047821 ],\n",
       "       [15.610222 ],\n",
       "       [44.52721  ],\n",
       "       [53.97787  ],\n",
       "       [99.39704  ],\n",
       "       [60.171402 ],\n",
       "       [68.476814 ],\n",
       "       [84.794945 ],\n",
       "       [ 9.080507 ],\n",
       "       [38.170826 ],\n",
       "       [43.52352  ],\n",
       "       [30.181852 ],\n",
       "       [92.041084 ],\n",
       "       [87.15336  ],\n",
       "       [14.854097 ],\n",
       "       [43.78475  ],\n",
       "       [47.781593 ],\n",
       "       [28.50602  ],\n",
       "       [44.741302 ],\n",
       "       [34.192844 ],\n",
       "       [65.42007  ],\n",
       "       [29.184631 ],\n",
       "       [37.66805  ],\n",
       "       [37.02436  ],\n",
       "       [79.67652  ],\n",
       "       [34.810593 ],\n",
       "       [94.873215 ],\n",
       "       [55.21173  ],\n",
       "       [20.066729 ],\n",
       "       [44.588177 ],\n",
       "       [22.911392 ],\n",
       "       [69.19665  ],\n",
       "       [88.52266  ],\n",
       "       [73.89652  ],\n",
       "       [66.08849  ],\n",
       "       [85.07289  ],\n",
       "       [82.14813  ],\n",
       "       [19.732859 ],\n",
       "       [38.78173  ],\n",
       "       [60.46537  ],\n",
       "       [82.865166 ],\n",
       "       [27.218227 ],\n",
       "       [20.374884 ],\n",
       "       [57.162663 ],\n",
       "       [29.16773  ],\n",
       "       [80.060356 ],\n",
       "       [29.659874 ],\n",
       "       [21.52411  ],\n",
       "       [97.10846  ],\n",
       "       [71.5197   ],\n",
       "       [58.99954  ],\n",
       "       [ 9.339449 ],\n",
       "       [71.660385 ],\n",
       "       [66.29798  ],\n",
       "       [95.25827  ],\n",
       "       [56.661713 ],\n",
       "       [75.863075 ],\n",
       "       [71.37698  ],\n",
       "       [71.947334 ],\n",
       "       [35.60276  ],\n",
       "       [61.7027   ],\n",
       "       [19.840914 ],\n",
       "       [70.898056 ],\n",
       "       [27.663767 ],\n",
       "       [52.01364  ],\n",
       "       [75.23481  ],\n",
       "       [41.65435  ],\n",
       "       [47.92778  ],\n",
       "       [94.14907  ],\n",
       "       [81.68798  ],\n",
       "       [38.999992 ],\n",
       "       [12.006826 ],\n",
       "       [66.19017  ],\n",
       "       [37.105167 ],\n",
       "       [97.68657  ],\n",
       "       [ 8.202413 ],\n",
       "       [99.123695 ],\n",
       "       [41.632835 ],\n",
       "       [19.400427 ],\n",
       "       [51.411823 ],\n",
       "       [54.311943 ],\n",
       "       [99.68957  ],\n",
       "       [51.81018  ],\n",
       "       [29.940035 ],\n",
       "       [48.02406  ],\n",
       "       [82.73452  ],\n",
       "       [69.477806 ],\n",
       "       [61.01166  ],\n",
       "       [42.68227  ],\n",
       "       [46.951977 ],\n",
       "       [74.27225  ],\n",
       "       [80.776566 ],\n",
       "       [52.228714 ],\n",
       "       [55.047085 ],\n",
       "       [ 8.319321 ],\n",
       "       [71.33504  ],\n",
       "       [40.815144 ],\n",
       "       [19.506878 ],\n",
       "       [79.81781  ],\n",
       "       [18.538984 ],\n",
       "       [75.86508  ],\n",
       "       [82.992775 ],\n",
       "       [36.189896 ],\n",
       "       [14.154114 ],\n",
       "       [23.619059 ],\n",
       "       [91.64897  ],\n",
       "       [22.318722 ],\n",
       "       [54.337425 ],\n",
       "       [47.51118  ],\n",
       "       [84.947685 ],\n",
       "       [57.18938  ],\n",
       "       [31.622595 ],\n",
       "       [43.19817  ],\n",
       "       [92.448425 ],\n",
       "       [64.52513  ],\n",
       "       [79.08581  ],\n",
       "       [44.773983 ],\n",
       "       [14.819932 ],\n",
       "       [ 4.8806415],\n",
       "       [54.459263 ],\n",
       "       [46.29559  ],\n",
       "       [21.935377 ],\n",
       "       [56.01905  ],\n",
       "       [31.389263 ],\n",
       "       [54.890427 ],\n",
       "       [90.70988  ],\n",
       "       [47.918694 ],\n",
       "       [64.515656 ],\n",
       "       [84.827156 ],\n",
       "       [55.029053 ],\n",
       "       [18.73835  ],\n",
       "       [31.811504 ],\n",
       "       [87.71693  ],\n",
       "       [38.11649  ],\n",
       "       [89.44499  ],\n",
       "       [ 7.7501235],\n",
       "       [43.48927  ],\n",
       "       [18.42538  ],\n",
       "       [76.96617  ],\n",
       "       [ 6.660412 ],\n",
       "       [67.029686 ],\n",
       "       [84.883835 ],\n",
       "       [ 8.523844 ],\n",
       "       [37.719646 ],\n",
       "       [28.34416  ],\n",
       "       [37.683086 ],\n",
       "       [43.853836 ],\n",
       "       [44.876514 ],\n",
       "       [94.81877  ],\n",
       "       [12.168156 ],\n",
       "       [22.091066 ],\n",
       "       [84.183464 ],\n",
       "       [62.175606 ],\n",
       "       [98.20133  ],\n",
       "       [24.011253 ],\n",
       "       [27.521166 ],\n",
       "       [51.802757 ],\n",
       "       [12.355768 ],\n",
       "       [85.4318   ],\n",
       "       [58.5071   ],\n",
       "       [ 9.460913 ],\n",
       "       [ 6.128225 ],\n",
       "       [13.805138 ],\n",
       "       [31.633713 ],\n",
       "       [40.36485  ],\n",
       "       [13.52146  ],\n",
       "       [97.552795 ],\n",
       "       [54.17442  ],\n",
       "       [91.80402  ],\n",
       "       [67.682396 ],\n",
       "       [60.834366 ],\n",
       "       [32.787743 ],\n",
       "       [88.0409   ],\n",
       "       [43.750443 ],\n",
       "       [23.397942 ],\n",
       "       [89.81179  ],\n",
       "       [75.0485   ],\n",
       "       [82.39999  ],\n",
       "       [66.42252  ],\n",
       "       [18.658064 ],\n",
       "       [62.011765 ],\n",
       "       [ 5.4870257],\n",
       "       [56.676334 ],\n",
       "       [98.94552  ],\n",
       "       [37.93123  ],\n",
       "       [31.311407 ],\n",
       "       [43.755245 ],\n",
       "       [93.62475  ],\n",
       "       [90.76799  ],\n",
       "       [39.670128 ],\n",
       "       [90.36412  ],\n",
       "       [36.516483 ],\n",
       "       [35.73324  ],\n",
       "       [ 7.169939 ],\n",
       "       [58.412323 ],\n",
       "       [12.19746  ],\n",
       "       [26.052902 ],\n",
       "       [54.174877 ],\n",
       "       [87.49812  ],\n",
       "       [82.242775 ],\n",
       "       [41.870876 ],\n",
       "       [34.44625  ],\n",
       "       [43.02531  ],\n",
       "       [17.032364 ],\n",
       "       [83.62484  ],\n",
       "       [10.968736 ],\n",
       "       [32.439606 ],\n",
       "       [65.60616  ],\n",
       "       [76.31281  ],\n",
       "       [23.848944 ],\n",
       "       [40.616318 ],\n",
       "       [ 5.294032 ],\n",
       "       [99.77863  ],\n",
       "       [26.977072 ],\n",
       "       [88.56875  ],\n",
       "       [85.86819  ],\n",
       "       [29.372385 ],\n",
       "       [39.822018 ],\n",
       "       [12.650776 ],\n",
       "       [63.014507 ],\n",
       "       [84.632164 ],\n",
       "       [95.18376  ],\n",
       "       [26.433874 ],\n",
       "       [77.83252  ],\n",
       "       [14.875907 ],\n",
       "       [85.1401   ],\n",
       "       [74.27663  ],\n",
       "       [31.248857 ],\n",
       "       [35.64906  ],\n",
       "       [31.05498  ],\n",
       "       [99.68828  ],\n",
       "       [79.05538  ],\n",
       "       [20.441633 ],\n",
       "       [ 2.5813007],\n",
       "       [92.59693  ],\n",
       "       [ 3.1809459],\n",
       "       [68.43465  ],\n",
       "       [65.15832  ]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5703129 ],\n",
       "       [0.57280207],\n",
       "       [0.5679818 ],\n",
       "       [0.57011825],\n",
       "       [0.57204247],\n",
       "       [0.56776875],\n",
       "       [0.5654014 ],\n",
       "       [0.5724563 ],\n",
       "       [0.57283044],\n",
       "       [0.56393886],\n",
       "       [0.57289624],\n",
       "       [0.5642747 ],\n",
       "       [0.5647346 ],\n",
       "       [0.5661392 ],\n",
       "       [0.5688555 ],\n",
       "       [0.57289284],\n",
       "       [0.571633  ],\n",
       "       [0.5640221 ],\n",
       "       [0.56396693],\n",
       "       [0.5722762 ],\n",
       "       [0.5651685 ],\n",
       "       [0.5717611 ],\n",
       "       [0.5726231 ],\n",
       "       [0.5721698 ],\n",
       "       [0.5723208 ],\n",
       "       [0.5675191 ],\n",
       "       [0.5669347 ],\n",
       "       [0.5665778 ],\n",
       "       [0.56739116],\n",
       "       [0.56959313],\n",
       "       [0.56619525],\n",
       "       [0.5644275 ],\n",
       "       [0.5675455 ],\n",
       "       [0.5665789 ],\n",
       "       [0.5662262 ],\n",
       "       [0.56419235],\n",
       "       [0.56527543],\n",
       "       [0.57188416],\n",
       "       [0.5696301 ],\n",
       "       [0.5680497 ],\n",
       "       [0.56851643],\n",
       "       [0.5661863 ],\n",
       "       [0.56520784],\n",
       "       [0.57040405],\n",
       "       [0.56423104],\n",
       "       [0.57337195],\n",
       "       [0.56394553],\n",
       "       [0.56419635],\n",
       "       [0.565021  ],\n",
       "       [0.5708281 ],\n",
       "       [0.5644102 ],\n",
       "       [0.57095367],\n",
       "       [0.57024676],\n",
       "       [0.5640254 ],\n",
       "       [0.5651984 ],\n",
       "       [0.5651871 ],\n",
       "       [0.5643718 ],\n",
       "       [0.5723469 ],\n",
       "       [0.56881607],\n",
       "       [0.5691171 ],\n",
       "       [0.56658953],\n",
       "       [0.5685335 ],\n",
       "       [0.571077  ],\n",
       "       [0.5714173 ],\n",
       "       [0.56753737],\n",
       "       [0.5668126 ],\n",
       "       [0.5639459 ],\n",
       "       [0.56633687],\n",
       "       [0.5657036 ],\n",
       "       [0.56445915],\n",
       "       [0.57232875],\n",
       "       [0.5682773 ],\n",
       "       [0.5676267 ],\n",
       "       [0.5693866 ],\n",
       "       [0.5641986 ],\n",
       "       [0.56436646],\n",
       "       [0.5715228 ],\n",
       "       [0.5676018 ],\n",
       "       [0.5672878 ],\n",
       "       [0.56961924],\n",
       "       [0.567521  ],\n",
       "       [0.56882966],\n",
       "       [0.5659366 ],\n",
       "       [0.56952506],\n",
       "       [0.56834716],\n",
       "       [0.5684365 ],\n",
       "       [0.56484944],\n",
       "       [0.5687439 ],\n",
       "       [0.5641013 ],\n",
       "       [0.56671804],\n",
       "       [0.5707951 ],\n",
       "       [0.56753266],\n",
       "       [0.5703981 ],\n",
       "       [0.5656486 ],\n",
       "       [0.56431943],\n",
       "       [0.5652903 ],\n",
       "       [0.56588566],\n",
       "       [0.564438  ],\n",
       "       [0.56466097],\n",
       "       [0.5708417 ],\n",
       "       [0.5681925 ],\n",
       "       [0.56631446],\n",
       "       [0.5646063 ],\n",
       "       [0.56979805],\n",
       "       [0.57075214],\n",
       "       [0.56656826],\n",
       "       [0.5695274 ],\n",
       "       [0.56482023],\n",
       "       [0.569459  ],\n",
       "       [0.5705917 ],\n",
       "       [0.5640245 ],\n",
       "       [0.5654715 ],\n",
       "       [0.5664263 ],\n",
       "       [0.57229257],\n",
       "       [0.56546074],\n",
       "       [0.5658697 ],\n",
       "       [0.5640881 ],\n",
       "       [0.5666068 ],\n",
       "       [0.56514025],\n",
       "       [0.5654823 ],\n",
       "       [0.56543887],\n",
       "       [0.56863385],\n",
       "       [0.5662201 ],\n",
       "       [0.57082665],\n",
       "       [0.5655189 ],\n",
       "       [0.56973624],\n",
       "       [0.56696326],\n",
       "       [0.56518817],\n",
       "       [0.56780505],\n",
       "       [0.5672766 ],\n",
       "       [0.5641262 ],\n",
       "       [0.5646961 ],\n",
       "       [0.5681622 ],\n",
       "       [0.5719203 ],\n",
       "       [0.5658779 ],\n",
       "       [0.56842524],\n",
       "       [0.56400466],\n",
       "       [0.5724513 ],\n",
       "       [0.5639553 ],\n",
       "       [0.5678071 ],\n",
       "       [0.57088816],\n",
       "       [0.5670094 ],\n",
       "       [0.566787  ],\n",
       "       [0.5639359 ],\n",
       "       [0.5669789 ],\n",
       "       [0.56942016],\n",
       "       [0.5672692 ],\n",
       "       [0.56461626],\n",
       "       [0.56562716],\n",
       "       [0.5662728 ],\n",
       "       [0.56770694],\n",
       "       [0.5673514 ],\n",
       "       [0.56526154],\n",
       "       [0.5647656 ],\n",
       "       [0.56694674],\n",
       "       [0.5667307 ],\n",
       "       [0.57243496],\n",
       "       [0.5654856 ],\n",
       "       [0.56791013],\n",
       "       [0.57087326],\n",
       "       [0.5648387 ],\n",
       "       [0.5710084 ],\n",
       "       [0.5651401 ],\n",
       "       [0.56459653],\n",
       "       [0.5685524 ],\n",
       "       [0.5716205 ],\n",
       "       [0.57029927],\n",
       "       [0.5642121 ],\n",
       "       [0.57048076],\n",
       "       [0.56678504],\n",
       "       [0.5673085 ],\n",
       "       [0.56444746],\n",
       "       [0.5665661 ],\n",
       "       [0.5691865 ],\n",
       "       [0.5676577 ],\n",
       "       [0.5641846 ],\n",
       "       [0.5660049 ],\n",
       "       [0.5648945 ],\n",
       "       [0.5675184 ],\n",
       "       [0.57152754],\n",
       "       [0.572915  ],\n",
       "       [0.5667756 ],\n",
       "       [0.5674017 ],\n",
       "       [0.57053435],\n",
       "       [0.566656  ],\n",
       "       [0.56921893],\n",
       "       [0.5667426 ],\n",
       "       [0.56424433],\n",
       "       [0.56727725],\n",
       "       [0.5660056 ],\n",
       "       [0.56445664],\n",
       "       [0.5667319 ],\n",
       "       [0.57098055],\n",
       "       [0.5691603 ],\n",
       "       [0.56434715],\n",
       "       [0.5682848 ],\n",
       "       [0.5642877 ],\n",
       "       [0.5725144 ],\n",
       "       [0.56763   ],\n",
       "       [0.5710243 ],\n",
       "       [0.56505615],\n",
       "       [0.5726666 ],\n",
       "       [0.5658139 ],\n",
       "       [0.5644524 ],\n",
       "       [0.5724064 ],\n",
       "       [0.5683399 ],\n",
       "       [0.56964177],\n",
       "       [0.568345  ],\n",
       "       [0.5675952 ],\n",
       "       [0.5675106 ],\n",
       "       [0.5641032 ],\n",
       "       [0.57189775],\n",
       "       [0.5705126 ],\n",
       "       [0.5645057 ],\n",
       "       [0.56618404],\n",
       "       [0.56398696],\n",
       "       [0.57024455],\n",
       "       [0.56975603],\n",
       "       [0.5669794 ],\n",
       "       [0.5718715 ],\n",
       "       [0.56442565],\n",
       "       [0.56646377],\n",
       "       [0.5722756 ],\n",
       "       [0.57274085],\n",
       "       [0.5716692 ],\n",
       "       [0.56918496],\n",
       "       [0.56797266],\n",
       "       [0.5717088 ],\n",
       "       [0.56400925],\n",
       "       [0.56679755],\n",
       "       [0.5642067 ],\n",
       "       [0.5657641 ],\n",
       "       [0.5662863 ],\n",
       "       [0.56902474],\n",
       "       [0.564336  ],\n",
       "       [0.567605  ],\n",
       "       [0.57033014],\n",
       "       [0.56427515],\n",
       "       [0.56520236],\n",
       "       [0.5646418 ],\n",
       "       [0.56586015],\n",
       "       [0.57099175],\n",
       "       [0.56619656],\n",
       "       [0.5728303 ],\n",
       "       [0.5666057 ],\n",
       "       [0.5639614 ],\n",
       "       [0.56831056],\n",
       "       [0.5692298 ],\n",
       "       [0.5676046 ],\n",
       "       [0.5641442 ],\n",
       "       [0.5642423 ],\n",
       "       [0.56806916],\n",
       "       [0.56425625],\n",
       "       [0.568507  ],\n",
       "       [0.5686158 ],\n",
       "       [0.5725955 ],\n",
       "       [0.56647104],\n",
       "       [0.57189363],\n",
       "       [0.5699599 ],\n",
       "       [0.5667975 ],\n",
       "       [0.56435466],\n",
       "       [0.56465375],\n",
       "       [0.56778437],\n",
       "       [0.5687945 ],\n",
       "       [0.5676742 ],\n",
       "       [0.5712187 ],\n",
       "       [0.5645484 ],\n",
       "       [0.5720652 ],\n",
       "       [0.5690731 ],\n",
       "       [0.56592244],\n",
       "       [0.565106  ],\n",
       "       [0.57026714],\n",
       "       [0.56793773],\n",
       "       [0.57285726],\n",
       "       [0.5639328 ],\n",
       "       [0.56983155],\n",
       "       [0.5643178 ],\n",
       "       [0.5644106 ],\n",
       "       [0.56949896],\n",
       "       [0.568048  ],\n",
       "       [0.57183033],\n",
       "       [0.5661201 ],\n",
       "       [0.56447154],\n",
       "       [0.5640906 ],\n",
       "       [0.56990695],\n",
       "       [0.5649901 ],\n",
       "       [0.5715197 ],\n",
       "       [0.56443566],\n",
       "       [0.5652612 ],\n",
       "       [0.5692384 ],\n",
       "       [0.5686274 ],\n",
       "       [0.5692653 ],\n",
       "       [0.5639359 ],\n",
       "       [0.5648969 ],\n",
       "       [0.5707428 ],\n",
       "       [0.5732359 ],\n",
       "       [0.5641795 ],\n",
       "       [0.57315224],\n",
       "       [0.56570673],\n",
       "       [0.5659566 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "reload(tegrta)\n",
    "loaded_model = tf.keras.models.load_model('./my_model.h5', custom_objects={'FeatureNormalization': tegrta.FeatureNormalization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weights = loaded_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26898333, -0.01164479,  0.16062233, -0.18270159,  0.14628926,\n",
       "         0.2748221 , -0.00174278,  0.180469  , -0.504072  ,  0.2314341 ,\n",
       "        -0.35277727,  0.29731816, -0.05552946, -0.19825546, -0.2693522 ,\n",
       "        -0.47772452, -0.04418206, -0.4411161 ,  0.22911055,  0.06180804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26898333, -0.01164479,  0.16062233, -0.18270159,  0.14628926,\n",
       "         0.2748221 , -0.00174278,  0.180469  , -0.504072  ,  0.2314341 ,\n",
       "        -0.35277727,  0.29731816, -0.05552946, -0.19825546, -0.2693522 ,\n",
       "        -0.47772452, -0.04418206, -0.4411161 ,  0.22911055,  0.06180804]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    assert np.allclose(weights[0], loaded_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(y, y_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
